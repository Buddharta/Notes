\documentclass{report}
\pagenumbering{Roman}
%Paquetes necesarios para el idioma español.
\usepackage[T1]{fontenc} %Codificacion de fuente
\usepackage[activeacute,spanish]{babel}
\usepackage[utf8]{inputenc} % Escribir con acentos, n, ...
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{amsmath, amsfonts, amsthm, fouriernc}
\usepackage{slashed}


\begin{document}

\title{Notas del curso de Mec\'anica Cu\'antica Relativista}% Force line breaks with \\
%\thanks{A footnote to the article title}%

\author{Dr. Rodolfo Mart\'inez y Romero.}


\chapter{Introducci\'on}

\section{Ecuaci\'on de Klein Gordon. }

\[E = \sqrt{P^2 + m _{0}^2}\]

\[P^{\mu} = \pi ^{\mu} + e A^{\mu}   \pi^{\mu} = p ^{\mu} - e A^{\mu}\]

\[P^{\mu} \rightarrow \pi ^{\mu} = P^{\mu} - e A^{\mu}\]

\[E - e \phi = \sqrt{(\overrightarrow{P} -e \overrightarrow{A})^2 + m_0}\]

\[\overrightarrow{P} = -i \nabla\]

\[E \rightarrow H = i \partial_t\]

del conmutador

$[P_X.X] \neq 0$ esto es $= \frac{1}{i}$

\[P^{\mu} = (E, \overrightarrow{P})\]
\[P^{\mu} = i \partial ^{\mu}\]
\[P^{0} = E\]
\[P^{i} = P_{x}\]

Dualidad onda-partícula

\begin{equation}
\Psi \sim e^{i P/\hbar X}  \frac{\hbar}{i} \partial_{x} \rightarrow P_{x}
\end{equation}

\begin{equation}
H = \sqrt{(\overrightarrow{P}-e\overrightarrow{A})^2+m_{0}^2} + e\psi
\end{equation}

\begin{equation}
(E-e\phi)^2=(\overrightarrow{P}-e\overrightarrow{A})^2+m_{0}^2
\end{equation}

Y la ecuación de Klei-Gordon, siendo el primer término $E-e\phi$ y el segundo $\overrightarrow{P} -e\overrightarrow{A}$

\begin{equation}
\lbrace(i\partial_{t}-e\phi)^2 - (-i\nabla - e\overrightarrow{A})^2 - m_{0}^2\rbrace \psi = 0
\end{equation}

\[(E - e \psi)^2 - (\overrightarrow{P}-e\overrightarrow{A})^2 - m_{0}^2\]
\[ = P^{\mu}P_{\mu}-m_{0}^2\]
\[ [P^{\mu} P_{\mu} - m_{0}^2 ]\psi = 0\]

Klein Gordon

\[= [(P^{\mu}-eA^{\mu})(P_{\mu}-eA_{\mu})-m_{0}^2]\psi = 0\]

\[P^{\mu} = i \partial^{\mu}\]
\[=[(i \partial ^{\mu} - eA^{\mu})(i \partial_{\mu} - eA_{\mu})-m_{0}^2]\psi = 0\]
\[[(\partial^{\mu}ieA^{\mu})(\partial_{\mu}+ieA_{\mu})+m_{0}^2]\psi = 0\]

Se define la derivada covariante $D_{\mu} \equiv \partial_{\mu} + ieA_{\mu}$ y la ecuación de K-G

\begin{equation}
[D^{\mu}D_{\mu}+m_{0}^2]\psi = 0
\end{equation}

en el caso libre $A_{\mu} = 0$ entonces K-G

\begin{equation}
[\partial^{\mu} \partial_{\mu}+m_{0}^2]\psi = 0
\end{equation}

con $\partial_{\mu} \partial^{\mu} = \partial^{\mu} \partial_{\mu} = \Box$

entonces K-G libre:

\begin{equation}
[\Box + m_{0}]\psi = 0 \longleftrightarrow [(\frac{\partial^2}{\partial^2}-\nabla^2)+m_{0}^2]\psi = 0
\end{equation}

K.G es una ecuación con segundas derivadas en el tiempo, se necesita a $\psi$ y $\partial\psi/\partial t$ inicial. Sean

\[\psi_1 \equiv \psi + \frac{i}{m_{0}} \frac{\partial \psi}{\partial t}\] ; \[\psi_2 \equiv \psi - \frac{i}{m_{0}} \frac{\partial \psi}{\partial t}\]

\[\psi = \frac{1}{2} (\psi_1 + \psi _2) \] ; \[\psi_1-\psi_2 = \frac{2i}{m_0} \frac{\partial \psi}{\partial t}\]

\[\frac{\partial \psi}{\partial t} = \frac{m_0}{2i}(\psi_1-\psi_2) \]

\[\frac{\partial\psi_1}{\partial t} = \frac{\partial \psi}{\partial t} + \frac{i}{m_{0}} \frac{\partial^2\psi}{\partial t^2} = \frac{m_{0}}{2i}(\psi _1 - \psi_2)+\frac{i}{m_{0}}(\nabla ^2 - m_{0}^2)\psi\]
\[= \frac{m_o}{2i} (\psi_1 - \psi _2) + \frac{i}{2m_0}(\nabla ^2 - m_0)(\psi_1 + \psi _2)\]

\[\frac{\partial \psi_2}{\partial t} = \frac{\partial \psi}{\partial t} - \frac{i}{m_0} \frac{\partial \psi }{\partial t} = \frac{m_0}{\partial i} (\psi _1 - \psi _2)- \frac{i}{2 m_0} (\nabla ^2 - m_0 ^2) (\psi _1 + \psi _2)\]

Sea \[ \Psi \equiv \binom{\psi_{1}}{\psi_{2}}\]


\[\frac{\partial \psi}{\partial t} = \frac{m_0}{2i} \left ( \begin{array}{cc}
 1 & -1  \\
 1 & -1 \\ \end{array} \right) \Psi + \frac{i}{2 m_0} (\nabla^2 - m_{0}^2) \left ( \begin{array}{cc}
 1 & 1  \\
 -1 & -1 \\ \end{array} \right) \Psi \]

se puede renombrar la primer matriz como $A$ y la segunda como $B$ en

\begin{equation}
\frac{\partial \Psi}{\partial t} \pm \frac{m_0}{2i} A \Psi + \frac{i}{2 m_0} (\nabla ^2 - m_0) B \Psi
\end{equation}

\[A^2 = \left ( \begin{array}{cc}
 1 & -1  \\
 1 & -1 \\ \end{array} \right) \left ( \begin{array}{cc}
 1 & -1  \\
 1 & -1 \\ \end{array} \right) 
 = \left ( \begin{array}{cc}
 0 & 0  \\
 0 & 0 \\ \end{array} \right)\]

\[B^2 = \left ( \begin{array}{cc}
 1 & 1  \\
 -1 & -1 \\ \end{array} \right) \left ( \begin{array}{cc}
 1 & 1  \\
 -1 & -1 \\ \end{array} \right) 
 = \left ( \begin{array}{cc}
 0 & 0  \\
 0 & 0 \\ \end{array} \right)\]

con la multiplicación de matrices 

\[AB = \left ( \begin{array}{cc}
 1 & -1  \\
 1 & -1 \\ \end{array} \right) \quad \left ( \begin{array}{cc}
 1 & 1  \\
 -1 & -1 \\ \end{array} \right) 
 = \left ( \begin{array}{cc}
 2 & 2  \\
 2 & 2 \\ \end{array} \right) = \left 2 ( \begin{array}{cc}
 1 & 1  \\
 1 & 1 \\ \end{array} \right) \]

\[A^t = B\]

tomando el límite no relativista $E \simeq m_{0}$

con $i \frac{\partial \psi}{\partial t} \simeq m_{0} \psi$ una ecuación de primer orden, una componente debe desaparecer

\[ \psi_{1} \simeq \psi + \psi = 2 \psi\]

\[\psi _{2} \simeq \psi - \psi = 0\]

y la Densidad de probabilidad

\[\frac{\partial \phi}{\partial t} + \nabla \cdot \overrightarrow{J} = 0 \]

\[\partial _{\mu} j^{\mu} = 0\]

\[j^{\mu} \equiv (\rho , \overrightarrow{J})\]

con $j^{0} = \rho$ y $j^{\alpha} = j_{x}$, etc. Se tiene

\begin{equation}
(\Box + m_{0}^2)\psi = 0 \Rightarrow (\Box + m_{0}^2)\psi^{*} = 0
\end{equation}

\[ \begin{cases}
    \psi^{*} \Box \psi = -m_{0} \psi^{*} \psi      \\
    \psi \Box \psi^{*} = -m_{0} \psi^{*} \psi \\
  \end{cases} \Rightarrow \psi^{*}\Box \psi - \psi \Box \psi^{*} = 0
\]

i. e 

\begin{equation}
\psi^{*}(\partial_{\mu} \partial^{\mu} \psi) - \psi (\partial_{\mu} \partial^{\mu} \psi^{*}) = 0
\end{equation}

Definiendo por analogía con Schrödinger

\begin{equation}
j^{\mu} = (\psi^{*} \partial_{t} \psi)-(\psi \partial_{t}\psi^{*}) \frac{i}{2m}
\end{equation}

con $\partial_{\mu} j^{\mu} = 0$ se puede definir

\begin{equation}
\rho \equiv \frac{1}{2m} (\psi^{*}\partial _{t} \psi) - (\psi \partial_{t} \psi^{*})
\end{equation}

y la densidad de corriente 

\begin{equation}
\overrightarrow{J} = -\frac{i}{2m} (\psi^{*} \nabla \psi - \psi \nabla \psi^{*})
\end{equation}

No es definida positiva, si se multiplica por la unidad de carga $e$ $\rho = e \rightarrow e\rho$ pero $\overrightarrow{j} \rightarrow e \overrightarrow{j}$ aunque se conserva la carga, ¡no se conserva el número de partículas!

De la ecuación para onda onda plana

\begin{equation}
\psi \sim e^{-i(Et-\overrightarrow{P} \cdot \overrightarrow{r})} = e^{-i P^{\mu} \chi _{\mu}}
\end{equation}

entonces

\begin{equation}
(\partial^{\mu} \partial_{\mu} + m_{0}^{2})\psi = 0 \quad \partial^{\mu} \psi = -i P^{\mu} \psi
\end{equation}

esto es 

\[\partial_{\mu} \partial^{\mu} \psi = - P_{\mu} P^{\mu} \psi \]

por tanto 

\[\therefore (-P_{\mu}P^{\mu} + m_{0}^2)\psi = 0 \]

\[E^2 - \overrightarrow{P}^2 - m_{0}^2 = 0\]

\begin{equation}
E = \pm \sqrt{\overrightarrow{P}^2 + m_{0}^2}
\end{equation}

se obtienen por tanto energías negativas.

%FALTA UN EJEMPLO 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Ecuaci\'on de Dirac}


\begin{equation}
H_{D} = \overrightarrow{\alpha} \cdot \overrightarrow{P} + \beta m_{0}+ \overrightarrow{\gamma} \cdot \overrightarrow{r} + \delta
\end{equation}

\begin{equation}
H_{D} = \overrightarrow{\alpha} \cdot \overrightarrow{P} + \beta m_{0}
\end{equation}

\[i \partial t \psi = (\overrightarrow{\alpha} \cdot \overrightarrow{P} + \beta m_{0}) \psi\]

Una teoría que conserva el número de partículas implica un hamiltoniano hermitiano

\[i \partial _{t} \psi = H \psi\]

\[i \partial_{t} \psi^{*} = -(H \psi)^{*}\]

\[\partial _{t} |\psi|^2 = \partial_{t} \psi^{*} \psi = (\partial_{t}\psi^{*})\psi + \psi^{*}\partial_{t}\psi\]

\[= i [(H\psi)^{*}\psi - \psi^{*}(H\psi)]\]

\[\frac{dN}{d t} = \int_{\Omega} \partial _{t} |\psi|^2 d^3 r= i \int d^3 r (H \psi)^{*} \psi - i \int \psi^{*} (H \psi) d^3 r =0\]

\begin{equation}
\int (H\psi)^{*} \psi d^{2}r= \int \psi^{*}(H \psi) d^3 r
\end{equation}

\[\langle \psi | H \psi \rangle \equiv \langle H^{\dagger} \psi | \psi \rangle = \langle h \psi | \psi \rangle\]

\begin{equation}
H = H^{\dagger}
\end{equation}

Se define

\begin{equation}
E = \overrightarrow{\alpha} \cdot \overrightarrow{P} + \beta m_{0}
\end{equation}


con $E^{2} = p^{2} + m_{0}^2$ dado que las soluciones a Dirac son ecuaciones a K-G

\begin{equation}
(E-\overrightarrow{\alpha} \cdot \overrightarrow{P} - \beta m_{0}) \psi = 0
\end{equation}



\[(E^2 - P^2 -m_{0}^2)\psi = 0\]

multiplicando (2.6) por $E + \overrightarrow{\alpha} \cdot \overrightarrow{P} + \beta m_{0}$

\[(E + \overrightarrow{\alpha} \cdot \overrightarrow{P} + \beta m_{0})(E + \overrightarrow{\alpha} \cdot \overrightarrow{P} - \beta m_{0})\psi_{D} = 0\]

\[(E^2 + -E \overrightarrow{\alpha} \cdot \overrightarrow{P} -E m_{0} \beta + \overrightarrow{\alpha} \cdot \overrightarrow{P} E -(\overrightarrow{\alpha} \cdot \overrightarrow{P})(\overrightarrow{\alpha} \cdot \overrightarrow{P})- \overrightarrow{\alpha} \cdot \overrightarrow{P} \beta m_{0}+ \beta m_{0}E-\beta \overrightarrow{\alpha} \cdot \overrightarrow{P} m_{0} -\beta^2 m_{0}^2)\psi_{D} = 0\]

\[\lbrace E^2 -[(\overrightarrow{\alpha} \cdot \overrightarrow{P})(\overrightarrow{\alpha} \cdot \overrightarrow{P})+(\beta \overrightarrow{\alpha} \cdot \overrightarrow{P}+ \overrightarrow{\alpha} \cdot \overrightarrow{P} \beta) m_{0}+ \beta^{2} m_{0}^2]\rbrace \psi_{0}=0 \]

nótese que del producto $(\overrightarrow{\alpha} \cdot \overrightarrow{P})(\overrightarrow{\alpha} \cdot \overrightarrow{P}) \equiv \alpha^i P^i \alpha^j P^j$

\[\lbrace E^2 - \sum (\alpha^i)^2(P^i)^2 - \sum_{i <j} (\alpha^o \alpha^j + \alpha ^j \alpha^i) P^i P^j + ()m_{0} i - \beta ^2 m_{0}^2 \rbrace \psi_{D} = 0\]

con $(\alpha ^i)^2 = 1$ para $i = 1, 2, 3$ se define entonces un  álgebra de Clifford

\[\alpha^i \alpha ^j + \alpha ^j \alpha^i = 0 \quad \beta \alpha^i + \alpha ^i \beta =0 \quad \beta ^2 = 1\]

\[(\alpha ^i)^2 = 1 , \quad \lbrace \alpha^i, \alpha^j \rbrace = 0 \quad
i \neq j\]

\[\beta^2 = 1 \quad \lbrace \alpha^i, \beta \rbrace = 0\]

\textbf{Caso:}  Aplicando un campo electromagnético $ E \rightarrow E-e\phi$ cuyo acoplamiento máximo $\overrightarrow{P} \rightarrow e \overrightarrow{A}$

\[E = E\phi + \overrightarrow{\alpha} \cdot (\overrightarrow{P} - e \overrightarrow{A}) + \beta m_{0}\]

\[i \partial _{t} \psi = \lbrace e \phi + \overrightarrow{\alpha } \cdot (-i \nabla - e \overrightarrow{A}) + \beta m_{0}\]

\begin{equation}
H_{D} = e \phi + \overrightarrow{\alpha} \cdot (\frac{1}{i} \nabla - e \overrightarrow{A}) + \beta m_{0}
\end{equation}

ojo:

\[\alpha^{1} = \alpha_{X} \quad \alpha^2 = \alpha_{y} \quad \alpha^3 = \alpha _{z} \]

Las $\alpha^i$'s son hermitianas porque $H_{D}$ y $\beta$ lo son; además

\[\alpha^i = - \beta \alpha^i \beta\]

\[tr \alpha ^ i = - ts (\beta \alpha^i \beta) = - tr(\beta ^2 \alpha^i) = -ts \alpha ^i\]

\[\Rightarrow tr \alpha^i = 0 \quad \text{análogamente} \quad  tr \beta =0\]

\[\text{como} \beta^2 = (\alpha ^i)^2 = 1 \quad \text{sus eigenvalores son } \quad \pm 1\]

$\therefore$ la dimensión de las matrices de Dirac es par. Su dimensión es 4 porque 2 sólo acomoda a las matrices de Pauli y a la identidad.

\section{Recordatorio:}

\[\sigma_{x} = \left ( \begin{array}{cc}
 0 & 1  \\
 1 & 0 \\ \end{array} \right)  
 \quad \sigma_{y}\left ( \begin{array}{cc}
 0 & -1  \\
 i & 0 \\ \end{array} \right) 
 \quad \sigma_{z}= \left ( \begin{array}{cc}
 1 & 0  \\
 0 & -1 \\ \end{array} \right) =\]

Se define

\begin{equation}
\overrightarrow{S} = \frac{1}{2} \overrightarrow{\sigma}
\end{equation}

\[i) \quad[S_{i},S_{j}] = i \epsilon_{ijk}S_{k} \]

\[\text{Con} \quad \epsilon_{123} =
  \begin{cases}
    1       & \quad \text{si } \epsilon \text{ es par}\\
    -1  & \quad \text{si } \epsilon \text{ es impar}\\
    0  & \quad \text{ otro caso}\\
  \end{cases}\]

\[ii) \quad S^2 \psi_{1/2} = \frac{1}{2}(\frac{1}{2} + 1) \psi_{1/2} = \frac{3}{4} \psi _{1/2}\]

\[S^{2} = \frac{1}{4} (\sigma_{x}^{2} +\sigma_{y}^{2} +\sigma_{z}^{2} ) \quad \sigma^2 \quad \sigma^{1} = \sigma_{x} \text{etc.}\]

\[iii) \quad \lbrace \sigma_{i},\sigma_{j} \rbrace = 0 \quad i \neq j \quad \lbrace \sigma_{i} , \sigma_{j} = 2 \delta _{i j}\]

\[\Rightarrow [\sigma_{i},\sigma_{j}] = i \epsilon_{ijk} 2 \sigma_{k}\]

\[
  \begin{cases}
    \sigma _{i} \sigma_{j} + \sigma_{j} \sigma_{i} = 2 \delta_{ij}\\
    \sigma _{i} \sigma_{j} - \sigma_{j} \sigma_{i} = 2 i \epsilon_{ijk} \sigma_{k}\\
  \end{cases}\]

con

\[2 \sigma _{i} \sigma_{j} = 2 \delta_{ij} + 2 i \epsilon_{ijk} \sigma_{k}\]

\[\sigma _{i} \sigma_{j} = \delta_{ij} + i \epsilon _{ijk} \sigma_{k} \]

\subsection{Tarea:}

1) Demostrar que: $(\overrightarrow{\sigma} \cdot \overrightarrow{A})(\overrightarrow{\sigma} \cdot \overrightarrow{B}) = \overrightarrow{A} \cdot \overrightarrow{B} + i \overrightarrow{\sigma} \cdot (\overrightarrow{A} \times \overrightarrow{B})$

\textbf{Definición:}

\[\sigma_{x} = - i \alpha_{y} \alpha_{z}\]

\[\sigma_{y} = - i \alpha_{z} \alpha_{x}\]

\[\sigma_{z} = - i \alpha_{x} \alpha_{y}\]


\[\text{Con} \quad \sigma_{x}^{\dagger} = i \alpha_{z}^{\dagger} \alpha_{y}^{\dagger} = - i \alpha_{y} \alpha_{z} \quad \text{etc. Son hermitianos}\]

2) Demostrar que:

\[i)[\sigma _{i} , \sigma _{j}] = i \epsilon _{ijk} \sigma _{k} 2 \]

\[ii)\sigma _{i}^{2} = 1\]

\[iii)\lbrace \sigma _{i} , \sigma _{j} = 2 \delta _{ij}\]   

\[\text{Definición:} \quad \rho_{1} = - i \alpha_{x} \alpha_{y} \alpha_{z} \\
\rho _{2}= - \beta \alpha_{x} \alpha_{y} \alpha_{z}\\
\rho_{3} = \beta\\ \]

\[i) \rho_{1}^2 = 1 \quad \text{en general} \quad \rho_{i}^2 = 1\]

\[ii)\lbrace \rho_{1},\rho_{2}\rbrace = 0 \quad \text{etc} \quad [\rho_{i} , \rho_{j}] = 2 i \epsilon_{ijk} \rho_{k} \]

\[iii)\rho_{i} \quad \text{son hermitianas} \]

y además, $[\rho_{i}, \sigma_{j}] = 0 \forall i,j $ i.e. se tiene $SU(2) \otimes SU(2)$


\section{Covarianza} 

\[[(i \partial_{t} - e \phi)- \overrightarrow{\alpha} \cdot (\frac{1}{i} \nabla -e \overrightarrow{A})-\beta m_{0}] \psi = 0\]

\[[\beta(i \partial_{t} - e \phi)-\beta \overrightarrow{\alpha} \cdot (\frac{1}{i} \nabla - e \overrightarrow{A})m_{0}] \psi = 0\]

Si $\gamma^{\mu} \equiv (\beta , \beta \overrightarrow{\alpha})$

\[[\gamma ^{\mu} (i \partial_{\mu} - e A_{\mu}) - m_{0}] \psi = 0\]

\[[ i \gamma (\partial_{\mu} + i e A_{\mu}) - m_{0}] \psi = 0 \]

\[\Rightarrow [i \gamma^{\mu} D_{\mu} - m_{0}]\psi = 0\]

\section{Propiedades de las matrices gamma}

\begin{equation}
\gamma^{\mu} \gamma ^{\nu} + \gamma ^{\nu} \gamma ^{\mu}= 2 g^{\mu \nu}
\end{equation}

\[\gamma^{0\dagger} = \beta^{\dagger} = \gamma^{0}\]

\[\gamma^{i \dagger} = (\beta \alpha^i)^{\dagger} = - \gamma^{i} = \gamma_{i}\]

\[\gamma^{\mu \dagger} \equiv \gamma^0 \gamma ^{\mu} \gamma ^0 = \gamma _{\mu}\]

\[(\gamma^{0})^2 = 1 \quad \gamma^{\mu \dagger} = \gamma_{\mu} = (\gamma ^{\mu})^{-1}, \quad (\gamma ^i)^2\]

\textbf{Definición de Corriente}:

\begin{equation}
[i \gamma^{\mu}(\partial_{\mu} + i e A_{\mu}) - m_{}] \psi = 0
\end{equation}0

\[\psi^{\dagger [\gamma^{\mu \dagger}(- i \partial_{\mu} - e A_{\mu}) - m_{0}] = 0}\]

con $\overline{\psi} \equiv \psi^{\dagger} \gamma^{0}$

\[\overline{\psi}[\gamma^{0} \gamma^{\mu \dagger} \gamma^{0}(-i \partial_{\mu}-eA_{\mu})-m_{0}]=0\]

\[\overline{\psi} [\gamma^{\mu} (-i\partial_{\mu} - eA_{\mu}) - m_{0}]=0\]

\[(-i \partial_{\mu} - eA_{\mu}) \overline{\psi} \gamma^{\mu} - m_{0} \overline{\psi} = 0\]

entonces

\[\partial_{\mu} (\overline{\psi} \gamma^{\mu} \psi) = (\partial_{\mu} \overline{\psi}) \gamma^{\mu} \psi + \overline{\psi} \gamma^{\mu} \partial_{\mu} \psi \]

\[-i \partial_{\mu} \overline{\psi} \gamma^{\mu} = m_{0} \overline{\psi} + e A_{\mu} \overline{\psi} \gamma^{\mu}\]

\begin{equation}
\partial_{\mu} \overline{\psi} \gamma^{\mu} = i (m_{0} \overline{\psi} + e A_{\mu} \overline{\psi} \gamma^{\mu})
\end{equation}

\[i \gamma^{\mu} \partial_{\mu} \psi = e \gamma^{\mu}A_{\mu} \psi + m_{0} \psi\]

\begin{equation}
 \gamma^{\mu} \partial_{\mu} \psi = -i(e \gamma^{\mu} A_{\mu} \psi + m_{0} \psi )
\end{equation}

\[\partial_{\mu} (\overline{\psi} \gamma^{\mu} \psi) = i (m_{0} \overline{\psi} \psi + e A_{\mu} \overline{\psi} \gamma^{\mu} \psi)\]

\[- i (eA_{\mu} \overline{\psi} \gamma^{\mu} \psi + m_{0} \overline{\psi} \psi) = 0\]

\[j^{\mu} \equiv \overline{\psi} \gamma^{\mu} \psi = (\psi^{\dagger} \psi , \psi^{\dagger} \alpha^{i} \psi) \equiv (\rho, \overrightarrow{j})\]

matrices $F^{A}$ como las $\gamma^{\mu}$ (no son únicas) anticomutany el cuadrado es $\pm 1$, se pueden agrupar como:

\begin{center}
    \begin{tabular}{| l | l | l |}
    \hline
     & $(\Gamma^{A})^2 = 1$ & $(\Gamma^A)^2=1$  \\ \hline
    E & I &    \\ \hline
    V & $\gamma^0$ & $\gamma^{1} \gamma^2 \gamma^3$  \\ \hline
    T & $\gamma^1 \gamma^0, \gamma^2 \gamma^0, \gamma^3 \gamma^0$ & $\gamma^2 \gamma^3 , \gamma^3 \gamma^1 . \gamma^1 \gamma^2$ \\ \hline
    NA & $\gamma^1 \gamma^2 \gamma^3$ & $\gamma^0 \gamma^2 \gamma^3 , \gamma^0 \gamma^3 \gamma^1 , \gamma^0 \gamma^1 \gamma^2$ \\ \hline
P & &$\gamma^0 \gamma^1 \gamma^2 \gamma^3$
\\ \hline
    \end{tabular}
\end{center}

Si $\Gamma  \neq I$ conmuta con 8 matrices $\Gamma^A$ y anticonmuta con las otras 8.

Se define $\gamma^5 = \gamma^0 \gamma^1 \gamma^2 \gamma^3$ (o $i \gamma^0 \gamma^1 \gamma^2 \gamma^3 $)

\[(\gamma^1)^2 = -1 \quad \text{o} \quad (\gamma^5)^2 = 1\]

Denotaremos la inversa de $\Gamma^A$ como $\Gamma_A = \pm \Gamma^A$ dependiendo si $(\Gamma ^A)^2 =\pm 1$.

\begin{itemize}

\item \textbf{Lema de ordenamiento}: Por inspección, si se multiplican 2 matrices $\Gamma^A$ se vuelve a encontrar una matriz $\Gamma^A$ excepto por un signo.


\item Traza y determinante. si $\Gamma^{A} \neq \Gamma \quad (tr I = 4)$

\[ \Gamma^A = \Gamma^B \Gamma^A \Gamma^B \quad \text{donde} \quad \Gamma^B \quad \text{es una de las 8 matrices que anticonmutan con} \quad \Gamma^B \]

\[tr \Gamma^A = -tr (\Gamma^B \Gamma^A \Gamma^B) = - tr \Gamma^A = 0\]

como

\[(\Gamma^A)^2 = \pm 1 \Rightarrow (\text{det} \Gamma^A)^2 = 1 \quad \text{det} \Gamma^A = \pm 1 \]


\item Independencia lineal

Sea $\sum_{A} C_{A} \Gamma^A = 0$ con $\Gamma^1 = I$

\[tr(\sum_{A} C_{A}\Gamma^A) = 4 C_{1} = 0 \quad \Rightarrow C_{1} = 0\]

multiplicando por $\Gamma_{e}$ para $e = 2,...,16$ y tomando la traza $C_{c} = 0$ con $c = 2,...,16$

\item Toda matriz $M$ es una combinación lineal de las $16 \Gamma^A$'s


Si $M = \Gamma_{A} m_{A} \Gamma^A$

\[tr(\Gamma_{B}M) = m_{b}4 \quad \Rightarrow m_{b} = \frac{1}{4} tr(\Gamma_{B}m) \quad \text{con} \quad B=1,...,16\]

\item Lema: si $[M, \gamma^{\mu} = 0 \forall \mu = 0, 1, 2, 3 $ entonces $M = cI$.

Sea $M = \sum _{B} \chi _{B} \Gamma^{B}$

\[= \chi_{A} \Gamma^A + \sum _{B \neq A} \chi _{B} \Gamma^B \quad \text{para alguna} A = 1,...,16 \quad \text{con} \quad \Gamma^A \neq I \quad \text{i.e.} \quad A = 2,...,16 \quad \text{formando} \Gamma^1 \ I\]

Sea $\Gamma^c$ alguna de las 8 matrices que anticonmutan con $\Gamma^A$. Por hipótesis $\Gamma_{c} M \Gamma^c = M$ con $(\Gamma_{c} \Gamma^c = I)$

\[= \Gamma_{c}(\chi _{A} \Gamma^{A} + \sum _{B \neq A} \chi _{B} \Gamma^B) \Gamma^c = M\]

\[= - \chi _{A} \Gamma ^{A} + \sum _{B \neq A} \chi _{B} \Gamma ^{B} (\pm )\]

\[\Rightarrow M \Gamma^A = \chi _{A} (\Gamma^A)^2 + \sum_{B \neq A} \chi_{B} \Gamma^B \Gamma^A \]

\[M \Gamma^A = - \chi _{A} (\Gamma^A)^2 + \sum_{B \neq A} (\pm) \chi_{B} \Gamma^B \Gamma^A \]

al tomar las trazas $\chi_{A} = - \chi _{A} = 0$ con $A = 2,...,16$

\[\therefore M = c I\]

\item Teorema fundamental: Sean 2 conjuntos de matrices: $\gamma^{\mu} y \gamma^{\mu \prime}$ que cumplen con el álgebra de Clifford, es decir

\[\gamma^{\mu} \gamma^{\nu} + \gamma^{\nu} \gamma^{\mu} = 2 \eta^{\mu\nu} \quad \text{y} \quad \gamma^{\mu \prime} \gamma^{\nu \prime} + \gamma^{\nu \prime} \gamma^{\mu \prime} = 2 g^{\mu\nu}\]

entonces, existe una matriz $S$ no singular (invertible) tal que

\begin{equation}
\gamma^{\prime \mu} = \delta \gamma^{\mu} S^{-1}
\end{equation}

\end{itemize}

\textbf{Demostración:} Sea $F \neq 0$ una matriz arbitraria no nula de $4 \times 4$, y sea $S = \sum \Gamma^ {\prime i} F \Gamma_{i}$ y que haga a $S$ invertible, donde las $\Gamma^{\prime i}$ se construyen de las $\gamma^{\prime \mu}$ de la misma manera que las $\Gamma^i$ se construyen de las $\gamma^{\mu}$; $\Gamma_{i}$ es la inversa de $\Gamma^i$ 

Sea $\Gamma_{j} \Gamma_{i} = \epsilon_{ij} \Gamma_{K}$ con $\epsilon_{ji} = \pm 1$ segun el caso

\[\Gamma_{j} \Gamma_{i} \Gamma_{j} \Gamma_{i} = (\epsilon_{ji})^2 (\Gamma_{k})^2 = \epsilon_{kk} \quad \text{según el caso}\]

\[\Gamma_{i} \Gamma_{j} = \epsilon_{kk} \Gamma^{j} \Gamma^{i} \Rightarrow\]

\[\Gamma_{j} \Gamma_{i} = \epsilon_{kk} \Gamma_{i} \Gamma_{k}\]

análogamente $\Gamma_{\prime j} \Gamma_{\prime i} = \epsilon_{kk} \Gamma_{j}^{\prime} \Gamma_{i}^{\prime}$

entonces 

\[\Gamma ^{'j} S \Gamma_{j} = \sum_{i} \Gamma^{\prime j} \Gamma ^{\prime i} F \Gamma_{i} \Gamma _{j}\]

\[\sum_{i} \epsilon _{kk} \Gamma_{i}^{\prime} F \Gamma_{i} \Gamma_{j}\]

\[\sum_{i \rightarrow k} \epsilon_{kk} \epsilon_{ij} \Gamma_{k}^{\prime} F \epsilon_{ij} \Gamma_{k}\]

\[\sum_{k} \epsilon_{kk} \Gamma_{k}^{\prime} F \Gamma_{k} \quad \text{pero} \quad (\Gamma_{k}^{\prime})^2 = \epsilon_{kk} = \Gamma_{k}^{\prime} \Gamma_{k}^{\prime}\]

\[ \Rightarrow \Gamma_{k}^{\prime} = \epsilon_{kk} \Gamma^{\prime k}\]

\[\sum_{k} \Gamma^{\prime k} F \Gamma_{k} = S\]

\[\Rightarrow \Gamma^{\prime j} S \Gamma_{j} = S \quad \Rightarrow \quad \Gamma^{\prime j} = S \Gamma^{j} S^{-1}\]

S es única (excepto por un factor). Sean $S_{1}$ y $S_{2}$ tal que $\gamma^{\prime \mu} = S_{1} \gamma^{\mu} S_{1}^{-1}$ y $\gamma^{\prime \mu} = S_{2} \gamma^{\mu} S_{2}^{-1}$

\[\Rightarrow S_{1} \gamma^{\mu} S_{1}^{-1} = S_{2} \gamma^{\mu} S_{2}^{-1}\]

\[S_{2}^{-1} S_{1} \gamma^{\mu} = \gamma^{\mu} S_{2}^{-1}S_{1} \quad \Rightarrow \quad S_{2}^{-1}S_{1} = \text{cte}I\]

\[ \Rightarrow S_{1} = \text{cte}S_{2}\]

Usualmente se pide que $det S = 1$

\textbf{Corolario:} Si $\gamma^{\prime \mu} = \gamma^{\mu}$

\[\gamma^{\prime \mu} = S \gamma^{\mu} S^{-1} \]

\[\gamma^{\prime \mu} S = S \gamma^{\mu} \quad \Rightarrow \quad \gamma^{\mu} S = S \gamma^{\mu}\]

\[S = \text{cte} I\]

\[c I = \sum \Gamma^{i} F \Gamma_{i} \text{i.e.}\]

\begin{equation}
C \delta _{\rho \sigma} = \sum_{i}^{16} \Gamma_{\rho \rho}^{i} F_{\rho^{\prime} \sigma^{\prime}} (\Gamma)_{\sigma^{\prime} \sigma}
\end{equation}

como $F$ es arbitraria $\Rightarrow \sum_{i = 1}^{16} \Gamma_{\rho \rho^{\prime}}^{i}(\Gamma_{i})_{\sigma^{\prime}\sigma} = b _{\rho^{\prime}\sigma^{\prime}} \delta_{\rho \sigma}$

\[\text{Si} \quad \rho = \sigma 4 b_{\rho ^{\prime} \sigma^{\prime}} = \sum_{i = 1} ^{16} (\Gamma_{i})_{\sigma^{\prime} \sigma} \Gamma_{\sigma \rho ^{\prime}} ^{i} = \sum _{i =1} ^{16} \delta _{\sigma ^{\prime} \rho^{\prime}} = 16 \delta _{\sigma^{\prime} \rho^{\prime}}\]

\[b _{\rho^{\prime} \sigma^{\prime}} = 4 \delta _{\sigma^{\prime} \rho^{\prime}}\]

\[\Rightarrow \sum_{i = 1}^{16} \Gamma_{\rho \rho^{\prime}}^{i} (\Gamma_{i})_{\sigma^{\prime} \sigma} = 4 \delta_{\rho \sigma} \delta_{\sigma^{\prime} \rho^{\prime}}\]

Corriente: $\overline{\psi} \gamma^{\mu} \psi$

Sea $\overline{\psi} \equiv \psi ^{\dagger} \gamma^0$ con $\delta^{\mu \dagger } = \gamma^0 \gamma^{\mu} \gamma^{0}$

\[[i \gamma^{\mu} (\gamma_{\mu} + i e A_{\mu}) - m_{0}] \psi = 0 \quad \Rightarrow i \gamma^{\mu} \partial_{\mu} \psi = (e \gamma^{\mu} A_{\mu} + m_0) \psi\]

\[ \Rightarrow \psi^{\dagger} [-i \gamma^{\mu \dagger}(\partial_{\mu}) - i e A_{\mu} - m_0] =0 \]

\[\overline{\psi} \gamma^0 [-i \gamma^{\mu \dagger} (\partial_{\mu} - ieA_{\mu})-m_0 ]= 0\]

\[\overline{\psi} [-i \gamma^{\mu} (\partial_{\mu} - ieA_{\mu})-m_0 ]= 0\]

\[(-i \partial_{\mu} - e A_{\mu})\overline{\psi} \gamma^{\mu} - m_{0} \overline{\psi} = 0\]

\[-i \partial_{\mu} \overline{\psi} \gamma^{\mu} = m_0 \overline{\psi} + e A_{\mu} \overline{\psi} \gamma^{\mu}\]
\begin{equation}
= \overline{\psi}(m_0 + e A_{\mu} \gamma^{\mu})
\end{equation}

\[\partial_{\mu} (\overline{\psi} \gamma^{\mu} \psi) = (\partial_{\mu} \overline{\psi} \gamma^{\mu}) \psi + \overline{\psi} \gamma^{\mu} \partial_{\mu} \psi\]

\[= i \overline{\psi} (m_0 + e A_{\mu} \gamma^{\mu}) \psi - i \overline{\psi} (e \gamma^{\mu} A_{\mu} + m_0) \psi = 0\]

\[= i e [A_{\mu} \overline{\psi} \gamma^{\mu} \psi - A_{\mu} \overline{\psi} \gamma^{\mu} \psi ] = 0\]

Nótese que $j^{\mu} = (\rho, \overrightarrow{j})$ se trasforma como

\[j^{1} = j _{x} \quad j^{2} = j _{y} \quad j^{3} = j _{z} \quad j^{0} = \rho \]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Invariancia relativista}


\begin{equation}
[i \gamma^{\mu} (\partial_{\mu} + ieA_{\mu})-m_0] \psi (x) = 0
\end{equation}

Se transforma como 

\[x^{\mu} \rightarrow^{L} \quad x^{\prime \mu} = \Lambda^{mu}_{\nu} x^{\nu} \]

con $\Lambda_{\nu}^{\mu}$ una transformación de Lorentz

\[x^{\prime 0} = \gamma(x^{0}-\beta x^{1})\]

\[x^{\prime 1} = \gamma(x^{0}-\beta x^{0})\]

\[x^{\prime 2} = x^{2}\]

\[x^{\prime 3} = x^{3}\]

Siendo la matriz de transformación

\[\Lambda_{\nu}^{\mu} = \left ( \begin{array}{cccc}
 \gamma & -\beta \gamma & 0& 0 \\
 -\beta \gamma & \gamma & 0 &0 \\
0 & 0 & 1 &0 \\
0 & 0 & 0 &1 \\ \end{array} \right) 
\]

también se puede ver cómo se transforma la función de onda bajo Lorentz, es decir, se obtiene un espinor

\[\Lambda_{\nu}^{\mu} = \left ( \begin{array}{cccc}
 \psi_{1}(x)  \\
  \psi_{2}(x)  \\
 \psi_{3}(x)  \\
 \psi_{4}(x) \\ \end{array} \right) 
\]

\[\psi (x) \rightarrow^{L} \psi^{\prime}(x^{\prime}) = ? \]

¿Cómo se transforma?

\[L \rightarrow [i \gamma^{\mu} (\partial_{\mu}^{\prime} + ie A_{\mu}^{\prime} (x^{\prime}))-m_0] \psi^{\prime}(x^{\prime}) = 0\]

\[x^{\mu} \rightarrow x^{\prime \mu} = \Lambda_{\nu}^{\mu} x^{\nu}\]

\[\Lambda _{\mu}^{\rho} x^{\prime \mu} = \Lambda_{\mu}^{\rho} \Lambda_{\nu}^{\mu} x^{\nu} = \delta_{\nu}^{\rho} x^{\nu} = x^{\rho}\]

nótese que el término $\Lambda_{\mu}^{\rho} = (\Lambda_{\mu}^{\rho})^{-1}$ entonces 

\[x^{\mu} = \Lambda_{\rho}^{\mu} x^{\prime \rho}\]

\[x_{\mu} = \Lambda_{\mu}^{\rho} x_{\rho}^{\mu}\]

\begin{equation}
\partial_{\mu} = \Lambda_{\mu}^{\rho} \partial_{\rho}^{\prime}
\end{equation}

con $\partial_{\rho}^{\prime} \equiv \frac{\partial}{\partial x^{\prime \rho}}$

\begin{equation}
x^{\prime \mu} = \Lambda_{\rho}^{\mu} x^{\rho}
\end{equation}

\begin{equation}
[i \gamma^{\mu} (\Lambda_{\mu}^{\rho} \partial_{\rho}^{\prime} + ie \Lambda_{\mu}^{\rho} A_{\rho}^{\prime}(x^{\prime}))-m_0]\psi = 0
\end{equation}


con $\psi (x) = \psi (L^{-1} x^{\prime})$ corresponde al último término de lado izquierdo de la ecuación

\begin{equation}
i \gamma^{\mu} \Lambda_{\mu}^{\rho} (\partial_{\rho}^{\prime} + ieA_{\rho}^{\prime} (x^{\prime}))-m_0] \psi (L^{-1}x^{\prime})=0
\end{equation}


con $\psi(x)$ corresponde al último término de lado izquierdo de la ecuación

\[\hat{\gamma}^{\mu} \equiv \Lambda_{\rho}^{\mu} \gamma^{\rho}\]

\[\hat{\gamma}^{\mu} \hat{\gamma}^{\rho} + \hat{\gamma}^{\rho} \hat{\gamma}^{\mu} = \Lambda_{\sigma}^{\mu} \Lambda_{\lambda}^{\rho} \gamma^{\sigma} \gamma^{\lambda} + \Lambda_{\lambda}^{\rho}\Lambda_{\sigma}^{\mu}\gamma^{\lambda}\gamma^{\sigma}\]

\[= \Lambda_{\sigma}^{\mu} \Lambda_{\lambda}^{\rho} (\gamma^{\sigma} \gamma^{\lambda} + \gamma^{\lambda} \gamma^{\sigma})\]

\begin{equation}
= 2 \Lambda_{\sigma}^{\mu} \Lambda_{\lambda}^{\rho} g^{\sigma \lambda}
\end{equation}

nótese que $(\gamma^{\sigma} \gamma^{\lambda} + \gamma^{\lambda} \gamma^{\sigma}) = 2g^{\sigma \lambda}$; pero $ \Lambda g \Lambda^{t} = \Lambda^{t} g \Lambda = g$ entonces la ecuación (3.6) resulta

\begin{equation}
2 g^{\mu \rho}
\end{equation}

por tanto, existe una matriz transformación $\Lambda$ tal que

\begin{equation}
\hat{\gamma}^{\mu} = \Lambda^{-1} \gamma^{\mu} \Lambda = \Lambda_{\rho}^{\mu} \gamma^{\rho}
\end{equation}

\[i \Lambda^{-1} \gamma^{\mu} \Lambda (\partial_{\rho}^{\prime} + ie A_{\rho}^{\prime} (x^{\prime}))-m_0] \psi(x)=0\]

\[i \gamma^{\mu} (\partial_{\rho}^{\prime} + i eA_{\rho}^{\prime} (x^{\prime}))-m_0] \Lambda_{0}\psi (x) = 0 \]

\[\psi^{\prime} (x^{\prime}) \equiv \Lambda \psi (x) \quad \psi (x) \rightarrow \psi ^{\prime} (x^{\prime}) \equiv \Lambda \psi (x)\]

obviamente $\psi^{\dagger} (x) \beta  \equiv \overline{\psi} (x) \rightarrow \psi ^{\dagger} \Lambda^{\dagger} \beta$ con $\beta = \gamma^{0}$

\[\Lambda_{\rho}^{\mu} \gamma_{\rho}^{\mu} = \Lambda^{\dagger} \gamma^{\mu \dagger}(\Lambda^{-1})^{\dagger}\]

\[\Lambda_{\rho}^{\mu} \gamma^{\rho} = \gamma^{0} \Lambda^{\dagger} \gamma^{0} \gamma^{\mu} \gamma^0 (\Lambda^{-1})^{\dagger} \gamma^0 = \Lambda^{-1} \gamma^{\mu} \Lambda\]

\[\gamma^0 \Lambda^{\dagger} \gamma^0 = \Lambda^{-1}\]

\[\Lambda^{\dagger} = \gamma^0 \Lambda^{-1} \gamma^0\]

\[\overline{\psi} (x) \rightarrow \overline{\psi}^{\prime} (x^{\prime}) = \psi ^{\dagger} \gamma^0 \Lambda^{-1} \gamma^0 \gamma^0\]

\begin{equation}
= \overline{\psi} \Lambda^{-1}
\end{equation}

¿Cómo se transforma $j^{\mu}$?

\[j^{\mu} = \overline{\psi} \gamma^{\mu} \psi \rightarrow \overline{\psi} \Lambda^{-1} \gamma^{\mu} \Lambda \psi = \overline{\psi} \Lambda_{\nu} ^{\mu} \gamma^{\nu} \psi\]

\[= \Lambda_{\nu}^{\mu} \overline{\psi} \gamma^{\nu} \psi = \Lambda_{\nu}^{\mu} j^{\nu}\]

con

\begin{equation}
j^{\mu} (x) \rightarrow j^{\mu ^{\prime}} (x^{\prime}) = \Lambda_{\nu}^{\mu} j^{\nu} (x)
\end{equation}

que transforma como un cuadrivector y $\overline{\psi} (x) \gamma^{\mu} \gamma^{\nu} \psi (x)$ se transforma como un tensor.

\[\overline{\psi} (x) \psi (x) \rightarrow \overline{\psi}(x)\Lambda^{-1} \Lambda \psi (x) = \overline{\psi} (x) \psi (x)\]

\textbf{Definición}: Transformaciones infinitesimales propias.

Sea $x^{\mu} \rightarrow \Lambda_{\nu}^{\mu} x^{\nu}$

\begin{equation}
\Lambda^{(\alpha \beta) \mu} _{\nu} = \delta_{\nu}^{\mu} + \epsilon \mathscr{m}^{(\alpha \beta) \mu}_{\nu}
\end{equation}

con $\mathscr{m}^{(\alpha \beta)}$ el generador

\[\text{i.e.} \quad \Lambda^{(\alpha \beta)}_{(\epsilon)} =  I + \epsilon \mathscr{m}^{(\alpha \beta)}\]

\[\Lambda_{(\theta)}^{(\alpha \beta)} = \lim _{n \to \infty} \Lambda^{(\alpha \beta)} (n \theta /n) = (I + \theta/n) \mathscr{m}^{(\alpha \beta)}) (I + \theta / n \mathscr{m}^{(\alpha \beta)})\]

se expande $n$ veces tal que

\begin{equation}
= \lim_{n \to \infty} (I + \theta n \mathscr{m}^{(\alpha \beta) n} = e^{\mathscr{m}^{(\alpha \beta)} \theta} = \Lambda_{(\theta)}^{(\alpha \beta)}
\end{equation}

\[\Rightarrow \mathscr{m}^{(\alpha \beta)} = \frac{de^{(\alpha \beta)}}{d \theta} |_{\theta = 0}\]

Sea la matriz transformación

\[\Lambda_{(\theta)}^{01} = \left ( \begin{array}{cccc}
 \gamma & -\beta \gamma & 0& 0 \\
 -\beta \gamma & \gamma & 0 &0 \\
0 & 0 & 1 &0 \\
0 & 0 & 0 &1 \\ \end{array} \right) 
\]

con

\[\sinh x = \frac{e^x - e^{-x}}{2}\]

\[\cosh = \frac{e^x + e^{-x}}{2}\]

\[\tgh \theta = \beta\]

\[\cos^2 h x = \sin^2 hx = 1\]

Utilizando la identidad $\frac{\sinh \theta}{\cosh \theta}$

\[\sinh \theta = \beta \cosh \theta\]

\[\sin^2 h \theta = \beta^{2} cos^2 h \theta = \beta^2 (1+sin^2 h \theta)\]

\[\sin^2 h \theta (1- \beta^2) = \beta^2\]

\[\sinh \theta = \frac{\beta}{\sqrt{1-\beta^2}} = \gamma \beta\]

\[\Rightarrow \cosh \theta = \gamma\]

\[\Lambda_{(\theta)}^{01} = \left ( \begin{array}{cccc}
 \cosh \theta & -\sinh \theta & 0& 0 \\
 -\sinh \theta & \cosh \theta & 0 &0 \\
0 & 0 & 1 &0 \\
0 & 0 & 0 &1 \\ \end{array} \right) 
\]

\[\frac{d}{d \theta} [\Lambda_{(\theta)}^{01}]|_{\theta = 0} = \left ( \begin{array}{cccc}
 \sinh \theta & -\cosh \theta & 0& 0 \\
 -\cosh \theta & \sinh \theta  & 0 &0 \\
0 & 0 & 0 &0 \\
0 & 0 & 0 &0 \\ \end{array} \right) |_{\theta = 0} = \left ( \begin{array}{cccc}
 0 & -1 & 0& 0 \\
 -1 & 0  & 0 &0 \\
0 & 0 & 0 &0 \\
0 & 0 & 0 &0 \\ \end{array} \right)
\]

\[= \mathscr{m}^{01}\]

análogamente

\[\Lambda_{(\theta)}^{02} = \left ( \begin{array}{cccc}
 \gamma & 0 & -\beta \gamma & 0 \\
 0 & 1 & 0 &0 \\
-\beta \gamma & 0 & 0 &0 \\
0 & 0 & 0 &1 \\ \end{array} \right) 
\]

esto es

\[\mathscr{m}^{02} = \left ( \begin{array}{cccc}
 0 & 0 & 1 & 0 \\
 0 & 0 & 0 &0 \\
-1 & 0 & 0 &0 \\
0 & 0 & 0 &0 \\ \end{array} \right) 
\]

etc. Ahora, consideremos una rotación alrededor del eje $z$


\[\Lambda_{(\theta)}^{12} = \left ( \begin{array}{cccc}
 1 & 0 & 0 & 0 \\
 0 & \cos \theta & \sin \theta &0 \\
 0 & -\sin \theta & \cos \theta &0 \\
 0 & 0 & 0 & 1 \\ \end{array} \right) 
\]

\[\frac{d}{d \theta}[\Lambda^{12}(\theta)]|_{\theta = 0} = \left ( \begin{array}{cccc}
 0 & 0 & 0 & 0 \\
 0 & 0 & 1 &0 \\
 0 & -1 & 0 &0 \\
0 & 0 & 0 &0 \\ \end{array} \right) = \mathscr{m}^{12}
\]

se define

\begin{equation}
\mathscr{m}^{\alpha \beta} \equiv - \mathscr{m}^{\beta \alpha}
\end{equation}

Se puede demostrar (3.13) fácilmente

\[\mathscr{m}^{01} = \left ( \begin{array}{cccc}
 0 & -1 & 0 & 0 \\
-1 & 0 & 0 &0 \\
0 & 0 & 0 &0 \\
0 & 0 & 0 &0 \\ \end{array} \right) \quad \mathsrc{m}^{02} \left ( \begin{array}{cccc}
 0 & 0 & -1 & 0 \\
 0 & 0 & 0 &0 \\
 -1 & 0 & 0 &0 \\
0 & 0 & 0 &0 \\ \end{array} \right)
\]

\[\mathscr{m}^{03} = \left ( \begin{array}{cccc}
 0 & 0 & 0 & -1 \\
 0 & 0 & 0 &0 \\
 0 & 0 & 0 &0 \\
 -1 & 0 & 0 &0 \\ \end{array} \right) \quad \mathsrc{m}^{12} \left ( \begin{array}{cccc}
 0 & 0 & 0 & 0 \\
 0 & 0 & 1 & 0 \\
 0 & -1 & 0 &0 \\
0 & 0 & 0 &0 \\ \end{array} \right)
\]

\[\mathscr{m}^{31} = \left ( \begin{array}{cccc}
 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 0 &0 \\
0 & 0 & 0 &0 \\ \end{array} \right) \quad \mathsrc{m}^{23} \left ( \begin{array}{cccc}
 0 & 0 & 0 & 0 \\
 0 & 0 & 0 &0 \\
 0 & 0 & 0 &1 \\
0 & 0 & -1 &0 \\ \end{array} \right)
\]

Explícitamente

\begin{equation}
\mathsrc{m}^{(\alpha \beta)\mu}_{\nu} = A \delta^{\alpha}_{\nu} g^{\beta \mu} + B g^{\alpha \mu} \delta^{\beta}_{\nu}
\end{equation}

si se considera

\[(\mathsrc{m}^{01})^{1}_{0} = -1 = A \delta_{0}^{0} g^{11} \quad \Rightarrow A = 1\]

\[(\mathsrc{m}^{01})^{0}_{1} = -1 = B g^{00} \delta_{1}^{1} \quad \Rightarrow A = 1\]

\begin{equation}
\mathsrc{m}^{(\alpha \beta) \mu}_{\nu} = \delta_{\nu}^{\alpha} g^{\beta \mu} - g^{\alpha \mu} \delta_{\nu}^{\beta}
\end{equation}

Si ahora se considera la siguiente transformación infinitesimal

\[x^{\mu} \rightarrow \Lambda_{(\epsilon)\nu}^{(\alpha \beta)\mu} x^{\nu} \]

con

\begin{equation}
\Lambda_{(\epsilon)\nu}^{(\alpha \beta)\mu} = \delta_{\nu}^{\mu} + \epsilon \mathsrc{m}^{(\alpha \beta)\mu}_{\nu}
\end{equation}

con el último término de lado derecho de la ecuación como el generador de la representación vectorial

\[\hat{\gamma}^{\mu} \equiv \Lambda_{(\epsilon)\nu}^{(\alpha \beta)\mu} \gamma^{\nu} = \Lambda^{-1} \gamma^{\mu} \Lambda\]

y dicha transformación

\[\psi (x) \rightarrow \psi^{\prime} (x^{\prime}) = \Lambda \psi (x)\]

esto es

\[\Lambda = I + i\epsilon S^{(\alpha \beta)}\]

el generador de la representación espinorial. Se tiene, entonces

\[(\gamma^{\mu}_{\nu} + \epsilon \mathsrc{m}^{(\alpha \beta) \mu}_{\nu})\gamma^{\nu} = (I - i \epsilon S^{(\alpha \beta)})\gamma^{\mu} (I + i \epsilon S^{(\alpha \beta)}) \gamma^{\mu} + \epsilon i \gamma^{\mu} S^{(\alpha \beta)} \gamma^{\mu}\]

\[+ \epsilon[\delta^{\alpha}_{\nu} g^{\beta \mu} - g^{\alpha \beta} - g^{\alpha \mu} \delta^{\beta}_{\nu}]\gamma^{\nu}\]

\[= \gamma^{\mu} + \epsilon i \gamma^{\mu} S^{(\alpha \beta)} - i \epsilon S^{(\alpha \beta)} \gamma^{\mu}\]

a primer orden en $\epsilon$

\[\Rightarrow \gamma^{\alpha} g^{\beta \mu} - g^{\alpha \mu} \gamma^{\beta} = i (\gamma^{\mu} S^{(\alpha \beta)} - S^{(\alpha \beta)} \gamma^{\mu})\]

claramente se puede ver como el conmutador

\[[\gamma^{\mu},s^{(\alpha \beta)} = i [\gamma^{\beta} g^{\alpha \mu} - \gamma^{\alpha} g^{\beta \mu}\]



\[= i g^{\mu \alpha} \gamma^{\beta} - i \gamma^{\alpha} g^{\beta \mu} - \gamma^{\alpha} (\alpha g^{\mu \beta} - \gamma^{\beta} \gamma^{\mu}) \]

\[=i[\gamma^{\beta} g^{\alpha \mu} - \gamma^{\alpha} g^{\beta \mu}\]

entonces

\[S^{\alpha \beta} = \frac{i}{2} \gamma^{\alpha} \gamma^{\beta}\]

\begin{equation}
= \frac{i}{4} [\gamma^{\alpha}, \gamma^{\beta}]
\end{equation}

\[\Lambda_{\epsilon} \equiv I + i \epsilon (\frac{i}{4} [\gamma^{\alpha} , \gamma^{\beta}])\]

siendo

\[\Lambda^{(\alpha \beta)}_{(\theta)} = e^{i \theta S^{\alpha \beta}}\] 

con $S^{\alpha \beta} = \frac{i}{4} [\gamma^{\alpha}, \gamma^{\beta}]$


Si se toma 

\begin{equation}
\Lambda^{(01)}_{(\phi)} = \Lambda^{(tx)}_{(\phi)} = e^{i \psi S^{(01)}}
\end{equation}

\[S^{(01)} = \frac{i}{2} \gamma^{0} \gamma^1 = \frac{i}{2} \alpha^{\prime} = \frac{i}{2} \alpha_{x}\]

\[\Lambda^{(tx)}_{(\psi)} = e^{- \frac{\phi}{2} \alpha_{x}} = \sum_{n = 0}^{\infty}(-\frac{\phi}{2} \alpha_{x})^n \frac{1}{n!} \]

con $(\alpha_{x})^{2n} = 1$ y $(\alpha_{x})^{2n+1} = \alpha_{x}$

\[= 1 + (-\frac{\phi}{2})^2 \frac{1}{2!}+ (-\frac{\phi}{2})^4 \frac{1}{4!} + \ldots -[\frac{\phi}{2} + (\frac{\phi}{2})^3 \frac{1}{3!}+ \ldots \]

\[= \cosh \frac{\phi}{2} - \alpha_{x} \sinh \frac{\phi}{2}\]

haciendo el cambio $\tgh = \beta = v$. En general

\begin{equation}
\Lambda(\overrightarrow{v}) = \cosh \frac{\phi}{2} - \overrightarrow{\alpha} \dot{ \hat{n}} \sinh \frac{\phi}{2}
\end{equation}

como $\sinh \phi = \gamma \beta$ y $\cosh \phi = \gamma$ se tiene, además $\hat{n} \equiv \frac{\overrightarrow{v}}{v}$

\[\gamma = \cosh \phi = \cosh (\frac{\phi}{2} + \frac{\phi}{2}) = \cos^2 h \phi/2 + \sin ^2 h \phi/2\]

\[\cos ^2h \phi/2 - \sin^2 h \phi/2 = 1 = 2 \cos^2h \phi/2 - 1 = \gamma \]

\[\gamma v = \sin h \phi = \sinh (\frac{\phi}{2} + \frac{\phi}{2}) = 2 \sinh \phi / 2 \cosh \phi / 2\]

\[2 \cosh^2 \phi / 2 = \gamma + 1\]

\begin{equation}
\Rightarrow \cosh \phi/2 = (\frac{1+\gamma}{2})^{1/2}
\end{equation}

\[\gamma v = 2 \sinh \phi / 2 (\frac{1+\gamma}{2})^{1/2}\]

\begin{equation}
\Rightarrow \sinh \phi/2 = \frac{1}{2} (\frac{2}{1+\gamma})^{1/2} \gamma v
\end{equation}

\[\Lambda(\overrightarrow{v}) = (\frac{1+\gamma}{2})^{1/2} [1-\frac{\overrightarrow{v} \cdot \overrightarrow{\alpha}}{v} \frac{1}{2} (\frac{2}{1+\gamma})\gamma v]\]

\[= \sqrt{\frac{e+m_0}{2m_0}}[1-\frac{\overrightarrow{\alpha} \cdot \overrightarrow{P}}{E+m_0}\]

\begin{equation}
= \frac{1}{\sqrt{2()1+\gamma}} [(1+ \gamma) - \overrightarrow{\alpha} \cdot \overrightarrow{v} \gamma]
\end{equation}

\section{Rotaciones Espaciales}

\[\Lambda^{(xy)}_{(\phi)} = e^{i S^{(12)}\psi}\]

\begin{equation}
= e^{i \sigma_{z/2} \phi} = \cos \phi/2 + i \sigma_{z} \sin \phi/2
\end{equation}

con

\[S^{(12)} = \frac{i}{2} \gamma^1 \gamma^2 = \frac{i}{2} \beta \alpha_x \beta \alpha_y\]

\begin{equation}
= \frac{-i/2}{2}\alpha_x \alpha_y  = \sigma_{z/2}
\end{equation}

en general $\Lambda_{(\phi)} = \cos \frac{\phi}{2} + i \overrightarrow{\sigma} \cdot \hat{n} \sin \frac{\phi}{2}$. Nótese además que

\begin{equation}
\Lambda_{(2n\pi)} = (-)^{n}I
\end{equation}

\section{Reflexiones}

Se define una reflexi\'on $\overrightarrow{r} \rightarrow -\overrightarrow{r}$

\begin{equation}
\psi (t,\overrightarrow{r})\rightarrow^{R} \psi^{\prime}(t,-\overrightarrow{r}) = \Lambda_{S} \psi (t,\overrightarrow{r})
\end{equation}

\[\psi^{\prime} (t, \overrightarrow{r}) = \Lambda_{S} \psi (t,- \overrightarrow{r})\]

\[[\Lambda^{\mu}_{s \nu} = \left ( \begin{array}{cccc}
 1 & 0 & 0 & 0 \\
 0 & -1 & 0 &0 \\
 0 & 0 & -1 & 0 \\
0 & 0 & 0 & -1 \\ \end{array} \right) = \mathscr{m}^{12}
\]

\[\Lambda_{\nu}^{\mu} \gamma^{\nu} = \Lambda_{s}^{-1} \gamma^{\mu} \Lambda , \quad p\]

\[ \Rightarrow
  \begin{cases}
    \gamma^0 = \Lambda_{S}^{-1} \gamma^0 \Lambda\\
    -\gamma^i = \Lambda_{S}^{-1} \gamma^0 \Lambda\\
  \end{cases}
  \Lambda_{S} = \beta = \gamma^0 \]

como $\gamma^5 \gamma^{\mu} + \gamma^{\mu} \gamma^{5} = 0$

\[\overline{\psi} \gamma^5 \psi \rightarrow^{L} \overline{\psi} \Lambda^{-1} \gamma^5 \Lambda \psi = \overline{\psi} \gamma^5 \psi \]

sabemos que

\[\Lambda \equiv e^{iS^{(\alpha \beta)}} \quad \text{además con} \quad S^{(\alpha \beta)} = \frac{i}{4} [\gamma^{\alpha}, \gamma^{\beta}]\]

se obtiene entonces, la reflexión

\begin{equation}
\overline{\psi} \gamma^{5} \psi \rightarrow^{\text{reflex}} \overline{\psi} \beta \gamma^5 \beta \psi = - \overline{\psi} \gamma^{5} \psi
\end{equation}

\textbf{Tarea:} Demostrar que $\overline{\psi} \gamma^{\mu} \gamma^{5} \psi$ se comportan como un vector axial.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Soluciones a la ecuaci\'on de Dirac (caso libre)}

Del hamiltoniano libre de Dirac

\begin{equation}
H_{D} = \overrightarrow{\alpha} \cdot \overrightarrow{P} + \beta m_0
\end{equation}

y definiendo las componentes de las matrices $\sigma_{i}$ como

\[\sigma_x = -i \alpha_y \alpha_z \]

\[\sigma_y = -i \alpha_z \alpha_x \]

\[\sigma_z = -i \alpha_x \alpha_y \]

con

\[\rho _1 = -i \alpha_x \alpha_y \alpha_z\]

esto es

\[\rho_1 \sigma_x = (-i \alpha_x \alpha_y \alpha_z) (-i \alpha_y \alpha_z) = + \alpha_x \quad \text{etc.}\]

es decir $\overrightarrow{\alpha} = \rho_1 \overrightarrow{\sigma}$ se puede reescribir el Hamiltoniano libre de Dirac

\begin{equation}
H_{D} = \rho_1 \overrightarrow{\sigma} \cdot \overrightarrow{P} + \beta m_0
\end{equation}

donde $\rho$ y $\overrightarrow{\sigma}$ conmutan. Si $\psi$ es tal que $H_{D} \psi = E \psi$

\[\rho_1 (\overrightarrow{\sigma} \cdot \overrightarrow{P} + \beta m_0) \psi = E \psi \]

\begin{equation}
H_D \psi = [\rho_1 (\overrightarrow{\sigma} \cdot \overrightarrow{P}) + \rho_3 m_0] \psi = E \psi
\end{equation}

\[(\rho_2 \psi) \quad \text{como} \quad \rho_2 H_D = - H_D \rho_2 \quad \lbrace \rho_2 , H_D \rbrace = 0\]

\[H_D(\rho_2 \psi) = - \rho_2 H_D \psi = -E \psi\]

\[\text{Si} \quad E = + \sqrt{\overrightarrow{P}^2 + m_{0}^{2}} \quad \text{de} \quad \psi\]

\[E = - \sqrt{\overrightarrow{P}^2 + m_{0}^{2}} \quad \text{de} \quad \rho_2 \psi\]

de donde se obtienen energías negativas.

\[[i \gamma^{\mu} (\partial_{\mu} + ie A_{\mu}) - m_0] \psi = 0\]

\[[i \gamma^{\mu}(\partial_{\mu} - ie A_{\mu})-m_0] \psi_c = 0\]

usando $K = c \gamma^0$

\[[-i \gamma^{\mu} (\partial_{\mu} -ieA_{\mu})-m_0] \psi^{*} = 0 \]

\[[i K(- \gamma^{\mu}) K^{-1}(\partial_{\mu} - ieA_{\mu})-m_0] K \psi^{*}\]

nótese que el último término de la ecuación es equivalente a $\psi_c$ con

\[\psi_{c} = C \gamma^0 \psi^{*}\]

\[C \gamma^0 \gamma^{\mu *} \gamma^{0} C^{-1} = - \gamma^{\mu}\]

\[C \gamma^{\mu t} C^{-1} = - \gamma^{\mu}\]

\[ \overrightarrow{\alpha} = \left ( \begin{array}{cc}
 0 & \overrightarrow{\sigma}  \\
 \overrightarrow{\sigma} & 0  \\
 \end{array} \right) = \mathscr{m}^{12}
\]

\begin{equation}
- \gamma^{\mu t}  = C^{-1} \gamma^{\mu} C
\end{equation}

\[\gamma^i = \left ( \begin{array}{cc}
 0 & \sigma^{i}  \\
 -\sigma^{-i} & 0  \\
 \end{array} \right) 
\]

con las matrices de Pauli:
 \[\sigma^1 = \left ( \begin{array}{cc}
 0 & 1  \\
 1 & 0  \\
 \end{array} \right)
\quad \sigma^2 = \left ( \begin{array}{cc}
 0 & -i  \\
 i & 0  \\
 \end{array} \right)
 \quad \sigma^3 \left ( \begin{array}{cc}
 1 & 0  \\
 0 & -1  \\
 \end{array} \right)\]

\[\beta = \left ( \begin{array}{cc}
 1 & 0  \\
 0 & -1  \\
 \end{array} \right) 
\]

\[\gamma^{1} = \left ( \begin{array}{cccc}
 0 & 0 & 0 & 1 \\
 0 & 0 & 1 & 0 \\
 0 & -1 & 0 & 0 \\
-1 & 0 & 0 & 0 \\ \end{array} \right) 
 \quad \gamma^{\Delta t} = -\gamma^1
\]

\[\gamma^{2} = \left ( \begin{array}{cccc}
 0 & 0 & 0 & -i \\
 0 & 0 & i & 0 \\
 0 & i & 0 & 0 \\
-i & 0 & 0 & 0 \\ \end{array} \right) 
 \quad \gamma^{2 t} = -\gamma^2 \quad \gamma^{0t} = \gamma^0
\]

\[\gamma^{3} = \left ( \begin{array}{cccc}
 0 & 0 & 1 & 0 \\
 0 & 0 & 0 & -1 \\
 -1 & 0 & 0 & 0 \\
 0 & 1 & 0 & 0 \\ \end{array} \right) 
 \quad \gamma^{3 t} = -\gamma^3
\]

con $C = i \gamma^{2}\gamma^0$ Para que $C$ sea unitaria.

\section{Soluci\'on de la ecuaci\'on de Dirac. Caso $A^{\mu} = 0$}

Del hamiltoniano libre de Dirac

\begin{equation}
H_{D} = \overrightarrow{\alpha} \cdot \overrightarrow{P} + \beta m_{0} = \rho_1 \overrightarrow{\sigma} \cdot \overrightarrow{P} + \rho_3 m_0
\end{equation}

de la definición de momento angular $vec{L} = \vec{r} \times \vec{P}$ esto es $\vec{L} = 0$

se debe poner atención a 

\[[\overrightarrow{J}, H_D]=0\]

y, por tanto

\[[\overrightarrow{J}^{2},H_D] = 0 \quad \text{de tarea demostrar que:} \quad [\overrightarrow{J}_z,H_D]=0 \quad \text{tal que} \quad  \overrightarrow{P}||\hat{z}\] 

además

\[[S^2, H_D]=0 \Rightarrow \quad \text{el esp\'in se conserva.}\]

Para este caso se tienen 4 soluciones a la ecuación de Dirac:

\[(E_+ , \frac{1}{2}) \quad , \quad (E_+ ,-\frac{1}{2}) \quad , \quad (E_- , \frac{1}{2}) \quad , \quad (E_- , - \frac{1}{2})\]

con el segundo término de cada paréntesis correspondiente al espín en la dirección del eje $Z$, esto es $S_{z} = \pm 1/2$

\[\psi^{r}(x) = u (\overrightarrow{P}) e^{-i \epsilon_{r} p^{\mu} \chi _{\mu}}\]

\[\epsilon_{r} = \pm 1  =
  \begin{cases}
    +1       & \quad \text{si }r=1,2\\
    -1  & \quad \text{si } r = 3,4\\
  \end{cases} \quad P^{\mu} \chi_{\mu} = E_{+}t-\overrightarrow{P} \cdot \overrightarrow{x}
\]

desde el sistema del centro de masa de la partícula

\[\psi^{r}(x) = u^{r}(0)e^{-i\epsilon_{r} m_{0}t}\]

\[P^{\mu} = (m_{0}, \overrightarrow{P}=0)\]

\[m_0 \beta u^{r}(0) = E u^{r}(0) = \epsilon_{r}m_0 u^{r}(0)\]



\[\text{como} \quad \beta = \left (\begin{array}{cc}
 1 & 0  \\
 0 & -1  \\
 \end{array} \right) \Rightarrow E = \pm m_0 \]

\[u^1 (0) = \left ( \begin{array}{cccc}
 1   \\
 0   \\
 0   \\
 0   \\
 \end{array} \right) \quad u^2 (0) = \left (\begin{array}{cccc}
 0   \\
 1   \\
 0   \\
 0   \\
 \end{array} \right) \quad u^3 (0) = \left ( \begin{array}{cccc}
 0   \\
 0   \\
 1   \\
 0   \\
 \end{array} \right) \quad u^{4}(0) = \left ( \begin{array}{cccc}
 0   \\
 0   \\
 0   \\
 1   \\
 \end{array} \right) 
 \]

debe notarse que $u^1 (0)$ y $u^2(0)$ corresponde a un sentido positivo de espín o ``hacia arriba'' mientras que $u^3  (0)$ y $u^4 (0)$ corresponde a un sentido de espín negativo o ``hacia abajo''. Adicionalmente se puede construír

\[\sigma_{i} \equiv \left ( \begin{array}{cc}
 \sigma_{i} & 0   \\
 0 & \sigma_{i}  \\
 \end{array} \right) \]

donde $\sigma_i$ son matrices de Pauli

\begin{equation}
[\alpha^{i} = \left ( \begin{array}{cc}
 0 & \sigma^{i}   \\
 \sigma^{i} & 0  \\
 \end{array} \right) \quad \gamma^{i} = \left ( \begin{array}{cc}
 0 & \sigma_i   \\
 -\sigma_i & 0  \\
 \end{array} \right)]
\end{equation}

\[\Rightarrow \sigma_{z} = \left ( \begin{array}{cc}
 \sigma_{z} & 0   \\
 0 & \sigma_{z}  \\
 \end{array} \right) = \left ( \begin{array}{cccc}
 1 & 0 & 0 & 0  \\
 0 & -1 & 0 & 0 \\
 1 & 0 & 1 & 0 \\
 0 & 0 & 0 & -1 \\
 \end{array} \right)
 \]

\[S_{z} \equiv \frac{1}{2} \sigma_{z} \quad \text{pero} \quad \beta u^{(1,2)}(0) = u^{(1,2)}(0)\]

\[\beta u^{(3,4)}(0) = u^{(3,4)}(0)\]

Si el electrón se mueve con velocidad $\overrightarrow{v} \Rightarrow \Lambda (-\overrightarrow{v})$ para pasar al sistema del laboratorio

\[\Lambda (- \overrightarrow{v}) = \frac{1}{\sqrt{2(1+\gamma)}} [1+\gamma + \overrightarrow{\alpha} \cdot \overrightarrow{v} \gamma]\]

con $\gamma = \frac{1}{\sqrt{1-v^2}}$. Para el sistema del laboratorio

\[\psi^{r} (x) = \Lambda (- \overrightarrow{v}) u^{r} (0) e^{-i \epsilon_{r} P^{\mu} \chi_{\mu}} \quad \text{con} \quad \chi^{\mu} \quad \text{en el sistema del lab}\]

se debe normalizar $\overline{u}(P) u (P) = 1$

\[u ^{\prime} (\overrightarrow{P}) = \frac{1}{\sqrt{2(1+\gamma)}} [1+\gamma + \overrightarrow{\alpha} \cdot \overrightarrow{v} \gamma ] \left ( \begin{array}{cccc}
 1   \\
 0   \\
 0   \\
 0   \\
 \end{array} \right) \]

tomando $\overrightarrow{P} || \hat{z}$ en dirección $\hat{k}$

\[\alpha_{z} = \left ( \begin{array}{cc}
 0 & \sigma_{z}   \\
 \sigma_{z} & 0  \\
 \end{array} \right) = \left ( \begin{array}{cccc}
 0 & 0 & 1 & 0  \\
 0 & 0 & 0 & -1 \\
 1 & 0 & 1 & 0 \\
 0 & -1 & 0 & 0 \\
 \end{array} \right)
 \]

\[E^2 = \overrightarrow{P}^2 + m_{0}^{2} = m^2 v^2 + m_{0}^2 = m_{0}^{2}(1+ \gamma^{2} v^{2})\]

\[= m_{0}^{2} (1+ \frac{v^{2}}{1-v^2}) = \gamma^{2} m_{0}^2\]

\[E \equiv + \sqrt{\overrightarrow{P}^2 + m_{0}^{2}}\]

\[= + \gamma m_0\]

\[P = \gamma m_0 v = E_{+} v \quad ; \quad \gamma= E_{+}/m_0\]

\[\Rightarrow \Lambda = \sqrt{\frac{m_0 +E}{2m_0}}[1+ \frac{\overrightarrow{\alpha} \cdot \overrightarrow{P}}{m_0 + E} \quad ; \quad \alpha_{z} = \left ( \begin{array}{cccc}
 0 & 0 & 1 & 0  \\
 0 & 0 & 0 & -1 \\
 1 & 0 & 1 & 0 \\
 0 & -1 & 0 & 0 \\
 \end{array} \right)\]

\[= \sqrt{\frac{m_{0} + E_{+}}{2m_0}} \left ( \begin{array}{cccc}
 1   \\
 0   \\
 \frac{P}{m_{0}+E_{+}}   \\
 0   \\
 \end{array} \right) \quad \text{etc.}\]

\[u^{(r)}(\overrightarrow{P}) = \sqrt{\frac{m_{0} + E_{+}}{2m_{0}}} ]\left[ \begin{array}{cccc}
 1 & 0 & \frac{P}{E_{+}+m_0} & 0  \\
 0 & 1 & 0 & -\frac{P}{E_{+}+m_0} \\
 \frac{P}{m_0 + E_{+}} & 0 & 1 & 0 \\
 0 & -\frac{P}{E_{+}+m_0} & 0 & 1 \\
 \end{array} \right] \]

son ortonormales. Se verifica que $\overline{u}^{r}(\overrightarrow{P}) u^{r^{\prime}}(\overrightarrow{P}) = \epsilon_{r} \delta_{rr^{\prime}}$

\[\overline{u}^{\prime}(0) = \left ( \begin{array}{cccc}
 1 & 0 & 0 & 0  \\
 \end{array} \right)
 \left ( \begin{array}{cccc}
 1 & 0 & 0 & 0  \\
 0 & 1 & 0 & 0 \\
 1 & 0 & -1 & 0 \\
 0 & 0 & 0 & -1 \\
 \end{array} \right) = (u_{0}^{1})^{\dagger}\beta
 \]

\[= \left ( \begin{array}{c}
 1 & 0 & 0 & 0  \\
 \end{array} \right)\]

\[\overline{u}^2 (0) = \left ( \begin{array}{c}
 0 & 1 & 0 & 0  \\
 \end{array} \right)
\quad \overline{u}^3 (0) = \left ( \begin{array}{c}
 0 & 0 & -1 & 0  \\
 \end{array} \right) 
 \]

\[\overline{u}^4 (0) = \left ( \begin{array}{c}
 0 & 0 & 0 & -1  \\
 \end{array} \right) \]

\[u_{\alpha}^{r}(0) = \delta_{\alpha r} \quad \overline{u}_{\beta}^{r}(0) = \epsilon_{r} \delta_{r \beta}\]

\[\sum_{r = 1}^{4} \epsilon_{r} u_{\alpha}^{r} (0) \overline{u}_{\beta}^{r} (0) = \sum_{r=1}^{4} \epsilon_{r} \delta_{r \beta} = \delta_{\alpha \beta} \]

\[\sum_{r = 1}^{4} \epsilon_{r} u_{\alpha}^{r} (\overrightarrow{P}) \overline{u}_{\beta}^{r}(\overrightarrow{P}) = \sum_{r = 1}^{4} \epsilon_{r} \Lambda_{\alpha \gamma} u_{\gamma}^{r}(0) \overline{u}_{\lambda}^{r}(0)\Lambda^{-1}_{\lambda \beta}  \]

\begin{equation}
= \Lambda_{\alpha \gamma} \delta_{\gamma \lambda} \Lambda^{-1}_{\lambda \beta = \delta_{\alpha \beta}}
\end{equation}

Ahora sea $\slashed{p} \equiv \gamma^{\mu}P_{\mu}$

\[\therefore \quad (i \gamma^{\mu} \partial_{\mu}-m_0) \psi (x) \quad \text{con } \quad \psi^{(r)}(x) \equiv u^{(r^{\prime})} (\overrightarrow{p}) e^{-i \epsilon_{r}P^{\mu} \chi_{\mu}}\]

\[\Rightarrow (\epsilon_{r} \slashed{p}-m_0 ) u^{(r)} (\overrightarrow{p}) = 0 \quad \Rightarrow (\slashed{p} -\epsilon_{r} m_0 )u^{(r)}(p) = 0\]

entonces, si

\[ E >0
  \begin{cases}
    \omega_{(p)}^{1} \equiv u(p,1/2) \\
    \omega_{(p)}^{2} \equiv u(p,-1/2) \\
  \end{cases}
\]

\[ E < 0
  \begin{cases}
    \omega_{(p)}^{3} \equiv u(p,1/2) \\
    \omega_{(p)}^{4} \equiv u(p,-1/2) \\
  \end{cases}
\]

entonces

\[\slashed{p} u (P,S) = m_0 u(P,S)\]

pero

\[\slashed{p} v (P,S) = - m_0 v(P,S)\]

\[\therefore (\frac{\slashed{p}+m_0 }{2m_0 }) u(P,S) = u(P,S)\]

\[\therefore (\frac{\slashed{p}+m_0 }{2m_0 }) v(P,S) = 0 \]

\[\Rightarrow \sum_{S} u_{\alpha} (P,S) \overline{u}_{\beta} (P,S) = \sum_{r=1}^{4} \epsilon_{r} (\frac{\slashed{p}+m_0 }{2m_0 })_{\alpha \gamma} \omega_{\gamma}^{r}(p) \overline{\omega}_{\beta}^{r}(p) \]

\[= (\frac{\slashed{p}+m_0 }{2m_0 })_{\alpha \gamma} \delta_{\gamma \beta} = (\frac{\slashed{p}+m_0 }{2m_0 })_{\alpha \beta}\]

\[\psi^{4} = \frac{1}{(2 \pi)^{3/2}} \left ( \begin{array}{cccc}
 0   \\
 0   \\
 0   \\
 1   \\
 \end{array} \right) e^{+imt} \quad E<0\]

\[\psi_{C} \equiv C \gamma^{0}\psi^{*} = i \gamma^2 \psi^{*}\]

recordando que $C = i \gamma^2 \gamma^0 $

\[\Rightarrow \psi_{C} = i \left ( \begin{array}{cccc}
 0 & 0 & 0 & -i  \\
 0 & 0 & i & 0 \\
 0 & i & -1 & 0 \\
 -i & 0 & 0 & 0 \\
 \end{array} \right) \quad  \left ( \begin{array}{cccc}
 0   \\
 0   \\
 0   \\
 1   \\
 \end{array} \right) \frac{1}{(2\pi^{3/2})} e^{-imt}\]

\[= \frac{1}{(2\pi^{3/2})} \left ( \begin{array}{cccc}
 1   \\
 0   \\
 0   \\
 0   \\
 \end{array} \right) e^{-imt} = \psi^1\]

i.e. la ausencia de un electrón de energía negativa y espín hacia abajo, se convierten en presencia de un positrón de energía positiva y espín hacia arriba.

\section{Proyector de energ\'ia}

\[\Lambda_{\pm}(p) \equiv \pm (\frac{\slashed{p}+m_0 }{2m_0 }) \]

si

\[u(P,S) \equiv \omega^{1}(p) \quad E>0 \quad S= + 1/2\]

\[u(P,-S) \equiv \omega^{2}(p) \quad E>0 \quad S= - 1/2\]

\[v(P,-S) \equiv \omega^{3}(p) \quad E<0 \quad S= - 1/2 \quad (E>0 , \quad S= +1/2) \quad \text{para su antipartícula}\]

\[v(P,S) \equiv \omega^{4}(p) \quad E<0 \quad S= + 1/2 (E>0 , \quad S= +1/2) \quad \text{para su antipartícula}\]

\[\Lambda_{+}(p) \omega^{1} = \omega^{1} \quad \Lambda_{+}(p) \omega(P, \pm S) = u(P, \pm S)\]

\[\Lambda_{+}(p) \omega^{2} = \omega^{2} \quad \Lambda_{+}(p) \omega^{3} = 0\]

\[\Lambda_{+}(p) \omega^{4} = 0 \]

\[\Lambda_{+}(p) v(P, \pm S) = 0 \quad \text{y} \quad \Lambda_{-} u(P, \pm S) = 0 \quad \text{pero} \quad \Lambda_{-}v(P, \pm S) = v (P, \pm S)\]

porque

\[(\frac{\slashed{p}+m_0 }{2m_0 }) \omega^{1,2} = \omega^{1,2}\]

pero

\[-(\frac{\slashed{p}+m_0 }{2m_0 }) \omega^{3,4} = \omega^{3,4}\]

entonces

\[\sum_{S} u_{\alpha} (P,S) \overline{u}_{\beta}(P,S) = \sum_{r} \epsilon_{r} (\frac{\slashed{p}+m_0 }{2m_0 })_{\alpha \rho} \omega_{\rho}^{(r)} \overline{\omega}_{\beta}^{(r)} \]

\begin{equation}
= (\frac{\slashed{p}+m_0 }{2m_0 })_{\alpha \rho} \delta_{\rho \beta} = (\frac{\slashed{p}+m_0 }{2m_0 })_{\alpha \beta}
\end{equation}

\section{Proyector de esp\'in}

\[\sigma_{z} = -i \alpha_{x} \alpha_{y} = +i \beta \alpha_{x} \beta \alpha_{y} = i \gamma^{1} \gamma^{2}\]

\[\gamma_{5} \gamma_{3} = i \gamma^0 \gamma^1 \gamma^2 \gamma^3 \gamma_3 = i \gamma^0 \gamma^1 \gamma^2\]

\[= \sigma_{z} \gamma^{0}\]

\[\therefore \sigma_{z} = \gamma^5 \gamma_{3}^{3} \gamma^0\]

desde el sistema del centro de masas

\[(\hat{u}) \equiv (0,0,0,1)\]

\[\frac{1+\sigma_{z}}{2} = \frac{1+\gamma^{5}\gamma_{3}\gamma^{0}}{2} = \frac{1+\gamma_{5}\gamma_{3} \hat{u}^{3} \gamma^0 }{2} \rightarrow \frac{1+ \gamma_{5} \gamma_{\mu} \hat{u}^{\mu} \gamma^{0}}{2}\]

\[= \frac{1 + \gamma^{5} \hat{u} \gamma^0 }{2}\]

\[\omega^{1}(0) = \left ( \begin{array}{cccc}
 1   \\
 0   \\
 0   \\
 0   \\
 \end{array} \right) \quad \text{con} E<0 \quad \uparrow 
 \quad \omega^{2}(0) = \left ( \begin{array}{cccc}
 0   \\
 1   \\
 0   \\
 0   \\
 \end{array} \right) \quad \text{con} E>0 \quad \downarrow \]

\[\omega^{3}(0) = \left ( \begin{array}{cccc}
 0   \\
 0   \\
 1   \\
 0   \\
 \end{array} \right) \quad \text{con} E<0 \quad \uparrow 
 \quad \omega^{4}(0) = \left ( \begin{array}{cccc}
 0   \\
 0   \\
 0   \\
 1   \\
 \end{array} \right) \quad \text{con} E>0 \quad \downarrow \]
 
\begin{equation}
\P(u) \equiv \frac{1 + \gamma^5 }{2}
\end{equation}

\[\sigma^{\mu \nu} = \frac{1}{4}[\gamma^{\mu},\gamma^{\nu}]\]

en el sistema del C.M

\[\gamma^{0} \omega^{1,2} = \omega^{1,2} \quad \gamma^0 \omega^{3,4} = - \omega^{3,4}\]

en general, $u^{\mu} \rightarrow s^{\mu}$

\begin{equation}
P(S) \equiv \frac{1+\gamma^5 \slashed{S}}{2}
\end{equation}

\subsection{Ejemplo: en presencia de campo electromagnético:}

\[P^{\mu} = (m, \overrightarrow{0})\]

\[S^{\mu} = (0,\overrightarrow{S})\]

\[P^{\mu} S_{\mu} = 0 \quad S^{\mu}S_{\mu} = -\overrightarrow{S}^{2} = -1\]

entonces, en el sistema C.M.

\[S^{\mu} = (0,0,0,1)\]

\[\frac{1+\gamma_5 \slashed{S}}{2 } \omega^{1}(0) = \frac{1+\gamma_5 \gamma_3 }{2 } \omega^1 (0)\]

observese que

\[\slashed{S} = \frac{1+\sigma_z }{2 } \omega^1 (0) = \omega^1 (0)\]

\[P(S)\omega^{1}(0)= \omega^1 (0) \quad P(-S) \omega^2 (0) = \omega^2 (0)\]

\[P(S)\omega^{2}(0)= 0) \quad P(-S) \omega^1 (0) = 0\]

\[P(-S)\omega^{3}(0)= \omega^3 (0) \quad P(S) \omega^3 (0) = 0\]

\[P(-S)\omega^{4}(0)= 0 \quad P(S) \omega^4 (0) = \omega^4 (0)\]

notar que si se quiere calcular $u_{\alpha}(P,S)$, con

\[ E>0
  \begin{cases}
    u_{\alpha}(P,S) = \omega^1 (P,S)\\
    u_{\alpha}(P,-S) = \omega^2 (P,S)\\
  \end{cases}
\]

se tiene

\[u(P,S) \overline{u}(P,S) = \sum_{S^{\prime}} (\frac{1+ \gamma_5 \slashed{S}}{2}) u(P,S^{\prime}) \overline{u}(S^{1}) \]

y como 

\[\sum_{S} u(P,S) \overline{u} (P,S) = \frac{\slashed{p} + m_0}{2m_0 }\]

normalmente en este caso $S^{\mu} = K^{\mu}$ que en reposo $K^{\mu} = (0,0,0,1)$ y se toma a $u$ en el sistema del $C.M.$

\[\frac{\slashed{P}+m_0}{2m_0} = \frac{1+ \beta}{2}\]

\[\Rightarrow u(P,S) \overline{u}(P,S) = (\frac{1+\gamma_5 \slashed{K}}{2}) (\frac{1+\beta}{2})\]

\section{Problemas}

\begin{itemize}
\item[1] Probar que 

\[\frac{1}{\slashed{p} \mp m_0} = \frac{\slashed{p}+m_0}{P^2 - m_{0}^{2}}\]

\item[2] Probar que

\[\hat{G}_{D} = \frac{\slashed{p}+m_0}{P^2 -m_{0}^{2}}\]

\item[3] Probar que 

\[tr (\gamma^1,...,\gamma^i) = 0 \quad \text{(impar)}\]

\item[4)] Probar que

\[[\hat{r},H] = \vec{\alpha}\]

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Examen Parcial}

\begin{itemize}

\item 1) Porbar que el momento en el C.M. de una reacción se escribe como

\begin{equation}
p^{*2} = \frac{[S- (m_1 + m_2 )^2 ] [S-(m_1 - m_2 )^2 ]}{4S}
\end{equation}

donde $S=(P_1 + p_2 )^2$ para una colisión $p_1 + p_2 \rightarrow p_3 + p_4 $

\item 2) Demostrar que el operador de velocidad $\vec{v} = \dot{\vec{r}}$ satisface:

\begin{equation}
\dot{\vec{r}} = -i [H_D , \vec{r}= \vec{\alpha}
\end{equation}

en donde $H_D =\vec{\alpha}\cdot \vec{p}+\beta m$ es el hamiltoniano de Dirac y $\vec{\alpha}$ está formado con as matrices de Dirac $\vec{\alpha} =(\alpha_x , \alpha_y , \alpha_z )$. Como los eigenvalores de $\vec{\alpha}$ son $\pm 1$,¿entonces una part{icula descrita por la ecuaci{on de Dirac se mueve a la velocidad de la luz?

\item 3) Probar que

\begin{equation}
\frac{1}{\slashed{p}-m} = \frac{\slashed{p} +m}{p^2 - m^2 }
\end{equation}

donde $\slashed{p} \equiv \gamma^{\mu} p_{\mu}$

\item 4) La función de Green $G_{KG} (x-y)$ para la ecuación de Klein-Gordon se define como

\begin{equation}
(\frac{\partial ^2}{\partial t^2 } -\nabla ^2 + m^2 ) G_{KG} (x-y) = \delta ^{4} (x-y)
\end{equation}


o sea, se define como el operador inverso de la ecuación diferencial misma. Análogamente, se puede definir la función de Green equivalente para la ecuación de Dirac como:

\begin{equation}
(i \slashed{\partial}-m)G_{D} (x-y) = \delta ^4 (x-y)
\end{equation}

con $\slashed{\partial} =\gamma^{mu\partial_{\mu}}$. Prueba que la función de Green de Klein Gordon se puede poner como

\begin{equation}
G_{KG}(x-y) = -\int \frac{d^4 k }{(2 \pi )^4 } e^{-ik\cdot (x-y)} \frac{1}{k^2 -m^2 }
\end{equation}

y que en consecuencia la función de Green para la ecuación de Dirac se puede escribir como

\begin{equation}
G_{D}(x-y) =\int \frac{d^4 k}{(2 \pi) ^4} e^{-ik \cdot (x-y} \frac{\slashed{k} + m}{^2 - m^2}
\end{equation}

donde $\slashed{k} \equiv \gamma^{\mu} k_{\mu}$

\textbf{Solución:}

\item 1) 

\[S = (p_1 + p_2 )^2 = (P_{1} ^{*} + p_{2}^{*})^2 = (E_{1}^{*} + E_{2}^{*})^2 - (\vec{p}_{1}^{*}+\vec{p}_{2}^{*})^2 = \] 

con $p_{1}^{*}=\vert \vec{p}_{1}\vert ^{*} = \vert \vec{p}_{2} \vert ^{*} = p_{2}^{*} = p^{*}$

\[= 2 P^{* 2} + m_{1}^{2} + m_{2}^{2} + 2 E_{1}^{*} E_{2}^{*}\] 

\[[S-2p^{*2} - m_{1}^{2} - m_{2}^{2}]^{2} =4 (p^{*2}+m_{1}^{2})(p^{*2} +m_{2}^{2})\] 

\[(S-2p^{2*}) - 2 (s-2 p^{*2} (m_{1}^{2} + m^{2}_{2}) +(m_{1}^{2} + m_{2}^{2})^{2}\] 

\[= 4 p^{*4} + 4p^{*2} m_{2}^{2} + 4 p^{*2}m_{1}^{*2}m_{1}^{2} + m_{1}^{2} m_{2}^{2} 4\] 

\[S^{2} -4S p^{* 2 } + 4 p^{4*} - 2S (m_{1}^{2} +  m_{2}^{2}) + 4 p^{*2}  (m_{1}^{2} + m_{2}^{2}) + (m_{1}^{2} + m_{2}^{2})^{2} \] 

\[= 4 p^{*4} + 4 p^{*} (m_{1}^{2} + m_{2}^{2}) + m_{1}^{3} + m_{2}^{2} 4 \] 

\[S^2 - 2S p^{*2} - 2S (m_{1}^{2} + m_{2}^{2}) + (m_{1}^{2} - m_{2}^{2})^2 = 0 \] 

\[\therefore  \frac{[S- (m_1 + m_2 )^2 ] [S-(m_1 - m_2 )^2 ]}{4S} = p^{*2} \] 

\item 2) 

\[\dot{\vec{r}} = - i [H_D , \vec{r} ] = - i [\vec{\alpha } \cdot \vec{p} + \beta m_{1} \vec{r} ] \] 

\[= - i \alpha ^j [p^j . \vec{r} ] = \vec{\alpha }\] 

\[\therefore p^j = - i \partial ^j = + i \partial _j \] 

\item 3)

\[(\frac{\slashed{p}+m}{p^2 - m^2 }) (\slashed{p}-m ) \quad \text{con} \slashed{p} \slashed{p} = p^2 \] 

\[= \frac{p^2 - m^2 }{p^2 - m^2 } = 1 \] 

\item 4)
\end{itemize}

\[(\Box + m^2 ) G_{kk} (x-y) = \delta ^4 (x-y)\] 

\[-(\Box + m^2 ) \int \frac{d^4 k}{(2 \pi )^4 } e^{-ik\cdot (x-y)} \frac{\slashed{k}+m}{k^2 - m^2 }\] 

\[= \delta^4 (x-y)\] 

\[(i \slashed{\partial} - m ) G_{D} (x-y) = \delta^{4} (x-y)\] 

\[(i \slashed{\partial} - m) \int \frac{d^4 k}{(2 \pi)^4 } e^{-ik \cdot (x-y)} \frac{(\slashed{k} - m) (\slashed{k}+m)}{k^2 - m^2 } \] 

\[\int \frac{d^4 k}{(2 \pi)^4 } e^{-ik \cdot (x-y)} \frac{(\slashed{k}-m)(\slashed{k}+m)}{k^2 - m^2 }\] 

\[= \int \frac{d^4 k}{(2 \pi)^4 } e^{-ik \cdot (x-y)}\] 

\[\therefore = \delta^{4}(x-y)\] 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Regla del Tri\'angulo}

De la definici\'on de momento angular cl\'asico

\[\overrightarrow{L} \equiv \overrightarrow{r} \times \overrightarrow{P}\]

\[[L_x ,L_y] = [yP_z -zP_y , zP_y , yP_z]\]

\[= [y P_z, zP_y] + [zP_y, yP_z] = y[P_z, zP_y ] + [y, z P_y]P_z + z[P_y . y P_z ] + [z, yP_z ]P_y  \]

\[=y[P_z , z] P_y + z [y, P_y] P_z + z[P_y , y] P_z + y[z, P_z ] P_y\]

\[= [yP_z , zP_x ] - [yP_z , zP_z ]- [zP_y , zP_x] + [zP_y , xP_z ] \]

\[= y[P_z, zP_x] + P_y x [z,P_z ]\]

\[yP_x [P_z , z] + x P_y [Z. P_z ] = -i yP_x + i xP_y = i (x P_y - y P_x) = iL_z \quad \text{etc.}\]

Del grupo de simetrías de $SU(2) : [L^i , L^j ] = i \epsilon_{ijk}L^{k}$

además

\[[\overrightarrow{L}^2 , L^i ] = [L_{y}^{2}+L_{z}^{2}, L_x ] = L_y [L_y , l_x ] + [L_y , L_x]L_y + L_z [L_z , L_x] + [L_z , L_ x] L_z\]

\[= -i L_yL_z -i L_z L_y +i L_z L_y + i L_y L_z = 0\]

Como $\overrightarrow{J} = \overrightarrow{L} + \overrightarrow{S} = \overrightarrow{L} + 1/2 \overrightarrow{\sigma}$ se tiene que

\[[J^i , J^j ] = i \epsilon_{ijk } J^K \]

cuyas eigenfunciones son $| \alpha m \rangle$

\[\overrightarrow{J}^2 |\alpha m \rangle = \alpha | \alpha m \rangle\]

\[\overrightarrow{J}_z |\alpha m \rangle = m | \alpha m \rangle\]

esto implica claramente que

\[\overrightarrow{J}^2 \geq J_{z}^2 \quad \alpha\geq m^2 \quad \Rightarrow \quad \alpha \geq m \geq -\alpha\]

además $m_{max} \equiv j$. Del conmutador

\[[L^i , L^j] = i \epsilon _{ijk} L^k \Rightarrow [\overrightarrow{L}^2 , L^i ] = 0\]

y como $\overrightarrow{J} \equiv \overrightarrow{L} + \overrightarrow{S}$ cumple con $[J^i , J^j ] = i \epsilon_{ijk} J^{K}$

\[ \Rightarrow [\overrightarrow{J}^2 , J_{z}] = 0\]

A $\overrightarrow{J}^2$ se le llama el \textit{casimir} del grupo. Las eigenfunciones $| \alpha m \rangle$ son tales que

\[\langle\alpha^{\prime}m^{\prime}|\alpha m \rangle = \delta_{\alpha \alpha^{\prime}} \delta_{m m^{\prime}}\]

Se tiene como condición

\[j \geq m \geq -j \Rightarrow \quad j = 0, 1/2,1,3/2, \dotsc \]

para que se cumpla:

\[J_{-}|j ,_{im}\rangle = 0 \quad \Rightarrow J_{+}J_{-}|jm_{min}\rangle = 0\]

Sea

\[J_{\pm}\equiv J_x \pm J_y \Rightarrow [J_z, J_{\pm}] = [J_z , J_x \pm i J_y] = [J_{z},J_{x}] \pm i [J_{z},J_y ] = \]

dado que $[J_ z , J_x ] \equiv i J_y$

\[= \pm (J_x \pm i J_y ) = \pm J_{\pm}\]

\[\Rightarrow [J_z , J_{\pm}] = \pm J_{\pm} \quad \text{si} \quad |\alpha m \rangle \quad \text{son eigenvalores de } \quad \overrightarrow{j}^2 \quad \text{hay} \quad 2j+1\]

\[\text{también} \quad J_{\pm} |\alpha m \rangle \quad \text{dado que:} \quad J^2 (J_{\pm} |\alpha m \rangle) = \alpha(J_{\pm}|\alpha m \rangle )\]

además:

\[J_z (J_{\pm} | \alpha m \rangle ) = (\pm J_{\pm} + J_{\pm} J_z ) |\alpha m \rangle = (m \pm 1) (J_{\pm} |\alpha m \rangle)\]

\[\therefore J_{\pm} | \alpha m \rangle \sim |\alpha m \pm 1 \rangle \]

\[J_{-} J_{+} = (J_x -i J_y ) (J_x + i J_y ) ) J_{x}^{2} + J_{y}^{2} + i [J_x , J_y ]\]

\[= \overrightarrow{J}^2 - J_{z}^{2} - J_z = \overrightarrow{J}^2 - J_z (J_z +1)\]

\[J_{-} J_{+} | \alpha j \rangle = 0 = \alpha - j (j+1) \quad \Rightarrow \alpha \equiv j(j+1)\]

\[|\alpha m \rangle \rightarrow |j m \rangle \quad \overrightarrow{J}^2 | jm \rangle = j(j+1) |jm \rangle\]

\[J_{z} |jm \rangle = m | jm \rangle\]

\[\langle j^{\prime} m^{\prime} | k m \rangle = \delta_{j j^{\prime}} \delta_{m m^{\prime}}\]

\[\text{Def:} \quad J_{\pm} | jm \rangle = C^{\pm} _{m} | j m_{pm 1}\rangle\]

\[\langle jm | J_{-}J_{+} | jm \rangle = C_{m}^{+} C^{-}_{m+1}\]

\[= j (j+1) - m (m+1)\]

\[\text{además} \quad \langle jm | J^{+} | j m_{-1}\rangle = C^{+}_{m-1}\]

\[\Rightarrow C^{+}_{m-} = \langle jm_{-1} | J^{-}| jm \rangle = C^{-}_{m}\]

\[\therefore |C^{+}_{m}|^2 = j (j+1)-m(m+1) \quad \Rightarrow \quad C^{+}{m} = + \sqrt{j(j+1)-m(m+1)} \quad C^{-}_{m} = + \sqrt{j(j+1)-m(m-1)}\]

\begin{equation}
C^{\pm}_{m} = + \sqrt{j(j+1) - m(m\pm 1)}
\end{equation}

\section{Representaci\'on irreducible}

De los eigenvalores $|jm\rangle$ cuya representación irreducible

\[\langle jm | J_x | jm^{\prime }\rangle\]


\begin{center}
    \begin{tabular}{| l | l | l |}
    \hline
    $\mathscr{m}_{máx} = j$ & $(m_1 m_2 ) \quad j_{2} > j_1 $ &   \\ \hline
    $j_1 + j_2 $ & $(j_1 , j_2 )$ &    \\ \hline
    $\vdots$ & $\vdots$ &   \\ \hline
   $j_2 - j_1$ & $(-j_1 , j_2 ) , (-j_1 + 1 , j_2 -1 ) , \ldots , (j_1 j_2 -(j_1 +1)) $ &\\ \hline

    \end{tabular}
\end{center}

Por ejemplo, si $j_2 = 2 , j_1 = 1$

\begin{center}
    \begin{tabular}{| l | l | l |}
    \hline
    $\mathscr{m}_{máx} $ & $(m_1 m_2 ) $ & número de estados   \\ \hline
    $3$ & $(1 ,2 )$ & $1$   \\ \hline
    $2$ & $(0,2) . (1,1)$ & $2$   \\ \hline
  $1$ & $(-1,2) , (0,1) , (1,0)$  & $3$ \\ \hline

    \end{tabular}
\end{center}

\[\Rightarrow j = j_1 + j_2 , j_1 + j_2 -1 , \cdots , j_2 - j_1\]

\[(2j_1 + 1)(2j_2 +1) \quad \Rightarrow \sum _{j = |j_2 - j_1 |}^{j_2 + j_1 } 2j+1 = (2j_1 + 1) (2j_2 +1)\]



\[\text{Con} \quad j = j_1 + j_2 \cdots |j_2 - j_1 | \quad \text{usando la regla del triángulo}\]

\[1/2 \bigoplus 1/2 = 3 \bigoplus 1\]

\begin{equation}
1 \bigoplus 1 = 5 \bigoplus 3 \bigoplus 1
\end{equation}

\textbf{TAREA:} usando que $\sum_{A}^{B} j$ demostrar

\[\sum_{k}^{B} j \frac{1}{2} (A+B)(B+A+1)\]

\[\overrightarrow{J} = \overrightarrow{J}_1 \overrightarrow{J}_2\]

con

\[2j_1 +1 : \j_1 m_1 \rangle : \overrightarrow{J}_1 \quad 2j_2 +1 : \j_2 m_2 \rangle : \overrightarrow{J}_2 \]

y la componente unitaria $\overrightarrow{J} : |jm \rangle $ con $(2j_1 +1)(2j_2 +1)$ y el conmutador

\[[\overrightarrow{J}_1 , \overrightarrow{J}_2] = 0\]

\[\Rightarrow [J^i , J^j ] = i \epsilon_{ijk} J^k \]

se debe notar que $|n \rangle \langle n| \equiv I$

\[1 \psi \rangle = \sum_{n} a_{n}|n\rangle\]

\[\langle m | \psi \rangle = a_{m}\]

\[1 \psi \rangle = |n \rangle \langle n n| \psi \rangle \equiv (\sum_{n} |n \rangle \langle n|) \psi\]

fijemos $j_1 $ y $j_2 $ es decir, en su representación irreducible:

\[|j_1 m_1 \rangle | j_2 m_2 \equiv |m_1 m_2 \rangle \quad \text{(hay} (2j_1 +1) (2 j_2 +1))\]

\[|jm \rangle = |m_1 m_2 \rangle \langle m_1 m_2 | j m \rangle = \sum_{m_1 = -j_1 }^{j_1 } \sum_{m_2 = -j_2}^{j_2 } \langle m_1 m_2 | jm \rangle |m_1 m_2 \rangle\]

nótese que las componentes de los sumandos son no otra cosa que los coeficientes de Clebsch-Yordan. Como

\[J_z = J_{z_{1}} + J_{z_{2}} \Rightarrow m = m_1 + m_2\]

\[\therefore \langle m_1 m_2 | jm \rangle = 0 \quad \text{a menos que} \quad m_1 + m_2 = m\]

con $m_{max} = j_1 + j_2$.

\textbf{Caso:} $\overrightarrow{J} = \overrightarrow{L} + \overrightarrow{S}$

\[Y_{l}^{m} : \overrightarrow{L}^2 Y_{l}^{m_{0} } = l (l+1) Y_{l}^{m_{l}} \quad l = 0,1,2,\cdots\]

\[\chi _{\pm} : \overrightarrow{S}^2 \chi_{pm} = S(S+1) \chi_{pm} \quad S=1/2\]

\[L_z Y_{l}^{m_{l}} = m_{l} Y_{l}^{m_{l}} \quad l \geqslant m_{l} \geqslant -l\]

\[S_z \chi_{pm} = \pm 1/2 \chi _{\pm}\]

\[\chi _{+} =  \left ( \begin{array}{cc}
 1   \\
 0   \\
 \end{array} \right) \quad \chi_{-} = \left ( \begin{array}{cc}
 0   \\
 1   \\
 \end{array} \right) \quad S_z = 1/2 \sigma_z \]

\[|jm \rangle \equiv \overrightarrow{J}^2 |jm \rangle = j (j+1 |jm \rangle)\]

\[J_z |jm \rangle = m |jm \rangle \quad = 1/2,3/2, \cdots \]

\[|jm \rangle = \sum _{m_{l}S_{z}} |m_{l} S_{z} \rangle \langle m_{l}S_z | jm \rangle = sum_{m_{l} = -l}^{l} \sum_{S_z = -1/2}^{1/2} |m_l S_z \rangle \langle m_l S_z | jm \rangle \]

\[= \sum_{m_{l}} |m_l , 1/2 \rangle \langle m_l 1/2 |jm \rangle + |m_l , -1/2 \rangle \langle m m_l , -1/2\rangle\]

\[|m- 1/2 , 1/2 \rangle \langle m -1/2,1/2 |jm \rangle + |m +1/2 , -1/2 \rangle \langle m+1/2 , -1/2 |jm \rangle \]

haciendo el cambio de variable $a= \langle m -1/2,1/2 |jm \rangle $ y $b=\langle m+1/2 , -1/2 |jm \rangle$

\[| jm \rangle = a Y_{l}^{m-1/2} \chi_{+} + b 	Y_{l}^{m+1/2} \chi_{-} \quad j = l + 1/2 \quad , \quad  |l-1/2|\]

esto es

\[a \equiv \langle m -1/2,1/2 |jm \rangle =
  \begin{cases}
     \langle m -1/2,1/2 |l+1/2,m \rangle \\
    -\langle m -1/2,1/2 ||l-1/2| ,m \rangle \\
  \end{cases}\]

\[b \equiv \langle m +1/2, -1/2 |jm \rangle =
  \begin{cases}
     \langle m +1/2, -1/2 |l+1/2,m \rangle \\
    -\langle m -1/2,1/2 ||l-1/2| ,m \rangle \\  
  \end{cases}\]

\section{Clebsch-Yordan para $\overrightarrow{J} = \overrightarrow{l}+\overrightarrow{s}$}

De la definici\'on

\begin{equation}
\overrightarrow{J} = (\overrightarrow{l}+\overrightarrow{s})
\end{equation}

con $l _{\pm}(l_{x} \pm il_y )$, as\'i mismo para el caso $S_{\pm}$

\[J^2 = l^2 + s^2 + 2\overrightarrow{l} \cdot \overrightarrow{s} = l^2 + s^2 + 2l_z s_z + l_+ s_- + l_- s_+ \]


\[j_l \equiv a Y_{l}^{m - 1/2} \chi (1/2) + b Y_{l}^{m+1/2} \chi (-1/2) \quad j?l\pm 1/2\]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Espinores esf\'ericos}

\[a \equiv 
  \begin{cases}
     \langle m -1/2,1/2 |l+1/2,m \rangle \quad j=l+1/2\\
   -\langle m -1/2,1/2 |l-1/2 ,m \rangle \quad j= l-1/2\\
  \end{cases} \quad b = \langle m +1/2 ,1/2 |j,m\rangle\]

\[J^2 Y_{l}^{j,m} = j(j+1)Y_{l}^{j,m}=\]

\[= a[l(l+1)+3/4+m^{-1/2}] Y_{l}^{m-1/2} \chi(1/2) + a \sqrt{l(l+1)-(m-1/2)(m+1/2)} \sqrt{3/4 -1/2(-1/2)} Y_{l}^{m+1/2} \chi (-1/2)\]

\[+ b [l(l+1)+3/4 - m^{-1/2}] Y_{l}^{m+1/2} \chi(-1/2) + b \sqrt{l(l+1)-(m+1/2)(m-1/2)} \sqrt{3/4 -(-1/2)1/2} Y_{l}^{m-1/2} \chi (1/2) \]

\[= \lbrace a(l(l+1))+1/4 + m ]+ b \sqrt{l(l+1)-(m^2 -1/4)}\rbrace Y_{l}^{m-1/2} \chi (1/2) \]

\[ \lbrace b(l(l+1))+1/4\ -m ]+ a \sqrt{l(l+1)-(m^2 -1/4)}\rbrace Y_{l}^{m+1/2} \chi (-1/2) \]

\[= j(j+1)[a Y_{l}^{m-1/2} \chi (1/2) + b Y_{l}^{m+1/2} \chi (-1/2)] \]

\[a[j(j+1) - l(l+1)-m-1/4]-b \sqrt{l(l+1)-(m^2 -1/4)} = 0\]

\[b[j(j+1) - l(l+1)+m-1/4]-a sqrt{l(l+1)-(m^2 -1/4)} = 0\]

\[\text{Si} \quad j = l+1/2 \quad j(j+1)-l(l+1) = (l+1/2)(l+3/2)-l(l+1)\]

\[=l^2 + 3/2 l + 1/2 l +3/4 - l^2 -l = l+3/4\]

\begin{equation}
a[l-m+1/2]-b \sqrt{l(l+1)-(m^2-1/4)}=0\\
b[l+m+1/2]-a \sqrt{l(l+1)-(m^2-1/4)} = 0\\
\end{equation}

\[\langle jm|jm \rangle = 1 = \sum_{m_{l}s_{z}} \langle j m | m_{l}s_{z}\rangle \langle m_{l}s_{z}|jm\rangle\]

\[= \sum_{m_{l}} \langle jm| m_{l}1/2\rangle \langle m_l 1/2 |jm \rangle + \langle jm | m_l -1/2 \rangle\langle m_l -1/2 |jm \rangle \]

\[= \sum _{m_{l} } |\langle m_l , 1/2 | jm \rangle|^2 + |\langle m_l , -1/2 |jm\rangle|^2 \]

\[\Rightarrow |\langle m_l , 1/2 | jm \rangle|^2 + |\langle m_l , -1/2 |jm\rangle|^2 = 1\]

Con $a^2 + b^2 = 1 $ i.e. $b^2 = 1-a^2$

\[a^2 (l - m+1/2)^2 = b^2 [l(l+1)-(m^2 - 1/4)] = (1-a^2)[l(l+1)-(m^2 - 1/4)]\]

\[(l-m+1/2)(l+m+1/2) = l^2 + lm -lm + l/2 - m^2 -m/2 + l/2 + m/2 + 1/4\]

\[= l^2 + l - m^2 + 1/4\]

\[\Rightarrow a^2 (l-m 1/2 )^2 = (1-a^2)(l-m+1/2)(l+m+1/2)\]

\[\Rightarrow a^2[ (l-m +1/2 ) + (l+m+1/2)] = (l+m+1/2)\]

\[a^2 (2l + 1) = (l+m +1/2)\]

\begin{equation}
a = \sqrt{\frac{l+m+1/2}{2l+1}} 
\end{equation}

\[ \text{y para} \quad b^2 = 1- \frac{l+n+1/2}{2l+1} = \frac{l-m + 1/2}{2l+1}\]

\begin{equation}
b = \sqrt{\frac{l-m+1/2}{2l+1}}
\end{equation}

\subsection{Tarea}: En el par de ecuaciones

\[a[j(j+1) - l(l+1)-m-1/4]-b \sqrt{l(l+1)-(m^2 -1/4)} = 0\]

\begin{equation}
b[j(j+1) - l(l+1)+m-1/4]-a \sqrt{l(l+1)-(m^2 -1/4)} = 0
\end{equation}

sustituir por $J = l-1/2$ ¿Qué se obtiene bajo esta suposición?

\textbf{Solución:} Se sustituye $J = l-1/2$ en ()

\[j(j+1) - l(l+1) = (l-1/2)(l+1/2)-l(l+1) = - (l+1/4)\]

\[a[-l-1/2-m]-b \sqrt{l(l+1)-(m^2 - 1/4)} = 0 \]

\[b[-l-1/2+m]-a \sqrt{l(l+1)-(m^2 - 1/4)} = 0 \]

\[a(l+m+1/2) = -b \sqrt{l(l+1)-(m^2-1/4)} \quad \text{sabiendo que} \quad a^2 + b^2 = 1 \quad \text{y} \quad a^2 = 1-b^2\]

\[(1-b^2)(l+m+1/2)^2 = b^2 [l(l+1)-(m^2-1/4)]=b^2(l-m+1/2)(l+m+1/2)\]

\[b^2 [(l-m+1/2)+(l+m+1/2)] = l + m +1/2\]

\begin{equation}
b = \sqrt{\frac{l+m+1/2}{2l+1}} \quad ; \quad a = - \sqrt{\frac{(l+m+1/2)}{2l+1}} \sqrt{\frac{(l-m+1/2)(l+m+1/2)}{(l+m+1/2)^2} } = - \sqrt{\frac{(l-m+1/2)}{2l+1}}
\end{equation}

y resulta

\begin{equation}
\langle m - 1/2 , 1/2 |l+1/2,m\rangle = \sqrt{\frac{l+m+1/2}{2l+1}} \langle m +1/2 , -1/2 | j,m \rangle = \sqrt{\frac{l-m+1/2}{2l+1}}
\end{equation}

\begin{equation}
\langle m - 1/2 , 1/2 |l-1/2,m\rangle = - \sqrt{\frac{l-m+1/2}{2l+1}} \langle m +1/2 , -1/2 | l-1/2 ,m \rangle = \sqrt{\frac{l-m+1/2}{2l+1}}
\end{equation}

i.e.

\[ \langle m -1/2 , 1/2 | l\pm 1/2 ,m \rangle = \pm \sqrt{\frac{l \pm m+1/2}{2l+1}} \]

\[ \langle m + 1/2 , - 1/2 | l\pm 1/2 ,m \rangle =  \sqrt{\frac{l \mp m+1/2}{2l+1}} \]

\[Y_{l}^{l \pm 1/2 , m} = \frac{1}{\sqrt{2l+1}}  \left ( \begin{array}{cc}
 \pm Y_{l}^{m-1/2} \sqrt{l \pm m +1/2}   \\
 Y_{l}^{m + 1/2} \sqrt{l \mp m +1/2}    \\
 \end{array} \right) \]

\textbf{Tarea:} Al tomar al operador de proyección $\Lambda _{\pm} (p) \equiv \pm \frac{\slashed{p} + m_0}{2m_0 }$. Demostrar que:

\[ \Lambda_{+} (p) + \Lambda_{-} (p) = 1\]

\[ \Lambda_{\pm}^{2} (p)  = 1\]

\[ \Lambda_{+} (p) + \Lambda_{-} (p) = 0 \]

Si 

\[ 
  \begin{cases}
    u = u^{1,2}\\
    v = v^{1,2}\\
  \end{cases}
\]

Demostrar que:

\[\Lambda_{+}(p)u = u\]

\[\Lambda_{+}(p)v = 0\]

\[\Lambda_{-}(p)u = 0\]

\[\Lambda_{+}(p)v = v\]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Ecuaci\'on de Dirac: Potencial central.}

Del hamiltoniano de Dirac

\[H_{D} = \overrightarrow{\alpha} \cdot \overrightarrow{p	} + \beta m_0 + V (r) \quad r \equiv |\overrightarrow{r}|\]

\[\text{bajo una reflexión} \quad \overrightarrow{r} \rightarrow - \overrightarrow{r} \quad t \rightarrow t\]

\[\psi (x) \rightarrow \beta \psi (x) = \psi ^{\prime} (x^{\prime})\]

\[\psi (t, \overrightarrow{r}) \rightarrow \psi ^{\prime} (t,-\overrightarrow{r}) = \beta \psi (t,\overrightarrow{r})\]

\[\psi ^{\prime} (t,\overrightarrow{r}) = \beta \psi (t,-\overrightarrow{r}) \equiv P \psi (t,\overrightarrow{r})\]

\[P \psi (t, \overrightarrow{r}) = \psi ^{\prime } (t,\overrightarrow{r}) = \beta \psi (t,-\overrightarrow{r})\]

\begin{equation}
P \equiv \beta P^{\text{orb}} \quad P \equiv \beta P^{\text{orb}} \psi (t,\overrightarrow{r}) = \equiv (t,- \overrightarrow{r})
\end{equation}

si el sistema es invariante bajo reflexiones 

\begin{equation}
[P, H_{D} ] = 0
\end{equation}

debido al efecto del campo central. En el caso que sea invariante bajo rotaciones espaciales $\overrightarrow{J} = \overrightarrow{L} + \overrightarrow{S}$

\[[H_D , J^2] = 0\]

Tarea: Demostrar que $[\overrightarrow{J},H_D ] = 0$

\[\psi \equiv \left ( \begin{array}{cc}
 \psi   \\
 \chi   \\
 \end{array} \right) \quad \phi \equiv \left ( \begin{array}{cc}
 \psi_1   \\
 \psi _2   \\
 \end{array} \right) \quad \chi \equiv \left ( \begin{array}{cc}
 \psi_3   \\
 \psi_4   \\
 \end{array} \right) \]

\[J^2 \left ( \begin{array}{cc}
 \phi   \\
 \chi   \\
 \end{array} \right) = J(J+1) \left ( \begin{array}{cc}
 \phi   \\
 \chi   \\
 \end{array} \right)\]

\[J_z \left ( \begin{array}{cc}
 \phi   \\
 \chi   \\
 \end{array} \right) = M \left ( \begin{array}{cc}
 \phi   \\
 \chi   \\
 \end{array} \right)\]

\[\pi \equiv 
  \begin{cases}
    (+1)       & \quad \text{par } (-)^{J+1/2} \\
    (-1)  & \quad \text{par } (-)^{J-1/2} \\
  \end{cases} \quad (-)^{J+1/2 \pi} \equiv (-)^L \quad \beta = \left ( \begin{array}{cc}
 1 & 0   \\
 0 & -1   \\
 \end{array} \right) 
\]

\[P \left ( \begin{array}{cc}
 \phi   \\
 \chi   \\
 \end{array} \right) = \beta P^{\text{orb}} \left ( \begin{array}{cc}
 \phi   \\
 \chi   \\
 \end{array} \right) = (-) ^{J + 1/2 \pi} \left ( \begin{array}{cc}
 \phi   \\
 -\chi   \\
 \end{array} \right)\]

con $l \equiv j + 1/2 \pi$

\[J^2 \left ( \begin{array}{cc}
 \phi   \\
 \chi   \\
 \end{array} \right) = j (j+1) \left ( \begin{array}{cc}
 \phi   \\
 \chi   \\
 \end{array} \right) \quad j = l \pm 1/2 \Rightarrow j \mp 1/2]\]

\[J_z \left ( \begin{array}{cc}
 \phi   \\
 \chi   \\
 \end{array} \right) = m \left ( \begin{array}{cc}
 \phi   \\
 \chi   \\
 \end{array} \right) \quad \text{par} = (-)^l\]

Bajo la parte orbital: sea 

\[\pi \equiv  \begin{cases}
    1       & \quad \text{si la paridad es } (-)^{j+1/2}\\
   -1       & \quad \text{si la paridad es } (-)^{j-1/2}\\
  \end{cases} \quad \text{i.e} (-)^{k+1/2 \psi} = (-)^l\]

\[\text{i.e.} P \left ( \begin{array}{cc}
 \phi   \\
 \chi   \\
 \end{array} \right) = \beta P^{orb} \left ( \begin{array}{cc}
 \phi   \\
 \chi   \\
 \end{array} \right) = (-)^{j+1/2 \pi} \left ( \begin{array}{cc}
 \phi   \\
 -\chi   \\
 \end{array} \right)\]

es decir:

\[l = j + 1/2 \pi \quad j = l -1/2 \pi\]

\[\text{como} \quad \overrightarrow{J} = \overrightarrow{l} + \overrightarrow{S} \quad  \text{y} \ \quad \overrightarrow{S} = \frac{1}{2} \left ( \begin{array}{cc}
 \overrightarrow{\sigma} & 0 \\
 0 & \overrightarrow{\sigma} \\
 \end{array} \right) \]

\[\overrightarrow{J} = \left ( \begin{array}{cc}
 \overrightarrow{l}+1/2 \overrightarrow{\sigma} & 0 \\
 0 & \overrightarrow{l}+1/2 \overrightarrow{\sigma} \\
 \end{array} \right)\]

\[\phi = \frac{1}{r} F Y_{l=j+1/2 \pi}^{j,m} \quad Y_{j+1/2\pi}^{l-1/2\pi,m} \quad \text{sea} \quad l \equiv j+1/2 \pi\]

\[\chi = \frac{iG}{r}  Y_{l=j-1/2 \pi}^{j,m} \quad Y_{j-1/2\pi}^{l-1/2\pi,m} \quad \text{sea} \quad l^{\prime} \equiv j-1/2 \pi\]

\[\psi _{l}^{jm}(r,\theta \phi) = \frac{1}{r} \left ( \begin{array}{cc}
 F Y_{l}^{jm}  \\
 iG Y_{l^{\prime}}^{jm}   \\
 \end{array} \right)\]

\[\text{Con} \quad l = j + 1/2 \pi \quad j = l -1/2 \pi \quad \pi = \pm 1 \quad l^{\prime} = j-1/2 \pi\]

\[Y_{l}^{l \pm 1/2,m}(\theta , \phi) = \frac{1}{\sqrt{2l+1}} \left ( \begin{array}{cc}
 \pm \sqrt{l \pm m + 1/2} Y_{l}^{m-1/2}  \\
 \sqrt{l \mp m + 1/2} Y_{l}^{m+1/2}   \\
 \end{array} \right) \]

Definiendo:

\[P_{r} \equiv - i \frac{1}{r} \frac{\partial}{\partial r} r\]

\begin{equation}
\alpha_r \equiv \overrightarrow{\alpha} \cdot \hat{r} = P_1 (\overrightarrow{\sigma} \cdot \hat{r})
\end{equation}

\[(\overrightarrow{\alpha} \cdot \overrightarrow{r}) (\overrightarrow{\alpha} \cdot \overrightarrow{r}) = p (\overrightarrow{\sigma} \cdot \overrightarrow{r})P_1 (\overrightarrow{\sigma} \cdot \overrightarrow{p}) = (\overrightarrow{\sigma} \cdot \overrightarrow{r}) (\overrightarrow{\sigma} \cdot \overrightarrow{p})\]

\[\sum_{ij} (\sigma_i r_i )(\sigma_j p_j ) = \sum _{i=j} \sigma_i \sigma_j r_i p_j + \sum_{i \neq j} \sigma_i \sigma_j r_i p_j\]

\[\overrightarrow{r} \cdot \overrightarrow{P} + i \epsilon_{ijk} \sigma_k r_i P_j = \overrightarrow{r} \cdot \overrightarrow{P} + i \overrightarrow{\sigma} \cdot (\overrightarrow{r} \times \overrightarrow{P})\]

\[= \overrightarrow{r} \cdot \overrightarrow{P} + i \overrightarrow{\sigma \cdot \overrightarrow{L}}\]

\begin{equation}
\overrightarrow{r} \cdot \overrightarrow{P} = -i x_i \partial_i = -i x_i \partial_i r \frac{\partial}{\partial r}
\end{equation}

\[\partial_i r = \partial_i \sqrt{x^2 + y^2 + z^2} = \frac{X_i}{r}\]

\[\overrightarrow{r} \cdot \overrightarrow{P} = - i x_i \frac{x_i }{r} \frac{\partial}{\partial r} = -i r \frac{\partial}{\partial r}\]

\begin{equation}
r P_r = - i \frac{\partial}{\partial r} r = -i -ir \frac{\partial}{\partial r}
\end{equation}

\[\Rightarrow r P_r + i = r \cdot \overrightarrow{P}\]

\[(\overrightarrow{\sigma} \cdot \overrightarrow{r}) (\overrightarrow{\sigma} \cdot \overrightarrow{r}) = r P_r + i [\overrightarrow{\sigma} \cdot \overrightarrow{L}+1]\]

\[\Rightarrow (\overrightarrow{\sigma} \cdot \hat{r}) (\overrightarrow{\sigma} \cdot \overrightarrow{P}) = P_r + \frac{i}{r} (1+ \overrightarrow{\sigma} \cdot \overrightarrow{L})\]

\[(\overrightarrow{\alpha} \cdot \hat{r}) (\overrightarrow{\alpha} \cdot \hat{r}) = (\overrightarrow{\sigma} \cdot \hat{r}) (\overrightarrow{\sigma} \cdot \hat{r}) = 1\]

\[\Rightarrow (\overrightarrow{\alpha} \cdot \overrightarrow{P}) = \alpha_r P_r + i \frac{\alpha _r}{r} (1+\overrightarrow{\sigma} \cdot \overrightarrow{L})\]

\[= \alpha_r [P_r + \frac{i}{r} (1+ \overrightarrow{\sigma} \cdot \overrightarrow{L})\]

\[\overrightarrow{j} = \overrightarrow{L} + 1/2 \overrightarrow{\sigma} \Rightarrow \quad J^2 = j(j+1) = L^2 + \overrightarrow{L} \cdot \overrightarrow{\sigma} + 3/4\]

\[1+ \overrightarrow{\sigma} \cdot \overrightarrow{L} = j(j+1) - L^2 +1/4 = \]

\[(\overrightarrow{\alpha} \cdot \overrightarrow{P}) = \alpha _r [P_r + \frac{1}{r} [j(j+1)-L^2 + 1/4]\]

\[L^2 \psi _{l}^{j,m} = L^2 \frac{1}{r} \left ( \begin{array}{cc}
 F Y_{l}^{j,m}   \\
 iG Y_{l}^{j,m}   \\
 \end{array} \right) = \frac{1}{r} \left ( \begin{array}{cc}
 F l (l+1) Y_{l}^{j,m}   \\
 iG l^{\prime} (l^{\prime}+1)Y_{l^{\prime}}^{j,m}   \\
 \end{array} \right)\]

\[l = j+1/2 \pi \quad l^{\prime} = j-1/2 \pi\]

\[l(l+1) = (j+1/2 \pi) (j+1/2 \pi + 1) = j (j+1) +1/2 \pi (j+1) + 1/2 \pi j +1/4\]

\begin{equation}
1/2 \pi + (j+1) = j(j+1) + 1/2 \pi (2j+1)+1/4
\end{equation}

\[l^{\prime}(l^{\prime}+1) = (j-1/2 \pi) (j-1/2 \pi + 1) = j (j+1) -1/2 \pi j + 1/4 -1/2 \pi (j +1)\]

\begin{equation}
j(j+1)-1/2 \pi (2j+1 + 1/4)
\end{equation}

\[L^2 = j (j+1)+1/4 +1/2 \pi (2j+1) \beta\]

\[(\overrightarrow{\alpha} \cdot \overrightarrow{P}) = \alpha _r \lbrace P_r + \frac{i}{r} [-1/2 \pi (2j+1) \beta] \rbrace\]

\[\overrightarrow{\alpha} \cdot \overrightarrow{P} = \alpha _r [P_r - \frac{i}{2r} \pi (2j + 1) \beta \]

\begin{equation}
\lbrace \alpha _r [P_r - \frac{i}{2r} \pi (2j+1) \beta] + \beta m_0 + V(r) \rbrace \psi _{l}^{jm} = E \psi _{l}^{jm}
\end{equation}

es la ecuación de Dirac con simetría esférica

\[\alpha _r = \left ( \begin{array}{cc}
 0 & \overrightarrow{\sigma} \cdot \hat{r}   \\
 \overrightarrow{\sigma} \cdot \hat{r} & 0   \\
 \end{array} \right)  \quad \alpha _r \beta = \left ( \begin{array}{cc}
 0 & \overrightarrow{\sigma} \cdot \hat{r}   \\
 \overrightarrow{\sigma} \cdot \hat{r} & 0   \\
 \end{array} \right) \left ( \begin{array}{cc}
 1 & 0   \\
 0 & -1   \\
 \end{array} \right) = \left ( \begin{array}{cc}
 0 & -\overrightarrow{\sigma} \cdot \hat{r}   \\
 \overrightarrow{\sigma} \cdot \hat{r} & 0   \\
 \end{array} \right)  \]

\[\lbrace \left ( \begin{array}{cc}
 0 & \overrightarrow{\sigma} \cdot \hat{r}   \\
 \overrightarrow{\sigma} \cdot \hat{r} & 0   \\
 \end{array} \right) [P_r - \frac{i}{2r} \pi (2j+1) \left ( \begin{array}{cc}
 1 & 0   \\
 0 & -1   \\
 \end{array} \right)] + \left ( \begin{array}{cc}
 1 & 0   \\
 0 & -1   \\
 \end{array} \right) m_0 + V(r) \rbrace \psi^{jm}_{l} = E \psi _{l}^{jm} \]

\[\psi _{l}^{jm} = \frac{1}{r} \left ( \begin{array}{cc}
 F & Y_{l}^{jm}   \\
 iG & Y_{l}^{jm}   \\
 \end{array} \right) \quad P_r = -\frac{i}{r} \frac{\partial}{\partial r} r\]

\[\lbrace \left ( \begin{array}{cc}
 0 & \overrightarrow{\sigma} \cdot \hat{r}   \\
 \overrightarrow{\sigma} \cdot \hat{r} & 0   \\
 \end{array} \right) P_r \frac{1}{r} \left ( \begin{array}{cc}
 F & Y_{l}^{jm}   \\
 iG & Y_{l^{\prime}}^{jm}   \\
 \end{array} \right) - \frac{i}{2r} \pi (2j+1) \left ( \begin{array}{cc}
 0 & -\overrightarrow{\sigma}\cdot \hat{r}   \\
 \overrightarrow{\sigma} \cdot \hat{r} & 0   \\
 \end{array} \right) \frac{1}{r} \left ( \begin{array}{cc}
 F & Y_{l}^{jm}   \\
 iG & Y_{l^{\prime}}^{jm}   \\
 \end{array} \right)\]

\[+ \left ( \begin{array}{cc}
 1 & 0   \\
 0 & -1   \\
 \end{array} \right) m_0 \frac{1}{r} \left ( \begin{array}{cc}
 F & Y_{l}^{jm}   \\
 iG & Y_{l^{\prime}}^{jm}   \\
 \end{array} \right) + V(r) \frac{i}{r} \left ( \begin{array}{cc}
 F & Y_{l}^{jm}   \\
 iG & Y_{l^{\prime}}^{jm}   \\
 \end{array} \right) \rbrace = E \frac{1}{r} \left ( \begin{array}{cc}
 F & Y_{l}^{jm}   \\
 iG & Y_{l^{\prime}}^{jm}   \\
 \end{array} \right) \]

\[\overrightarrow{\sigma} \cdot \hat{r} |_{\theta = 0} = \left ( \begin{array}{cc}
 1 & 0   \\
 0 & -1   \\
 \end{array} \right) \]

\[Y_{l+1/2}^{m} (0,\phi) = \frac{1}{\sqrt{2l+1}} \left ( \begin{array}{cc}
 \sqrt{\frac{2l+1}{4\pi}} \delta m_0 \sqrt{l+m+1/2}    \\
 \sqrt{\frac{2l+1}{4\pi}} \delta m_0 \sqrt{l-m+1/2}   \\
 \end{array} \right) \]

\[= \delta \frac{1}{\sqrt{4 \pi}} \left ( \begin{array}{cc}
 \sqrt{l+m+1/2}   \\
  \sqrt{l-m+1/2}  \\
 \end{array} \right) \]

\[Y_{l-1/2}^{m} (0,\phi) = \frac{\delta m_0 }{\sqrt{4\pi}} \left ( \begin{array}{cc}
 -\sqrt{l-1/2}   \\
 \sqrt{l+1/2}   \\
 \end{array} \right) \]

Si $m=0$

\[Y_{l+1/2} ^{0} (0,\phi) = \frac{\sqrt{l+1/2}}{\sqrt{4\pi}} \left ( \begin{array}{cc}
 1   \\
 1   \\
 \end{array} \right) Y_{l-1/2}^{0} (0,\theta) = \frac{\sqrt{l+1/2}}{4\pi} \left ( \begin{array}{cc}
 -1   \\
 1   \\
 \end{array} \right)  \]

\[\overrightarrow{\sigma} \cdot \hat{r} Y_{l+1/2}^{m} (\theta , \phi) |_{\theta = 0} = - Y_{l-1/2}^{m} (\theta ,\phi )|_{\theta = 0}\]

Utilizando la identidad $\hat{r} = (\sin \theta \cos \phi , \sin \theta \sin \phi , \cos \theta)$

\[\hat{r} |_{\theta = 0} = (0,0,1) \quad \overrightarrow{\sigma} \cdot \hat{r} |_{\theta = 0} \sigma_{z}\]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Ondas Libres}

\[[- \frac{d}{dr} + \frac{\pi}{2r} (2j+1) ] G = (E-m_0 )F\]

\[[\frac{d}{dr} + \frac{\pi}{2r} (2j+1)] F = (E+m_0 ) G\]

\[[\frac{d^2}{dr^2} - \frac{l(l+1)}{r^2} + k^2] F_l = 0 \quad ; \quad F_l (kr) = kr j_l (kr) A\]

nótese que en el segundo término de lado derecho de la ecuación, se encuentran las funciones especiales de Bessel esféricas; esto es

\[(F_{l}^{(\rho)}) \equiv \sqrt{\frac{\pi}{2}} \rho^{1/2} J_{l+1/2}^{(\rho)}\]

donde se pueden encontrar las funciones de Bessel normales, las cuales son solución a la ecuación diferencial

\[y^{\prime \prime}+\frac{1}{\rho} y^{\prime} + (1- \nu/\rho^{2}) y\]

Sea $\rho \equiv kr$

\[\frac{d}{dr} = \frac{d f}{dr} \frac{d}{d \phi} = k \frac{d}{d\rho}\]

\[\frac{d^2}{dr^2} = k^2 \frac{d^2}{d \rho ^2}\]

\[[\frac{d^2}{d\rho ^2} - \frac{l(l+1)}{\rho^2} + 1 ] F_l = 0 \Rightarrow \quad [-\frac{d^2}{d \rho^2} + \frac{l(l+1)}{\rho ^2} ] F_l = F_l\]

Sea 

\[Q^{-} \equiv - \frac{d}{d\rho } + \frac{l}{\rho} \quad \text{y} \quad Q^{+} \equiv \frac{d}{d \rho } + \frac{l}{\rho }\]

\[Q^{-}Q^{+} = (- \frac{d}{d \rho } + \frac{l}{\rho})(\frac{d}{d\rho} + \frac{l}{\rho }) = - \frac{d^2}{d \rho ^2 } + \frac{l}{\rho ^2} + \frac{l^2}{\rho ^2}\]

\[= - \frac{d^2}{d \rho ^2} + \frac{l(l+1)}{\rho ^2}\]

\begin{equation}
Q^{-}Q^{+} F_{l} = F_{l}
\end{equation}

\[Q^{+}Q^{-} = (\frac{d}{d\rho} + \frac{l}{\rho })(- \frac{d}{d\rho } + \frac{l}{\rho })= - \frac{d^2 }{d \rho ^2 } - \frac{l}{\rho^2 } + \frac{l^2 }{\rho ^2 }\]

\[= - \frac{d^2}{d \rho^2 } + \frac{l(l-1)}{\rho ^2}\]

\begin{equation}
Q^+ Q^- F_{l-1} = F_{l-1}
\end{equation}

\[Q^+ Q^- (Q^+ F_l ) = (Q^+ F_l) \quad \Rightarrow \quad F_{l-1} = Q^+ F_l\]

\begin{equation}
F_{l-1} = (\frac{d}{d\rho } + \frac{l}{\rho}) F_l
\end{equation}

análogamente

\begin{equation}
F_l = (- \frac{d}{d\rho} + \frac{l}{\rho})F_{l-1}
\end{equation}

\[G = \frac{1}{E+m_0 } [\frac{d}{dr} + \frac{\pi}{2r} (2j+1)] F_l \]

encontrandose dos casos:

\begin{itemize}
\item $pi = 1$

\[G = \frac{1}{E+m_0 } (\frac{d}{dr} + \frac{(j+1/2)}{r}) F \quad l = j + 1/2 \pi = j+1/2 \]

\[G_{l^{\prime} = l-1} = \frac{1}{E+m_0 } (\frac{d}{dr} + \frac{l}{r})F_l = \frac{k}{E+m_0 } Q^+ F_l = \frac{k}{E+m_0 } F_{l-1}\]

\[G = G_{l^{\prime}} = \frac{k}{E+m_0 } F_{l-1=l^{\prime }}\]

\item $\pi = -1$
\end{itemize}

\[G = \frac{1}{E+m_0 }[ \frac{d}{dr} - \frac{1}{r}(j+1/2)] F = - \frac{k}{E+m_0 } (-\frac{d}{d\rho } + \frac{l^{\prime}}{\rho})F_{l-1=l}\]

\[l = j +1/2 \pi = j-1/2 \quad ; \quad l^{\prime} = j-1/2 \pi = j +1/2 \quad ; \quad l = l^{\prime} -1\]

\[G_{l^{\prime}} = - \frac{k}{E+m_0 } F_{l^{\prime}}\]

\[\text{en ambos casos} \quad G = G_{l^{\prime}} = \frac{\pi k}{E + m_0 } F_{l^{\prime}}\]

\[\psi _{R}^{jm} = \frac{1}{r} \left ( \begin{array}{cc}
 F_{k}Y_{l}^{jm}   \\
 iG Y_{l^{\prime}}^{jm}   \\
 \end{array} \right) \quad l = j+1/2 \pi \quad l^{\prime} = j -1/2 \pi \]

\[\int \psi _{R}^{\dagger jm} \psi _{k^{\prime}}^{j^{\prime}m^{\prime}} d^3 r = \begin{cases}
    \delta_{j j^{\prime}} \delta_{m m^{\prime}} \delta (E-E^{\prime})\\
    \delta _{jj^{\prime}} \delta_{mm^{\prime}} \delta (k-k^{\prime})\\
  \end{cases}\]

\[\int dr F_{k} F_{k^{\prime }} \int Y_{l}^{jm}(\Omega ) Y_{l^{\prime }}_{j^{\prime } m^{\prime }}(\Omega ) d \Omega + \int _{0}^{\infty } dr G_{k} G_{k^{\prime }} \int Y_{l^{\prime }}^{jm}(\Omega ) Y_{l^{\prime }}^{j^{\prime } m^{\prime }} (\Omega ) d \Omega \]

\[= \int _{0}^{\infty} [F_{k} F_{k^{\prime}} + G_{k} G_{k^{\prime}}] \delta _{j j^{\prime}} \delta_{m m^{\prime}}\]

\[\int_{0}^{\infty} [F_k F_k{^{\prime}} + G_{k} G_{k^{\prime}}] = \begin{cases}
    \delta(k-k^{\prime})\\
    \delta(E-E^{\prime})\\
  \end{cases}\]

\[F_R = k _r j_l (kr) A\]

\[G_k = \frac{\pi}{E+m_0 } k^2 r j_{l^{\prime}}(kr)A\]

\[j_l (kr) = \sqrt{\frac{\pi}{2}}(kr)^{-1/2}J_{l+1/2} (kr)\]

con $\nu \equiv l + 1/2$ y $\nu ^{\prime} \equiv l^{\prime} 
 1/2$

\[\int_{0}^{\infty} \rho J_{\nu}(\alpha \rho) J_{\nu} (\alpha ^{\prime}\rho ) d \rho = \frac{1}{\alpha } \delta (\alpha . \alpha ^{\prime}) \]

Las funciones Bessel $J_{l+1/2}$ cumplen con:

\begin{equation}
\lbrace \frac{d^2}{d \rho ^2} + \frac{1}{\rho} \frac{d}{d \rho } + [1- \frac{(l+1/2)^2}{\rho ^2}] \rbrace J_{l+1/2} = 0
\end{equation}

\[F_{R} = \sqrt{\frac{\pi}{2}}(kr)^{1/2} A J_{l+1/2} (kr)\]

\[G_k = \sqrt{\frac{\pi}{2}} \frac{\pi}{E+m_0 } (kr)^{1/2} A J_{l^{\prime}+1/2} (kr)\]

\[\frac{\pi}{2} |A|^2 k \int _{0}^{\infty} dr r[J_r (kr) J_{\nu} (k^{\prime}r)+ \frac{k^{\prime}}{(E+m_0 )(E^{\prime}+m_0 )} J_{\nu^{\prime}} (kr) J_{\nu^{\prime}}(k^{\prime}r)\]

\[= \frac{\pi}{2} |A|^2 k [1 + \frac{k^2}{(E+m_0 )^2}] \frac{1}{k} \delta (k-k^{\prime}) = \begin{cases}
    \delta (k-k^{\prime})\\
    \delta(E-E^{\prime})\\
  \end{cases}\]

\[\text{con} \quad k^2 = (E-m_0 )(E+m_0 )\]

\[= \pi |A|^{2} \frac{E}{E+m_0 } \delta (k-k^{\prime}) = \begin{cases}
    \delta (k-k^{\prime})\\
    \delta(E-E^{\prime})\\
  \end{cases} \]

normalizando a $\delta (k-k^{\prime}):$

\[|A|^{2} = \frac{E+m_0 }{\pi E}\]

\begin{equation}
\Rightarrow A = \frac{|E+m_0 |^{1/2}}{\sqrt{\pi}|E|^{1/2}}
\end{equation}

en el límite no relativista.

\[F_k = \frac{1}{\sqrt{2}} k^{1/2} \frac{|E+m_0 |^{1/2}}{|E|^{1/2}} r^{1/2} J_{\nu} (kr)\]

\[\text{i.e.} \quad F_{l}(kr) = \sqrt{\frac{k}{2|E|}} \sqrt{|E+m_0 |^{1/2}} r^{1/2}J_{\nu}(kr)\]

\[G_k = \frac{\pi k}{E+m_0 } (kr)^{1/2} \sqrt{\frac{\pi}{2}}J_{\nu^{\prime}}(kr) \frac{|E+m_0 |^{1/2}}{\sqrt{\pi}|E|^{1/2}}\]

\[= \pi \sqrt{\frac{k}{2|E|}}\frac{k}{E+m_0 }|E+m_0 |^{1/2} J_{\nu^{\prime}}(kr) r^{1/2}\]

\[k = [|E+m_0 |^{1/2} |E-m_0 |^{1/2} ] \quad  \frac{k}{e+m_0 } |E+m_0 |^{1/2} = \epsilon |E-m_0 |^{1/2}\]

como $\epsilon \equiv E/|E|$

\[G_l (kr) = \pi \epsilon \sqrt{\frac{k}{2|E|}} \sqrt{|E-m_0|} J_{\nu^{\prime}} (kr) r^{1/2}\]

\[\psi (r,\theta , \phi) = \sqrt{\frac{k}{2|E|r}} \left ( \begin{array}{cc}
 \sqrt{|E+m_0 }| J_{j+1/2 \pi+1/2}Y_{l=j+1/2\pi}^{j,m}   \\
 i pi \epsilon \sqrt{|E-m_0 }| J_{l^{\prime}+1/2}Y_{l^{\prime}=j-1/2\pi}^{j,m}   \\
 \end{array} \right)  \]

Tarea: Si se normaliza a $\delta (E-E^{\prime})$ demostrar que

\[|A| = |\frac{E+m_0 }{E-m_0 }|^{1/4} \sqrt{\pi}\]

Tarea:
Encontrar el comportamiento asintótico de

\[F_{l}(kr) \sim r \rightarrow \infty k \sqrt{\frac{2}{k}} \sin (kr - \pi l/2)|A|\]

\[G_{l^{\prime}} \sim \rightarrow \infty \frac{\pi k}{E+m_0 } \sin (kr - \pi l^{\prime}/2)|A|\]

es una superposición de una onda plaa entrante con una saliente.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Normalización relativista.} 

\[\psi _{k}^{jm} = \frac{1}{r} \left ( \begin{array}{cc}
  F_k Y_{l_{1}}^{jm}  \\
  iG_k Y_{l_{2}}^{jm}  \\
 \end{array} \right) \quad l_{1} = j+1/2 \epsilon \quad , \quad l_2 = j-1/2 \epsilon  \]

\[\int \overline{\psi}^{jm} \psi _{k^{\prime}}^{j^{\prime}m^{\prime}} d^{3}r = \begin{cases}
    \delta_{jj^{\prime}} \delta_{mm^{\prime}} \delta(E-E^{\prime})\\
    \delta_{jj^{\prime}} \delta_{mm^{\prime}} \delta(k-k^{\prime}) \\
  \end{cases} \quad , \quad k = \sqrt{(E^{2}-m_{0}^{2})}\]

\[\psi_{k^{\prime}}^{j^{\prime}m^{\prime}} = \frac{1}{r} \left ( \begin{array}{cc}
  F_{k^{\prime}} Y_{l_{1}^{\prime}}^{j^{\prime}m^{\prime}}  \\
  iG_{k^{\prime}} Y_{l_{2}^{\prime}}^{j^{\prime}m^{\prime}}  \\
 \end{array} \right) \]

\[= \int _{0}^{\int} dr F_{k}^{*} F_{k}^{\prime} \int Y_{l_{1}}^{*jm} (\theta , \phi) Y_{l_{2}^{\prime}}^{j^{\prime}m^{\prime}} (\theta , \phi ) d \Omega\]

\[- \int _{0}^{\infty} dr G_{k}^{*}G_{k^{\prime}} \in Y_{l_{2}}^{*jm} (\theta , \phi ) Y_{l_{2}^{\prime}}^{j^{\prime}m^{\prime}} (\theta , \phi ) d \Omega \]

\[= \int _{0}^{\infty} dr [F_k ^* F_{k^{\prime}} - G_{k}^{*}G_{k^{\prime}}] \delta _{jj^{\prime}} \delta _{mm^{\prime }}\]

\[\int_{0}^{\infty} dr [F_{k}^{*}F_{k^{\prime}}- G_{k}^{*}G_{k^{\prime}}] = \begin{cases}
    \delta(k-k^{\prime})\\
    \delta(E-E^{\prime}) \\
  \end{cases} \]

\[F(kr)=Akr j_{l_{1}}(kr) \quad j_{l_{1}}(kr)=\sqrt{\pi/2} (kr)^{-1/2}J_{l_{1}+1/2}(kr)\]

\[F(kr) = A \sqrt{\frac{\pi}{2}}(kr)^{1/2} J_{l_{1}+1/2 \equiv \nu} (kr)\]

\[G(kr) = \frac{\epsilon k}{(E+m_0 )} F_{l_{2}}(kr) = \frac{\epsilon k}{(E+m_0 )}A \sqrt{\frac{\pi}{2}}(kr)^{1/2} J_{l_{2}+1/2 \equiv \nu ^{\prime}} (kr)\]

Se usará que 

\[\int _{0}^{\infty} \rho J_{\nu} (\alpha \rho )J_{\nu} (\alpha^{\prime} \rho ) d \rho = \frac{1}{\alpha} \delta (\alpha - \alpha ^{\prime}) \quad ; \quad \rho \equiv kr\]

\begin{equation}
\therefore \int _{0}^{\infty } dr |A|^2 \pi /2 [(kr)^{1/2} (kr)^{1/2} J_{\nu}^{*}(kr) J_{\nu} (k^{\prime}r) - \frac{kk^{\prime}}{(E+m_0 )(E^{\prime}+m_0 )}(k r)^{1/2}(kr)^{1/2} J_{\nu^{\prime}}^{*}(kr)J_{\nu^{\prime}}(kr)]
\end{equation} 

\[= |A|^2 \pi/2 (1- \frac{k^2 }{(E+m_0 )^2}) \delta(k-k^{\prime}) = \begin{cases}
    \delta(E-E^{\prime})\\
    \delta(k-k^{\prime}) \\
  \end{cases} \]

\[\text{como} \quad k^2 = \sqrt{E^2 - m_{0}^{2}} = \sqrt{(E-m_0 )(E+m_0 )} = \sqrt{|(E-m_0 )(E+m_0 )|}\]

\[= |E-m_0 |^{1/2} |E+m_0 |^{1/2} \quad \text{pasando } \quad l_{1}\rightarrow l \quad l_{2} \rightarrow l^{\prime}\]

\[\text{queda:} \quad F_l (kr) = A (kr) j_l (kr)\]

\[= \frac{|E+m_0 |^{1/2}}{\sqrt{\pi m_0 }}|E-m_0 |^{1/2} |E+m_0 |^{1/2} r j_l (kr)\]

\begin{equation}
F_l (kr) = \frac{|E+m_0 |}{\sqrt{\pi m_0}} |E-m_0 |^{1/2} r j_{l}(kr)
\end{equation}

\[\text{para} \quad G_{l^{\prime}}(kr) = \frac{\epsilon k}{(E+m_0 )} F_{l^{\prime}} (kr) = \frac{\epsilon k}{(E+m_0 )} \frac{|E+m_0 |}{\sqrt{\pi m_0 }} |E- m_0 |^{1/2} r j_{l^{\prime}} (kr)\]

pero

\[\frac{k}{(E+m_0 )} |E+m_0 | = \frac{|E-m_0 |^{1/2}}{(E+m_0 )} |E+m_0 |^{3/2} \]

\[= \begin{cases}
     |E-m_0 | ^{1/2} |E+m_0 |^{1/2} \quad \text{si} \quad E>0  \\
   - |E-m_0 | ^{1/2} |E+m_0 |^{1/2} \frac{|E+m_0 |}{|E+m_0 |} \\
  \end{cases} = - |E-m_0 | ^{1/2} |E+m_0 |^{1/2} \quad \text{si} \quad E<0 \]

\[= \sigma k \quad \text{donde} \quad \sigma \equiv E/|E| = \begin{cases}
 +1 \quad E>0 \\
 -1 \quad E<0 \\
\end{cases}\]

\[\Rightarrow G_{l^{\prime}}(kr) = \frac{\epsilon \sigma k}{ \sqrt{\pi m_0 }} |E-m_0 |^{1/2} r j_{l^{\prime}}(kr)\]

\[\therefore \psi = \frac{k}{\sqrt{\pi m_{0} }} \begin{array}{cc}
  \sqrt{\vert E+m_{0} \vert } j_l (kr) Y_{l^{\prime }}^{jm}(\theta , \phi )  \\
  i \epsilon \sigma \sqrt{\vert E-m_0 \vert } j_{l^{\prime }} (kr) Y_{l^{\prime }}^{jm}(\theta , \phi )   \end{array} \right) \]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{\'Atomo de hidr\'ogeno}

Usaremos las siguientes convenciones. (Unidades at\'omicas, $x,y,z$, coord. vel.)

\[\rho \equiv k r \quad k \equiv \sqrt{m^2 - E^2}\]

\[V(r) \equiv - \frac{z \alpha}{r} \equiv \nu \equiv \sqrt{\frac{m-E}{m+E}}\]

\[\zeta \equiv z \alpha \equiv s = \sqrt{\tau_{j}^{2} - \zeta^{2}} \quad \tau _{j}\equiv \epsilon (j+1/2)\]

\[\mu = \frac{\zeta E}{\sqrt{m^2 - E^2 }} + 1/2\]

las ecuaciones son

\begin{equation}
(- \frac{d}{dt} + \frac{\tau _j }{r}) G + (m - \frac{\zeta}{r})F = EF
\end{equation}

\begin{equation}
( \frac{d}{dt} + \frac{\tau _j }{r}) F - (m + \frac{\zeta}{r})G = EG
\end{equation}

\begin{equation}
(- \frac{d}{d\rho} + \frac{\tau _j }{\rho}) G + (m - \frac{E}{k} - \frac{\zeta}{\rho })F = 0
\end{equation}

como el denominador del segundo término de lado izquierdo de la igualdad equivale a $\sqrt{m-E} \sqrt{m+E}$

\begin{equation}
(\frac{d}{d\rho} + \frac{\tau _j }{\rho})G + (\nu - \frac{\zeta}{\rho })F = 0
\end{equation}

\[((\frac{d}{d\rho } + \frac{\tau _j}{\rho }) F - (\frac{m+E}{k} + \frac{\zeta}{\rho }) G = 0\]

\begin{equation}
(\frac{d}{d\rho } + \frac{\tau _j }{\rho }) F - (\nu ^{-1} + \frac{\zeta}{\rho})G = 0
\end{equation}

Proponemos entonces

\[F(\rho) = \sqrt{m + E} (\psi _{+} - \psi _{-})\]

\[G(\rho) = \sqrt{m-E} (\psi _+ + \psi _- )\]

\[\text{tomemos} \quad (\frac{d}{d\rho} + \frac{\tau _j }{\rho}) F = (\nu ^{-1} + \frac{\zeta}{\rho})G\]

\[(\frac{d}{d\rho} + \frac{\tau _j }{\rho})(\psi _+ - \psi _-) = (\nu^{-1} + \frac{\zeta}{\rho}) \nu (\psi_+ + \psi _- )\]

\begin{equation}
= (1+ \frac{\zeta \nu}{\rho})(\psi _+ + \psi _- )
\end{equation}

ahora la otra que se hace de la misma forma

\begin{equation}
(- \frac{d}{d\rho} + \frac{\tau _j }{\rho})(\psi _+ + \psi _-) = (-1 + s \nu ^{-1}/\rho)(\psi _+ - \psi _-)
\end{equation}


\[2 \frac{d \psi_+ }{d \rho} - 2 \frac{\tau _j }{\rho} \psi _-\ = 2 \psi _+ + \frac{\zeta}{\rho}[\nu (\psi _+ + \psi _- )- \nu^{-1}(\psi _+ - \psi _- )\]

se puede notar que el segundo término en la suma de lado izquierdo de la igualdad es equivalente a

\[\sqrt{\frac{m-E}{m+E}} (\psi _+ + \psi _-) - \sqrt{\frac{m+E}{m-E}} (\psi _+ - \psi _-)\]

que a su vez equivale a escribir

\[(-2E \psi _+ + 2m \psi _- ) \frac{1}{\sqrt{m^2 - E^2 }}\]

\[\Rightarrow \lbrace \rho \frac{d}{d\rho} - \rho + \frac{\zeta E}{\sqrt{m^2 - E^2 }} \rbrace \psi _+ = (\frac{m \zeta }{\sqrt{m^2 - E^2}} + \tau _j ) \psi _- \]

Ahora 

\[-2 \frac{d}{d\rho} - \psi _- + 2 \frac{\tau _j }{\rho} \psi _+ = 2 \psi _- + \frac{\zeta}{\rho} [\nu (\psi _+ + \psi _- ) + \nu ^{-1} (\psi _+ - \psi _- )]\]

\[\sqrt{\frac{m-E}{m+E}} (\psi _+ + \psi _- ) + \sqrt{\frac{m+E}{m-E}} (\psi _+ - \psi _- )\]

\[\frac{1}{\sqrt{m^2 E-^2}} [2m \psi _+ - 2e \psi _- ]\]

\begin{equation}
\Rightarrow (- \rho \frac{d}{d\rho} - \rho + \frac{\zeta E}{\sqrt{m^2 - E^2}}) \psi _- = (\frac{\zeta m}{\sqrt{m^2 . E^2}} - \tau _{j})\psi _+ 
\end{equation}

las desacoplamos ahora:

\[(- \rho \frac{d}{d\rho} - \rho + \frac{\zeta E}{\sqrt{m^2 - E^2 }}) (\rho \frac{d}{d\rho} -\rho + \frac{\zeta E}{\sqrt{m^2 - E^2 }}) \psi _+ = (\frac{\zeta ^2 m^2 }{m^2 - E^2 - \tau ^{2}_{j}}) \psi _+ \]

\[[- \rho ^2 \frac{d^2 }{d \rho ^2 } - \rho \frac{d}{d \rho } + \rho + \rho^2 \frac{d}{d\rho} - \rho^2 \frac{d}{d \rho } + \rho ^2 - 2 \rho \frac{\zeta E}{\sqrt{m^2 - E^2 }} + \frac{\rho^2 E^2 }{m^2 - E^2 }] \psi_+  \]

\[= (\frac{\zeta^2 m^2 }{m^2 - E^2 })\psi_+ \quad \text{poniendo} \quad \mu \equiv \frac{\zeta E}{\sqrt{m^2 - E^2 }} + 1/2\]

\[\Rightarrow [-\rho ^2 \frac{d^2 }{d\rho ^2 } -\rho \frac{d}{d\rho } + \rho (\rho + 1)] - 2 \rho (\mu - 1/2 )] \psi _+ = (\zeta ^2 - \tau_{j}^{2})\psi _+ \]

\[[- \rho ^2 \frac{d^2 }{d \rho ^2} - \rho \frac{d}{d \rho } -2\rho (\mu - 1)+ \rho^2 + \tau_{j}^{2} - \zeta ^2 ] \psi _+ = 0 \]

ahora

\[\psi _+ (\rho ) \equiv \rho^s e^{- \rho } v(\rho )\]

\[\frac{d \psi _+}{d \rho } = s \rho^{s-1} e^{-\rho } v (\rho) - \rho^s e^{- \rho} v (\rho ) + \rho ^{s} e^{-\rho} \frac{dv}{d \rho }\]

\[\frac{d^2 \psi _+ }{d \rho ^2} = s(s-1) \rho ^{s-2} e^{-\rho } v (\rho ) - 2 s \rho ^{s-1} e^{-\rho } v (\rho )\]

\[+ s \rho ^{s-1} e^{-\rho} \frac{dv}{d\rho } + \rho^s e^{-\rho } v (\rho ) - \rho ^s e^{- \rho} \frac{dv}{d \rho} \]

\[+ s \rho ^{s-1} e^{- \rho} \frac{dv}{d \rho } - \rho ^s e^{- \rho } \frac{dv}{d \rho }\ + \rho ^s e^{- \rho} \frac{d^2 v}{d \rho ^2 }\]

\[- \rho^2 \frac{d^2 v}{d rho ^2} (2 \rho^2 - \rho -2s\rho ) \frac{dv}{d \rho }\]

\[+[2s\rho - s(s-1) - \rho ^2 + \rho - s - 2 \rho (\mu - 1) + \rho ^2 + \tau^{2}_{j} - \zeta ^2 ] v = 0\]

\[2 \rho [2s - (\mu -1)+1/2] -s^2 + \tau_{j}^{2}- \zeta ^2 \]

\[\Rightarrow \lbrace \rho^2 \frac{d^2 }{d \rho ^2 } + \rho (1 + 2s - 2 \rho) \frac{dv}{d \rho} 5+ (s^2 + \zeta ^2 - \tau _{j}^{2}) - 2 \rho [s-(\mu -1)+1/2] \rbrace v (\rho ) = 0\]

\[x = 2 \rho \quad v(\rho ) \rightarrow v (x/2) \equiv \mathscr{L} (x)\]

\[\frac{d \mathscr{L}}{dx} = \frac{1}{2} \frac{dv}{d \rho} (\rho)\]

\[\Rightarrow \lbrace x^2 \frac{d^2 }{dx^2 } + (1+2s-x) x \frac{d}{dx} + (s^2 + j^2 - \tau^{2}_{j}) - x[s-(\mu -1)+1/2]\rbrace \mathscr{L} (x) = 0\]

\[\lbrace x \frac{d^2}{dx^2} + (+12s-x) \frac{d}{dx} + \frac{(s^2 + \zeta ^2 - \tau _{j}^{2})}{x}-[s-(\mu -1)+1/2]) \mathscr{L}(x) = 0\]

nótese que el denominador en el tercer sumando de lado izquierdo de la igualdad diverje cuando $x \rightarrow 0$

\[s^2 + \zeta^2 - \tau _{j}^{2} = 0 \quad s = + \sqrt{- \zeta^2 + \tau_{j}^{2}} = + \sqrt{-z^2 \alpha ^2 + (j + 1/2)^2 }\]

\begin{equation}
\lbrace x \frac{d^2 }{dx^2 } + (1+2s-x)  \frac{d}{dx} + [(\mu - 1)-s -1/2]\rbrace \mathscr{L} (x) = 0
\end{equation}

$\mathscr{L} (x)$ debe ser un polinomio para evitar divergencias cuando $x \rightarrow \infty$. Este polinomio es el conocido polinomio generalizado de Laguerre o alterntivamente conocidos como polinomios de Sonin (Nikolay Yakovlevich Sonin). Esto dado que $\lbrace x \frac{d^2 }{dx^2 } - x \frac{d}{dx}\rbrace \mathscr{L}=0$ además, los términos $\mathsrc{L} \sim e^x = e^{2 \rho } \rightarrow \infty$ cuando $\rho \rightarrow \infty$ se desecian, entonces

\[\mu - s - 1/2 \equiv n \quad \text{con} \quad n = 0,1,2,\cdots \]

tomando el término de lado derecho de la igualdad se puede obtener el espectro de energía. Cabe señalar que no se trata estrictamente del polinomio asociado de Lagerre usual, dado que $s \notin \mathfrak{Z}$

\[\psi _+ (\rho ) = a \rho ^{s} e^{- \rho } \mathsrc{L} _{n-1}^{2s}(2 \rho ) \]

análogamente

\begin{equation}
\psi _- (\rho ) b \rho ^s e^{- \rho } \mathsrc{L} _{n}^{2s}(2 \rho )
\end{equation}

Nota: La ecuación diferencial para los polinomios asociados "usuales" es

\[[x \frac{d^2 }{dx^2 } + (1 + k - x)\frac{d}{dx}+n] \mathsrc{L}_{m}^{k}(x)=0 \]

\[F(\rho ) = \sqrt{m + E} \rho ^2 e^{-\rho }(-a \mathsrc{L}_{n-1}^{2s}(2 \rho )+b\mathsrc{L}_{n}^{2s}(2 \rho ))\]

\[G(\rho ) = \sqrt{m - E} \rho ^2 e^{-\rho }(a \mathsrc{L}_{n-1}^{2s}(2 \rho )+b\mathsrc{L}_{n}^{2s}(2 \rho ))\]

ahora $a$ y $b$ se determinan de la condición de renormalización y de pedir que $F$ y $G$ sean soluciones de las ecuaciones originales, lo que determina $a/b$; es decir

\begin{equation}
(\frac{d}{d \rho } + \frac{\tau _{j}}{\rho})F = (\nu ^{-1} + \frac{\zeta}{\rho})G 
\end{equation}
y
\begin{equation}
(-\frac{d}{d \rho } + \frac{\tau _{j}}{\rho})G = (\nu - \frac{\zeta}{\rho})F
\end{equation}

\subsection{Tarea:} Usando la ecuación (6.12) obtener

\[a[\frac{\tau_j - s - \rho \nu ^{-1}}{\rho } + 2]\mathsrc{L}_{n-1}^{2s} (2 \rho ) + b [\frac{\tau _j - s + \zeta \nu ^{-1}}{\rho}]\mathsrc{L}_{n}^{2s}(2\rho )-2a \mathsrc{L}_{n-1}^{\prime 2s}(2 \rho )-2b \mathsrc{L}_{n}^{\prime 2s} (2 \rho) \]

\textbf{Solución}: Derivando con respecto a $x$

\[\mathsrc{L}_{n-1}^{\prime 2s} (2 \rho ) = \mathsrc{L}_{n-1}^{2s}(2 \rho )+\mathsrc{L}_{n}^{\prime 2s} (2 \rho)\]

sustituimos $\mathsrc{L}_{n-1}^{\prime 2s}(2 \rho ):$

\[\Rightarrow a (\tau_j - s - \zeta \nu ^{-1})\mathsrc{L}_{n-1}^{2s}(2 \rho ) + b(\tau_j - s + \rho \nu ^{-1}) \mathsrc{L}_{n}^{2s} (2 \rho )\]

\[-2(a+b)\rho \mathsrc{L}_{n}^{\prime 2s}(2 \rho ) = 0\]

pero

\[-2 \rho \mathsrc{L}_{n}^{\prime 2s}(2 \rho) = (n + 2s)\mathsrc{L}_{n-1}^{2s}(2 \rho )-n \mathsrc{L}_{n}^{2s} (2 \rho )\]

\[\Rightarrow [a(\tau_j - s - \zeta \nu ^{-1})+(a+b)(n+2s)]\mathsrc{L}_{n-1}^{2s}(2 \rho )\]

\[+[b(\zeta _j - s + \zeta \nu ^{-1})-n(a+b)]\mathsrc{L}_{n}^{2s}\mathsrc{L}_{n}^{2s}(2\rho )=0 \]

\[\Rightarrow [a(\tau _j + s - \zeta \nu ^{-1} + n) + b (n+2s)]\mathsrc{L}_{n-1}^{2s}(2 \rho )\]

\[+[b(\tau _j -s + \zeta \nu ^{-1} - n) -an] \mathsrc{L}_{n}^{2s}(2 \rho) = 0\]

\[\Rightarrow a (\tau _j + s - \zeta \nu ^{-1} + n) + b (n+2s) = 0 \]

\[b(\tau _j - s + \zeta \nu ^{-1} -n) -an = 0\]

\[a = \frac{b}{n} (\tau _j - s + \zeta \nu ^{-1} -n) \Rightarrow \]

\[(\tau _j - s + \zeta \nu ^{-1} - n) (\tau _j + s - \zeta \nu ^{-1}+n)+n(n+2s) = 0\]

\[\tau_{j}^{2}-(n+s-\zeta \nu ^{-1})^2 + n (n+2s) = 0 \quad \tau _{j}^{2}-s^2 = \zeta ^2 \]

\[\tau _{j}^{2}-(n+s)^2 + 2 (n+s) \zeta \nu ^{-1} - \zeta ^2 \nu ^{-2} + n^2 + 2sn = 0\]

\[s^2 + 2(n+s)\zeta \nu ^{-1} - \zeta ^{2} \nu ^{-2} = 0 \quad \nu \equiv \sqrt{\frac{m-E}{m+E}}\]

\[\zeta ^2 (1- \frac{m+E}{m-E}) + 2 \zeta (n+s)\nu^{-1} = 0\]

\[- \zeta \frac{(2E)}{m-E} + 2 \nu ^{-1}(n+s)= 0\]

\[\frac{\zeta ^2 E^2}{(m-E)^2 } = (n+s)^2 \frac{m+E}{m-E}\]

\[\rho ^2 E^2 = (n+s)^2 (m+E)(m-E)\]

\[\zeta ^2 E^2 = (n+s)^2 (m^2 -E^2)\]

\[E^2 [\zeta ^2 + (n+s)^2]=m^2 (n+s)^2\]

\[E = m (n+s) [\zeta ^2 + (n+s)^2 ]^{1/2}\]

\[=m[\frac{1}{(n+s)^2}]^{-1/2} [\zeta ^2 + (n+s)^2 ]^{-1/2} \]

\begin{equation}
E = m[1+\frac{\zeta ^2 }{(n+s)^2}]^{-1/2}
\end{equation}

Tomemos

\[b = \frac{-a}{(n+2s)}(\tau _{j}+s+n-\zeta \nu ^{-1}) \Rightarrow\]

\[F(\rho ) = \sqrt{m+E} a \rho^s e^{-\rho } [\mathscr{L}_{n-1}^{2s}(2\rho )+ \frac{(\tau_j + s + n - \zeta \nu^{-1})}{(n+2s)} \mathscr{L}_{n}^{2s}(2 \rho) \]

que se reescribirá como

\[F(\rho ) c \sqrt{m+E} \rho ^s e^{- \rho } [(n+2s)(\tau_j + s + n- \zeta \nu^{-1})^{-1/2} \mathscr{L}_{n-1}^{2s} (2\rho ) + (\tau _j + s + n-\zeta \nu^{-1})^{1/2} \mathscr{L}_{n}^{2s}(2\rho)]\]

que es una expresión simétrica. La otra ecuación resulta:

\[G(\rho) = \sqrt{m-E}a \rho^s e^{-\rho } [\mathscr{L}_{n-1}^{2s}(2\rho)- \frac{(\tau:j + s + n-\zeta \nu^{-1})}{(n+2s)} \mathscr{L}_{n}^{2s}(2\rho)\]

\[= c \sqrt{m-E} \rho^s e^{- \rho} [(n+2s)(\tau_{j}+s+n-\zeta \nu^{-1})^{-1/2} \mathscr{L}_{n-1}^{2s}(2\rho)- (\tau _j + s + n-\zeta \nu ^{-1})^{1/2} \mathscr{L}_{n}^{2s}(2\rho)]\]

entonces:

\[F(\rho )= C \sqrt{m + E} \rho^s e^{-\rho} [En \mathscr{L}_{n}^{2s}(2\rho) + B_n \mathscr{L}_{n-1}^{2s}(2\rho)]\]

\[G(\rho) -c \sqrt{m-E} \rho^s e^{-\rho } [S_n \mathscr{L}_{n}^{2s}(2\rho) - B_n \mathscr{L}_{n-1}^{2s}(2\rho)]\]

\[\text{donde} \quad A_n = (\tau _j +s+n-\zeta \nu^{-1})^{1/2} \quad , \quad B_n (n+2s)(\tau _j + s + n-\zeta \nu^{-1})^{-1/2}\]

\textbf{Tarea}:

\[|c| = \frac{2^{s-1}}{z \alpha} \sqrt{\frac{n!k}{2m^3}} \Gamma^{-1/2} (n+2s+1)\]

\[\Gamma(x) \quad \text{es la función gama de } \quad x \]

\textit{sugerencia}: 

\[\int _{0}^{\infty} dx e^{-x} x^{2s} \mathscr{L}_{n}^{2s}(2 \rho ) \mathscr{L}_{n_{1}}^{2s}(2 \rho ) = \frac{\Gamma(n+2s+1)}{n!} \delta_{mn}\]

Retomando la expresi\'on para la energ\'ia 

\[E = , [1+ \frac{\zeta ^2 }{(n+s)^2 }]^{-1/2} \quad n = 0,1,2, \cdots\]

definiendo a $N$ y $\epsilon_j $ como

\[N= j+1/2 + n \quad n = ,1,2, \cdots \]

\[N-\epsilon = s+n \quad \tau _j = \epsilon (j+1/2) \quad sa = \sqrt{\tau_{j}^{2} \zeta ^2 } \quad \zeta \equiv ze^2\]

\[\Rightarrow j + 1/2 + n - \epsilon_k = s+n \quad \epsilon _j = j +1/2 - s\]

\[\Rightarrow E_{nj} = m [1 + \frac{z^2 e^4 }{(N-\epsilon _j )^2 }]^{-1/2}\]

\[\epsilon_j = j +1/2 - \sqrt{(j+1/2)^2 - z^2 e^4} \neq n \quad \text{tal que} \quad n\notin \mathfrak{Z} \quad \text{sea entero}\]

entonces

\[j = \frac{1}{2}, \frac{3}{2} , \cdots N-1/2 \quad N=1,2, \cdots \quad \text{se llama número cuántico principal}\]

\[N=1 \quad , \quad m = 0 \quad \text{y} \quad j = 1/2 \]

\[l = j + \epsilon/2 = 1/2 + 1/2 \epsilon = 1/2+1/2 = 1 \quad \text{si } \epsilon = 1 > j ! \quad \text{no es posible}\]


\[l^{\prime } = 1/ - \epsilon/2  \quad \text{sólo es posible si } \quad \epsilon = -1 \]

\[\Rightarrow l = 0 \quad , \quad \epsilon=-1 \quad (n l_{j}(\epsilon))\]

\[N = 1 \quad j = 1/ \quad 1 S_{1/2}(\epsilon =-1)\]

\[\text{para} \quad N=2 \quad , \quad 2 = j+1/2 + n \quad , \quad \Rightarrow n = 0 , 1, \quad j = 3/2 , 1/2 \quad \text{respectivamente }\]

\[ \text{si} \quad j = 3/2 (n=0) l=3/2 + 1/2 \epsilon =
  \begin{cases}
    2       & \quad \text{si } \epsilon = 1 ! \quad \text{no es posible}\\
    1  & \quad \text{si } \epsilon = -1\\
  \end{cases}
\]

Entonces

\[j = 1/2 \quad 2 S_{1/2} (\epsilon = -1) \quad 2 P_{1/2 }(\epsilon=1)\]

\[N=2 \quad j = 3/2 \quad 2 P_{3/2}(\epsilon =-1 ) \quad \text{etc.}\]

El resultado es entonces

\begin{center}
    \begin{tabular}{ | l | l | l | p{5cm} |}
    \hline
    $N$ & $j$ & $l(\epsilon)$ $0$ $1$ $2$ & $n$ \\ \hline
    $N=1$ & $1/2$& $1S_{1/2}(-)$ & $0$ \\ \hline
    $N=2$ & $1/2$& $2S_{1/2}(-)2P_{1/2}(+)$ & $1$ \\ \hline
     & $2P_{3/2}(-)$ & $0$ \\ \hline
     $N=3$ & $1/2$ & $3S_{1/2}(-)3P_{1/2}(+)$ & $2$ \\ \hline
      & $3/2$ & $3P_{3/2}(-)3d_{3/2}(+)$ & $1$ \\ \hline
     & $5/2$ & $3d_{5/2}(-)$ & $0$ \\ \hline    
    \end{tabular}
\end{center}

\[E_{Nj} = m [1 + \frac{z^2 e^4 }{(N-j-1/2 +  \sqrt{(j+1/2)^2 - z ^2 e ^4})^2}]^{-1/2}\]

\[= m - \frac{z^2 e^4}{2(N-j-1/2 + \sqrt{(j+1/2)^2 . z^2 e^4})^2 }+ \cdots  \]

Límite no relativista $\alpha \rightarrow 0 $

\[E _{Nj} \sim m - \frac{z^2 e^4 }{2N^2} \]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Límite no relativista}

Como $\partial_{\mu} \rightarrow D_{\mu} = \partial_{\mu} + ie A_{\mu}$ cuando aparece un campo eléctrico o magnético, entonces, sea

\[\psi = \left ( \begin{array}{cc}
 \phi   \\
 \chi   \\
 \end{array} \right) \]

\[(E-e\phi)\psi = (\overrightarrow{\alpha} \cdot ()\overrightarrow{P}-e\overrightarrow{A})+ \beta m ] \psi \]

en la representación de Dirac

\begin{equation}
(E-e\phi) \phi = \overrightarrow{\sigma} \cdot (\overrightarrow{P} - e\overrightarrow{A})\chi + m \phi
\end{equation}

\begin{equation}
(E-e \phi) \chi = \overrightarrow{ \sigma} \cdot ( \overrightarrow{P} - e \overrightarrow{A}) \phi - m \chi
\end{equation}

Definiendo 

\[\overrightarrow{\pi = \overrightarrow{P}-e\overrightarrow{A}} \quad , \quad \pi^{0}= E-e \phi \]

\[ T= E-m \quad M \equiv \frac{1}{2} (m+\pi ^0)\]

\[= \frac{1}{2}m + \frac{1}{2} (E-e \phi )\]

\[= m + \frac{1}{2} (T-e\phi )\]

de la ecuación (6.15) se puede definir

\[\chi = \frac{\overrightarrow{\sigma} \cdot \overrightarrow{\pi}}{\pi ^{0} + m} \phi = \frac{\overrightarrow{\sigma} \cdot \overrightarrow{\pi }}{2m} \phi\]

y sustituyendo en (6.14)

\[\Rightarrow T \phi = [(\overrightarrow{\sigma} \cdot \overrightarrow{\pi}) \frac{1}{2m} (\overrightarrow{\sigma}\cdot \overrightarrow{\pi}) + e \phi ] \phi \]

En el límite no relativista $e \phi <<m$ esto implica $E \sim m$ y $\pi^{0} \sim m$ 

\[|\overrightarrow{\sigma} \cdot \overrightarrow{\pi}|<<m\]

esto implica que, para componentes pequeñas

\begin{equation}
x \rightarrow 0
\end{equation}

y, además

\[T \phi = [\frac{(\overrightarrow{\sigma} \cdot \overrightarrow{\pi})(\overrightarrow{\sigma} \cdot \overrightarrow{\pi})}{2m}+e \varphi] \phi \]

\[(\overrightarrow{\sigma} \cdot \overrightarrow{\pi})(\overrightarrow{\sigma} \cdot \overrightarrow{\pi}) = \overrightarrow{\pi}^{2} + i \overrightarrow{\sigma } \cdot (\overrightarrow{\pi} \times \overrightarrow{\pi})\]

\[= (\overrightarrow{P}-e \overrightarrow{A})^2  + i \overrightarrow{\sigma} \cdot (\overrightarrow{P}-e\overrightarrow{A}) \times (\overrightarrow{P}-e\overrightarrow{A})\]

\[(\overrightarrow{P}-e \overrightarrow{A}) \times (\overrightarrow{P}-e\overrightarrow{A}) = -e \overrightarrow{P} \times \overrightarrow{A} - e \overrightarrow{A} \times \overrightarrow{P}\]

\[= ie(\nabla \times \overrightarrow{A}) = ir \overrightarrow{B}\]

si $T \rightarrow H_{N.R.}$

\[H_{N.R.} \phi = [ \frac{(\overrightarrow{P}-e \overrightarrow{A})^2 }{2m} - \frac{e}{2m} \overrightarrow{\sigma} \cdot \overrightarrow{B} + e \varphi ] \phi \]


\[\text{como} \quad U_{may} = - \overrightarrow{\mu } \cdot \overrightarrow{B} \quad \text{y} \quad \overrightarrow{\mu } = 2 \mu _{B} \frac{\overrightarrow{\sigma}}{2}\]

nótese que este último término, junto con el magnetón de Bohr, se encuentra uno de los mayores éxitos de la ecuación de Dirac.

\begin{equation}
\Rightarrow \mu _{\beta} = \frac{e}{2m} (= \frac{e \hbar }{2m} \quad \text{de tarea})
\end{equation}

con $\overrightarrow{\mu } = 2 \mu _{B} \overrightarrow{\sigma }/2$ Dirac predice, entonces que el factor giromagnético es una constante cuyo valor es $2$. Nota: $[\mu_{B}]=$ corriente $\times$ área $= \frac{\text{Coulomb}}{\text{seg.}} \times m^2 $. Se pueden ver sus unidades como

\[\mu _{B} = \frac{e}{2m} \hbar ^n c^m \quad \text{pero} \quad [\frac{e}{2m} = \frac{\text{Coulomb}}{\text{Kg.}}\]

\[[N_{B}] = \frac{\text{Coul.}}{\text{Kg.}} (\text{Joules-seg})^n (\frac{m}{\text{seg.}})^m\]

\[\text{Kg} \frac{m^2}{\text{seg}^2} \Rightarrow \quad n = 1 \quad m = 0\]

\[H_{n.r.} \phi = [ \frac{\overrightarrow{P}^2 }{2m} - \frac{e}{2m} (\overrightarrow{P} \cdot \overrightarrow{A} + \overrightarrow{A}\cdot \overrightarrow{P}) - \frac{e}{2m} (\overrightarrow{\sigma} \cdot \overrightarrow{B})+e \varphi + \frac{e^2}{2m} \overrightarrow{A}^2 ] \phi \]

\[\text{aparece entonces} \quad T_a = - \frac{e}{2m} (\overrightarrow{P} \cdot \overrightarrow{A} + \overrightarrow{A}\cdot \overrightarrow{P}) \]

el efecto espín-órbita entre el núcleo y el electrón; y si $\overrightarrow{B} = \overrightarrow{B}_{N}$ y $\overrightarrow{A} = \overrightarrow{A}_{N}$ núcleo entonces si son efectos espín-órbita y espín-espín.

\[I_a = - \frac{e}{2m} (\overrightarrow{\sigma} \cdot \overrightarrow{B}) \quad \text{efecto espín-espín o dipolo (núcleo) - (electrón)}\]

Como $\nabla \cdot \overrightarrow{A} = 0$ la norma de Coulomb

\[I_{a} = - \frac{e}{m} (\overrightarrow{A} \cdot \overrightarrow{P})\]

\[\text{para el núcleo} \quad \overrightarrow{A} = \nabla \times (\frac{\overrightarrow{\mu } _{N}}{r}) = \frac{\overrightarrow{\mu }_N \times \overrightarrow{r}}{r^3 } (\frac{\mu _0 }{4 \pi })\]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Nota: Norma de Coulomb}

\[\overrightarrow{A} = \frac{\mu _0 }{4 \pi } \int \frac{\overrightarrow{J}(\overrightarrow{v}^{\prime})}{|\overrightarrow{r}-\overrightarrow{r}^{\prime}| d^3 x ^{\prime}} \quad \text{en la norma de Coulomb}\]

\[\text{poniendo} \quad |\overrightarrow{r}-\overrightarrow{r}^{\prime}| ^{-1} \simeq \frac{1}{r} (1+ \frac{\overrightarrow{r} \cdot \overrightarrow{r}^{\prime}}{r^2 } + \cdots ) y \overrightarrow{J} d^3 x^{\prime } = i d \overrightarrow{l}^{\prime}\]

\[\Rightarrow \overrightarrow{A}(\overrightarrow{r}) \sim \frac{\mu _{0} i }{4 \pi } \int d \overrightarrow{r}^{\prime } [\frac{1}{r} + \frac{\overrightarrow{r}\cdot \overrightarrow{r}^{\prime}}{r^3 } \]

\[\text{Usando} \quad (\overrightarrow{A} \times \overrightarrow{B}) \times C = (\overrightarrow{A} \cdot \overrightarrow{C}) \overrightarrow{B} - \overrightarrow{A} (\overrightarrow{B} \cdot \overrightarrow{C})\]

\[\Rightarrow (\overrightarrow{r}^{\prime } \times d \overrightarrow{r}^{\prime}) \times \overrightarrow{r} = (\overrightarrow{r} \cdot \overrightarrow{r}^{\prime }) d \overrightarrow{r}^{\prime } - \overrightarrow{r}^{\prime}(\overrightarrow{r} \cdot d \overrightarrow{r}^{\prime })\]

\[\therefore d [\overrightarrow{r}^{\prime }(\overrightarrow{r} \cdot \overrightarrow{r}^{\prime})] = d \overrightarrow{r} ^{\prime} (\overrightarrow{r} \cdot \overrightarrow{r}^{\prime}) + \overrightarrow{r}^{\prime } (\overrightarrow{r} \cdot d\overrightarrow{r}^{\prime })\]

a $\overrightarrow{r}$ fijo. Entonces, queda

\[d\overrightarrow{r}^{\prime } (\overrightarrow{r} \cdot \overrightarrow{r}^{\prime }) = d [\overrightarrow{r}^{\prime}(\overrightarrow{r}\cdot \overrightarrow{r}^{\prime})] + (\overrightarrow{r}^{\prime} \times d \overrightarrow{r}^{\prime})\times \overrightarrow{r} - d\overrightarrow{r}^{\prime} (\overrightarrow{r} \cdot \overrightarrow{r}^{\prime})\]

\[\Rightarrow d \overrightarrow{r}^{\prime } (\overrightarrow{r} \cdot \overrightarrow{r}^{\prime }) = \frac{1}{2} \lbrace d [\overrightarrow{r}^{\prime} (\overrightarrow{r} \cdot \overrightarrow{r}^{\prime})] + (\overrightarrow{r}^{\prime} \times d \overrightarrow{r}^{\prime})\times \overrightarrow{r}\rbrace \]

\begin{equation}
\overrightarrow{A}(\overrightarrow{r}) = \frac{\mu_0 i}{4 \pi} [\oint \frac{1}{2} (\overrightarrow{r}^{\prime} \times d\overrightarrow{r}^{\prime})] \times \frac{\overrightarrow{r}}{r^3 } = \frac{\mu_0 }{4 \pi} \frac{\overrightarrow{m}\times \overrightarrow{r}}{r^3 }
\end{equation}

entonces

\[I_a = - \frac{e}{m} (\frac{\mu _N \times \overrightarrow{r} }{r^3 }) \cdot \overrightarrow{P} (\frac{\mu _0}{4 \pi }) \]

\[= - \frac{e}{m} (\frac{\mu _0 }{4 \pi }) \frac{\overrightarrow{\mu _N }}{r^3 } (\overrightarrow{r} \times \overrightarrow{P}) \]

\begin{equation}
- \frac{e}{m} \frac{\overrightarrow{\mu}_{N}}{r^3 } \cdot \overrightarrow{L} (\frac{\mu_0 }{4 \pi })
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Campo Magn\'etico del N\'ucleo }

Si se considera un campo magn\'etico, tal que

\[\overrightarrow{B} = \nabla \times \overrightarrow{A} \quad , \quad \overrightarrow{A} = (\frac{\mu_0 }{4 \pi }) \frac{\overrightarrow{\mu _{N}} \times \overrightarrow{r}}{r^3 }\]

i) Caso $\overrightarrow{r} = 0$ con $'nabla (1/r) = - \overrightarrow{r} / r^3 $

\[\overrightarrow{B} = \nabla \times \overrightarrow{A} \quad \text{utilizando índices}\]

\[B^{i} = \epsilon _{ijk} \partial_{j} [\epsilon _{klm} \partial_l (\mu _{N}^{m}/r)] (\frac{\mu_0 }{4 \pi})\]

\[= (\delta _{il} \delta_{jm} - \delta_{im} \delta_{jl}) \partial_j [\partial_{l} \mu_{N}^{\mu}/r] (\frac{\mu_0 }{4 \pi })\]

\[= [ \partial _m \partial_i (\mu_{N}^{m}/r) - \nabla^2 (\mu_{N}^{i}/r)] (\frac{\mu _0 }{4 \pi})\]

\[\text{Poniendo } \quad \partial_m \partial_i \rightarrow \frac{1}{3} \nabla ^2 \delta _{mi} \quad \text{por la simetría de los estados}\]

\[= \frac{\mu_0 }{4 \pi} [\frac{1}{3} \nabla^2 (\frac{\mu_{N}^{i}}{r}) - \nabla^2 (\frac{\mu_{N}^{i}}{r})] = - 2/3 \mu_{n}^{i} \nabla^2 (\frac{1}{r}) (\frac{\mu_0 }{4 \pi})\]

\[\text{con} \quad \mu _{N}^{i} = \mu_{N}^{i} (\overrightarrow{r} = 0) = \text{cte} \quad \nabla^2 (1/r) = - 4 \pi \delta (\overrightarrow{r}) \]

\begin{equation}
\overrightarrow{B} = \frac{2}{3} \overrightarrow{\mu}_{N} \mu _0 \delta^3 (\overrightarrow{r}) 
\end{equation}

ii) Caso $\overrightarrow{r} \neq 0$ con $\nabla \times \overrightarrow{A} = \nabla \times (\frac{\overrightarrow{\mu} _{N} \times \overrightarrow{r}}{r^3 }) (\frac{\mu_0 }{4 \pi })$

\[\text{pero} \quad \overrightarrow{A} \times (\overrightarrow{B} \times \overrightarrow{C}) = (\overrightarrow{A} \cdot \overrightarrow{C})\overrightarrow{B} - \overrightarrow{A} \cdot (\overrightarrow{B} \cdot \overrightarrow{C})\]

\[\overrightarrow{B} = ( \frac{\mu_0 }{4 \pi}) [\nabla \cdot (\frac{\overrightarrow{r}}{r^3 }) - \nabla (\overrightarrow{\mu _{N}} \cdot \frac{\overrightarrow{r}}{r^3 })] - \nabla (\frac{1}{r}) \sim \delta (\overrightarrow{r}) = 0\]

como $\overrightarrow{r} \neq 0$

\[= -\frac{\mu_0 }{4 \pi} \nabla (\overrightarrow{\mu} _{N} \cdot \frac{\overrightarrow{r}}{r^3 })\]

\[\nabla _j [\frac{\mu _{N}^{i} x^ i}{(x^2 + y^2 + z^2 )^3/2 }] = \frac{\mu_{N}^{j}}{r^3 } - 3 \frac{\overrightarrow{\mu _{N}} \cdot \overrightarrow{R}}{r^5 } x^j \]

\[\Rightarrow \overrightarrow{B} = - (\frac{\mu _0 }{4 \pi }) [\frac{\overrightarrow{\mu} _{N}}{r^3 } - \frac{3 (\overrightarrow{\mu} _{N} \cdot \overrightarrow{r})}{r^5 } \overrightarrow{r}] \]

\begin{equation}
\therefore \overrightarrow{B} = \frac{2}{3} \mu _{0} \overrightarrow{\mu}_{N} \delta ^3 (\overrightarrow{r}) + \frac{\mu _0}{4 \pi } [3 \frac{(\overrightarrow{\mu} _{N}) \cdot \overrightarrow{r}}{r^5} \overrightarrow{r} - \frac{\overrightarrow{\mu} _{N}}{r^3 }
\end{equation}

cuyo término de contacto 

\begin{equation}
\overrightarrow{\mu} _{N} = g_p \frac{e}{2m_p } (\overrightarrow{\sigma _p}/2)
\end{equation}

\[\overrightarrow{B} _{N} (\overrightarrow{r}) = \frac{2}{3} \mu _0 g _p \frac{e}{2m_p } (\frac{\overrightarrow{\sigma}_{p}}{2}) \delta ^3 (\overrightarrow{r}) \]

a primer orden en teoría de perturbaciones no relativista

\[\bigtriangleup E_m = \langle n | V_{int} | n \rangle \]

\[\text{con} \quad V_{int} = - \overrightarrow{\mu}_e \cdot \overrightarrow{B} = - \frac{e}{2m} \overrightarrow{\sigma} \cdot \overrightarrow{B} \]

\[\bigtriangleup E_m (l = 0) \quad \text{para los demás terminos es cero}\]

\[\bigtriangleup E_{m} = - \frac{e}{2m} (2/3) \mu_{0} g_{p} \frac{e}{2m_{p} } \langle n | \delta^{3} (\overrightarrow{r}) \overrightarrow{\sigma } \cdot \frac{\sigma_{p}}{2} | n \rangle \]

\[= - \frac{e^2 g_p \mu_0 }{6 m_e m_p } \overrightarrow{\sigma } \cdot \frac{\overrightarrow{\sigma} _p }{2} |\psi _{l=0} (0)|^2 \]

\[\text{pero} \quad |\psi_n (0)|^2 = \frac{4 m_{e}^{3} \alpha^4 z^3 }{n^3 e^2 } \quad \text{para estados } \quad S\]

Cabe señalar que $(F(kv) \sim (kr)^l e ^{-kr})$ es constante en el caso relativista, y $F$ y $G$ van como $r^5 e^{-kr}$ constantes; y en el caso $r=0$ son cero.

\[\Rightarrow |E_{m}| = \frac{2}{3} \frac{g_p \mu_0 m_{e}^{2} }{m_p n^3 } \alpha^4 z^3 \overrightarrow{\sigma} \cdot \overrightarrow{\sigma}_p /2\]

\[\overrightarrow{\sum } = (\overrightarrow{ \sigma} _{p}/2)+ \overleftarrow{\sum}/2)\]

\[\text{con} \quad \sum ^2 = s (s+1)  = \begin{cases} 2\quad s=1 &\mbox{triplete } \\

0 \quad s=0 & \mbox{singulete } \end{cases} \]

\[(\overrightarrow{\sigma}_{p}/2)+ \overleftarrow{\sum}/2^{2} = 3/2 + \overrightarrow{\sigma } \cdot \overrightarrow{\sigma}_{p}/2 =\begin{cases} 2 &\mbox{triplete } \\

0  & \mbox{singulete } \end{cases} \]

\[\text{triplete:} \quad 3/2 + \overrightarrow{\sigma} \cdot \overrightarrow{ \sigma }_p /2 = 2 \]

\[\Rightarrow \overrightarrow{\sigma } \overrightarrow{\sigma }_p / 2 = 1/2\]

\[\text{singulete} \quad 3/2 + \overrightarrow{\sigma } \cdot \overrightarrow{\sigma}_p /2 = 0 \Rightarrow \quad \overrightarrow{\sigma} \cdot \overrightarrow{\sigma}_p /2 = - 3/2\]

\[\delta _m =  |\bigtriangleup E_{n \text{triplete}} - \bigtriangleup E_{n \text{singulete}}\]

\[=\frac{2}{3}\frac{g_{p}\mu_{0} m_{e}^{2}}{m_{p} n^{3}} \alpha ^4 z^3\]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Tarea-Examen}

De los polinomios generales de Lagerre o Polinomios de Sonide 

\begin{equation}
y(x,z) \equiv \frac{e^{xz}/(1-z)}{1-z} = \sum _{k = 0}^{\infty} L _n (x) z^n \quad \vert z \vert <1 
\end{equation}

\[\Rightarrow L_n (x) = \frac{1}{2 \pi i} \oint _c \frac{e^{-x z/(1-z)}}{(1-z)z^{n+1}} dz \]

\[\text{poniendo} \quad s-x = \frac{xz}{1-z} \quad \Rightarrow z = \frac{s-x}{s}\]

\[L_n (x) = \frac{e^x }{2 \pi i} \oint \frac{s^{m} e^{-S}}(s-x)^{n+1} ds \]

\[= e^x \frac{d^n x^n e^{-x}}{n! dx^n }\]

de los polinomios de Laguerre:

\[L_{n}^{k} (x) = (-)^k \frac{d^k }{dx^k } L_{n+k} (x )\]

probar entonces que 

\[L_{n}^{\alpha } (x) = \frac{1}{2 \pi i} \int _c \frac{e^{-xz/(1-z)}}{(1-z)^{\alpha + 1} z^{n+1}} dz\]

donde $c$ es una trayectoria no cerrada; haciendo $xz/(1-z) = s-x$ probar que

\[L_{n}^{\alpha } (x) = \frac{e^x x^{- \alpha}}{n!} = \frac{d^n (x^{n+ \alpha} e^{-x})}{dx^n }\]

Solución: 


\[\frac{e^{-xz/1-z}}{1-z} = \sum_{n = 0}^{\infty } L_n z^n \quad 	L_{n}^{k} = (-)^k L_{n+k} \]

\[\Rightarrow \frac{d^k }{dx^k } [\frac{e^{-xz/1-z}}{1-z}] = \sum _{n= 0}^{\infty} \frac{d^k L_n }{dx^k } z^n \]

\[= (-)^k \frac{e^{-xz/1-z}}{(1-z)^{k+1}} \Rightarrow \frac{e^{-xz/1-z}}{(1-z)^{k+1}}  = \sum _{n=0}^{\alpha } (-)^k z^{-k} (\frac{d^k L_n }{dx^n })z^n  \]

\[= \sum _{n=0} ^{\infty } (-)^k \frac{d^k }{dx^k } L_{n+k} z^n \]

\[= \sum _{n=0}^{\infty } L_{n}^{k} z^n \]

por continuación analítica

\[\Rightarrow L_{n}^{\alpha} = \frac{1}{2 \pi i} \oint _c \frac{e^{-xz/1-z}}{(1-z)^{\alpha +1} z^{n+1}} dz  \]

\[\text{poniendo} \quad xz/1-z  = s-x \quad ; \quad dz = \frac{x}{s^2 }  ds \quad 1-z = \frac{x}{s} \quad ; z = \frac{s-x}{s}\]

\[\Rightarrow L_{n}^{\alpha} (x) = \frac{1}{2 \pi i } \oint \frac{s^{\alpha +1} e^{-(s-x)}}{x^{\alpha + 1}(s-x)^{n+a} } \frac{xs^{n+a}}{s^2 } dx = \frac{e^x }{x^{\alpha}} \frac{1}{2 \pi i} \oint _{c^{\prime}} \frac{e^{-s} s^{\alpha +n}}{(s-x)^{n+a}} ds \]

\[= \frac{e^x x^{-\alpha}}{n!} \frac{d^n}{dx^n } (e^{-x} x^{x+ \alpha })\]

derivando respecto a $z$

\[(\frac{e^{-xz/1-z}}{(1-z)^{\alpha + 1}}) = \frac{e^{-xz/1-z}}{(1-z)^{\alpha + 1}} [- \frac{x}{1-x} - \frac{xz}{(1-z)^2 } + \frac{e^{-xz/1-z}}{(1-z)^{\alpha + 2}} (\alpha + 1)\]

\[= - \frac{e^{-xz/1-z}}{(1-z)^{\alpha + 1}} \frac{x}{(1-z)^2} + \frac{e^{-xz/1-z}}{(1-z)^{\alpha + 2}} (\alpha + 1)\]

\[\Rightarrow - \frac{x}{(1-z)^2} \sum_{n=0}^{\infty} z^n L_n + \frac{(1+ \alpha)}{(1-z)} \sum_{n=0}^{\infty} z^n L_n = \sum _{n=0}^{\infty} n z^{n-1}L_{n} \]

\[\sum_{n = 0} ^{\infty } z^n L_n [(1+ \alpha ) (1-z) - x] = \sum_{n = 0}^{\infty } n z ^{n-1} L_n (1 - z)^2 \]

\[\Rightarrow (1 + \alpha - x) L_n - (1 + \alpha )L_{n-1}  = (n + 1 )L_{n+1} - 2 n L_n + (n-1) L_{n-1} \]

\begin{equation}
(1 + \alpha + 2n - x) L_n = (n+1) L_{n+1} + (n+ \alpha ) L_{n-1 }
\end{equation}

al derivar respecto a $x$: 

\[(1+\alpha + 2n - x) L_{n}^{\prime } - L_n = (n+1) L_{n+1}^{\prime} + (n + \alpha ) L_{n-1}^{\prime }\]

\[\text{pero} \quad L_n = L_{n}^{\prime } -L_{n + 1}^{\prime }\]

\[\Rightarrow (1 + \alpha + 2n - x) L_{n}^{\prime } - L_n = (n+1 ) (L_{n}^{\prime } - L_n ) + (n + \alpha ) L_{n-1}^{\prime }\]

\[(\alpha + n - x) L_{n}^{\prime } = - n L_n + (n + \alpha ) L_{n-1}^{\prime}\]

\[\text{pero} \quad L_{n-1}^{\prime } = L_{n-1}	+ L_{n}^{\prime } \]

\[(\alpha + n - x) L_{n}^{\prime} = -n L_n + (n + \alpha ) (L_{n-1} + L_{n}^{\prime })\]

\[- x L_{n}^{\prime} = -n L_n + (n+ \alpha ) L_{n-1 }\]

\begin{equation}
x L_{n}^{\prime} = n L_n - (n + \alpha ) L_{n-1 }
\end{equation}

2) Al derivar con respecto a $x$

\[\frac{e^{-xz/1-z}}{(1-z)^{\alpha + 1}} = - \frac{z}{1-z} \frac{e^{-xz/1-z}}{(1-z)^{\alpha + 1}}) = \sum_{n=0}^{\infty} z^n L_{n}^{\prime} (x)\]

\[-z \sum _{n = 0}^{\infty } z^n L_n = (1-z) \sum _{n=0} ^{\infty } z^n L_{n}^{\prime } \]

\[\Rightarrow -L_{n-1} = L_{n}^{\prime} - L_{n-1}^{\prime }\]

\begin{equation}
\therefore L_{n-1} = L_{n-1}^{\prime } - L_{n}^{\prime }
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Identidad de Gordon}

\[\overrightarrow{u}(p^{\prime}) \gamma^{\mu} u (p) = \overrightarrow{u} (p^{\prime}) [\frac{p^{\prime \mu + p^{\mu }}}{2m} + \frac{\sigma ^{\mu \nu } + p^{\mu}}{2m} q_{\nu} ] u (p)\]

\[\sigma^{\mu \nu} = \frac{i}{2} [\gamma^{\mu} , \gamma^{\nu} ] = \frac{i}{2} (\gamma^{\mu} \gamma^{\nu} - \gamma^{\nu} \gamma^{\mu} )\]

\[\frac{i}{2} [\gamma^{\mu} \gamma^{\nu} - (- \gamma^{\mu} \gamma^{\nu} + 2 \eta ^{\mu \nu })] = i \gamma^{\mu} \gamma^{\nu} - i \eta^{\mu \nu}\]

\[\frac{\overrightarrow{u}(p^{\prime })}{2m} [p^{\prime \mu } + p^{\mu } + (\eta^{\mu \nu} - \gamma^{\mu }\gamma^{\nu }) (P_{\nu }^{\prime }- p_{\nu })] u (p) \]

\[\frac{\overrightarrow{u}(p^{\prime})}{2m} [p^{\prime \mu} + \slashed{p}^{\mu} + p^{\prime \mu} - \slashed{p}^{\mu}- \gamma^{\mu} (\slashed{p}^{\prime} - \slashed{p})] u (p)-\]

\[-\gamma^{\mu} \gamma^{\nu } p_{\nu}^{\prime} - (- \gamma^{\nu} \gamma^{\mu}+2 \eta ^{\mu \nu})p_{\nu}^{\prime}\]


\[= \frac{\overrightarrow{u}(p^{\prime })}{2m} [\slashed{p}^{\prime } \gamma^{\mu}+ \gamma^{ \mu} \slashed{p} ] u (p) \quad \text{pero } \quad \slashed{p}u(p) = m u (p) \quad \overrightarrow{u}(p^{\prime}) \slashed{p} = \overrightarrow{u} (p^{\prime})m \]

\[\overrightarrow{u}(p^{\prime}) \gamma^{\mu}u (p)\]


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Estructura fina y el factor giromagnético}


\begin{equation}
E_{nj}= m \lbrace 1 + \frac{z^2 e^4 }{(n- \varepsilon _{j})^2 }\rbrace ^{-1/2}
\end{equation}

\[\varepsilon _{j} = j + 1/2 - \sqrt{(j+1/2)^2 - z^2 e^4} \]

\[\text{con} n = j + 1/2 + k^0 \]

\[j = 1/2 , 3/2, \cdots -1/2 \]

\[k^0 = 0,1,2, \cdots \]

\[\text{para} \quad n = 1 ; \quad \Rightarrow j =1/2 \quad ; \quad k^0 = 0 \]

de donde se obtienen 

\[l= j+1/2 \pi \quad \text{si} \pi = + 1 \quad l = 1/2 + 1/2 = 1 \text{! no es posible} \]

\[l^{\prime} = j -1/2 \pi \quad \text{porque} \quad  -(\text{máx} \quad j)<l<(\text{máx}j) \]

\[\text{si} \quad \pi= -1 \quad l =1/2 - 1/2 = 0\]

\[l^{\prime} = 1/2 + 1/2 = 1\]

Notación: $nlj$ con $\bot S_{1/1} \quad (\pi = -1)$

\[\text{para } \quad n = 2 \quad , \quad j = 1/2,3/2 \quad , \quad k^0 = 1,0 \quad l = 0,1,2,\cdots \equiv s,p,d,\cdots\]

\[l = j +1/2 \pi  = \begin{cases} 3/2+1/2 = 2 \quad ! & (\pi = +1, \quad j = 3/2) \\

3/2-/2 = 1 \quad  & (\pi = -1 , \quad j = 3/2)  \end{cases}\]

para $2 P_{3/2}(\pi= -1)$

\[l = 1/2 +1/2 \pi  = \begin{cases} 1/2+1/2 = 1  & \pi = + 1 \quad  2 P_{1/2}, \quad (\pi= +1) \\

1/2-1/2 = 0  & \pi = - 1 \quad  2 S_{1/2}, \quad (\pi= -1)  \end{cases}\]

dado que $l = 0,1,2,\cdots \equiv s,p,d,\cdots$

\[\text{para} n = s \quad , \quad j = 1/2,3/2,5/2 \quad k^0 =2,1,0 \quad \text{respectivamente }\]

\[l = 5/2 +1/2 \pi  = \begin{cases} 5/2+1/2 = 3 \quad \text{!}  & (\pi = + 1) \quad  s \\
5/2-1/2 = 2  & \pi = - 1 \quad  3 e_{5/2}  \end{cases} \]

\[l = 3/2 +1/2 \pi  = \begin{cases} 3/2+1/2 = 2  & (\pi = + 1) \quad  3d_{3/2} \\

3/2-1/2 = 1  & (\pi = - 1) \quad  3 S_{3/2}  \end{cases}\]

\[l = 1/2 +1/2 \pi  = \begin{cases} 1/2+1/2 = 1  & (\pi = + 1) \quad  3p_{1/2} \\

1/2-1/2 = 0  & (\pi = - 1) \quad  3 S_{1/2}  \end{cases}\]

\begin{table}[htbp]
\caption{}
\begin{tabular}{|l|l|l|l|}
\hline
$n$ & $j$ & $l=0,1,2$ & $k^{0}$ \\ \hline
$1$ & $1/2$ & $1S_{1/2}(-)$ & $0$ \\ \hline
$2$ & $1/2$ & $2S_{1/2}(-)2P_{1/2}(+)$ & $1$ \\ \hline
 & $3/2$ & $2P_{3/2}(-)$ & $0$ \\ \hline
$3$ & $1/2$ & $3S_{1/2}(-)3P_{1/2}(+)$ & $2$ \\ \hline
 & $3/2$ & $3p_{3/2}(-)3d_{3/2}(+)$ & $1$ \\ \hline
 & $5/2$ & $3d_{5/2}(-)$ & $0$ \\ \hline
\end{tabular}
\label{}
\end{table}


\[s = + \sqrt{\tau^2 - j^2}\]

\[|\tau| > \varsigma \quad \Rightarrow (j+1/2)>ze^2\]

\[\hbar (j+1/2)>\frac{ze^2}{4\pi \epsilon_0 C} \quad \Rightarrow (j+1/2)>\frac{ze^2}{4 \pi \epsilon_0 c \hbar} = \alpha z\]

\begin{equation}
E_{nj} = m \lbrace 1 + \frac{(z \alpha)^2 }{(n . (j+1/2) + \sqrt{(j+1/2)^2 - (z \alpha)^2})} \rbrace ^{-1/2}
\end{equation}

\section{L\'imite no relativista}

\[(E-e \varphi)\psi = [ \overrightarrow{\alpha} \cdot (\overrightarrow{p}-e \overrightarrow{A} ) + \beta m_0 ] \psi \quad \psi = \left ( \begin{array}{cc}
 \phi  \\
 \chi \\ \end{array} \right) \]

\[\overrightarrow{\alpha} = \left ( \begin{array}{cc}
 0 & \overrightarrow{\sigma}  \\
 \overrightarrow{\sigma} & 0 \\ \end{array} \right) \quad \beta =  \left ( \begin{array}{cc}
 1 & 0  \\
 0 & -1 \\ \end{array} \right)\]

\[E \phi = \overrightarrow{ \sigma} \cdot (\overrightarrow{P}-e\overrightarrow{A} \phi + m_0 \phi + e \varphi \phi)\]

\[E \chi = \overrightarrow{\sigma} \cdot (\overrightarrow{p}-e\overrightarrow{A}) \phi + (e \varphi - m_0 ) \chi \]

si se considera que el primer término de lado derecho de la igualdad $\pi \equiv (\overrightarrow{p}-e\overrightarrow{A})$ donde además, $\pi^0 = E-e \varphi$

\[T = e-m_0 \quad , \quad M \equiv \frac{1}{2} (m_0 + pi^0 )\]

\[= \frac{1}{2} (m_0 + E- e \varphi )\]

\[= m_0 + \frac{1}{2} (T-e \varphi)\]

\[E-e \varphi + m_0 = 2M\]

\[2M \chi = \overrightarrow{\sigma} \cdot \overrightarrow{\pi} \chi \]

\[(T-e \varphi ) \phi = \overrightarrow{\sigma}\ \cdot \overrightarrow{\pi} \chi \]

\[T \phi = \overrightarrow{\sigma} \overrightarrow{\pi} \chi + e \varphi \phi = [\frac{(\overrightarrow{\sigma} \cdot \overrightarrow{\pi})}{2M} (\overrightarrow{\sigma} \cdot \overrightarrow{\pi}) + e \varphi ] \phi \]

donde el límite no relativista $e \varphi << m_0 $ , $E \simeq m_0 $ , $\pi^0 \approx m_0 $ con $(c = 1)$

\[M = \frac{1}{2} (m_0 + \pi^0 ) \simeq m_0 \]

\[T \phi \cong [\frac{(\overrightarrow{\sigma} \cdot \overrightarrow{\pi})(\overrightarrow{\sigma} \cdot \overrightarrow{\pi})}{2 m_0 } + e \varphi ] \phi \]

\[\chi \cong \frac{2}{2 m_0 } \overrightarrow{ \sigma} \cdot \overrightarrow{\pi} \phi \quad |\overrightarrow{\pi}|<<m_0 \]

considerando el caso $\chi << \phi $

\[(\overrightarrow{\sigma} \cdot \overrightarrow{\pi}) \cdot (\overrightarrow{\sigma} \cdot \overrightarrow{\pi}) = \overrightarrow{\pi} \cdot \overrightarrow{\pi} + i \overrightarrow{\sigma} \cdot \overrightarrow{\pi} \times \overrightarrow{\pi }\]

\[(\overrightarrow{\pi} \times \overrightarrow{\pi}) = (\overrightarrow{p} -e \overrightarrow{A}) \times (\overrightarrow{p} -e \overrightarrow{A}) \]

con $\overrightarrow{p} = + \frac{1}{i} \overrightarrow{\nabla }$

\[\Rightarrow(p) \times \overrightarrow{A} + \frac{1}{i} \epsilon_{ijk} [(\partial_j A^k )+A^k \partial _j ] \]

\[= - i (\nabla \times \overrightarrow{A})_i - (\overrightarrow{A} \times \overrightarrow{p})_i \]

\[-e (\overrightarrow{p} \times \overrightarrow{A}) = ie\overrightarrow{B} + e (\overrightarrow{A} \times \overrightarrow{P})\]

\[\Rightarrow (\overrightarrow{\pi} \times \overrightarrow{\pi}) = ie \overrightarrow{B}\]

\[T \phi \simeq [\frac{\pi^2 - e \overrightarrow{\sigma} \cdot \overrightarrow{B}}{2 m_0 } + e \varphi] \phi \]

\[H _{n.r.} \phi = [\frac{(\overrightarrow{p}-e\overrightarrow{A})^2-e \overrightarrow{\sigma} \cdot \overrightarrow{B}}{2m_0 }\ + e \varphi] \phi \] 

en partícular, aparece el término $- e/2m_0 \overrightarrow{\sigma} \cdot \overrightarrow{B}$

\[\textbf{U} \equiv - \overrightarrow{\mu} \cdot \overrightarrow{B} \quad \Rightarrow \equiv mu_{B} \overrightarrow{\sigma}/2 \equiv \mu_{B} \overrightarrow{S}\]

\[\Rightarrow \mu_{B} = e/m_0 \]

\[\mu_{B} = g_e e/2m_0 \quad (= \frac{e g_e }{2m_0 })\]

de donde se obtiene el factor giromagnético, una constante cuyo valor es $g_e = 2$

\[\overrightarrow{\rho} _0 \equiv \nabla \times \overrightarrow{m} \quad ; quad \overrightarrow{m}(\overrightarrow{r})= \frac{g_p e}{2m_p } \rho (\overrightarrow{r}) \overrightarrow{I}\]

utilizando las unidades en S.I. $g_p e /2 (4 \pi \epsilon_0 ) \rho (\overrightarrow{r})$

\[\overrightarrow{M} \equiv \frac{g_p e}{2m_p } \int d^3 r^{\prime} \rho (r^{\prime}) \overrightarrow{I} \equiv \mu \overrightarrow{I} \quad \mu_0 = 1\]

\[\overrightarrow{A} (\overrightarrow{r}) = \frac{1}{4 \pi} \int \frac{\overrightarrow{j}_m (\overrightarrow{r}^{\prime})}{\vert \overrightarrow{r}-\overrightarrow{r}^{\prime}\vert } d^3 r^{\prime}\]

utilizando la identidad $\oint \overrightarrow{F} \cdot d\overrightarrow{l} = \int_s (\nabla \times \overrightarrow{F})\cdot d\overrightarrow{s}$

\[= \frac{g_p e}{2m_p } \int d^3 r^{\prime } \frac{\nabla^{\prime} \rho (\overrightarrow{r}^{\prime})I}{4 \pi \vert \overrightarrow{r}-\overrightarrow{r}^{\prime}\vert}\]

\[= \overrightarrow{I} \rho (r^{\prime}) \times \nabla^{\prime} \frac{1}{\vert \overrightarrow{r} - \overrightarrow{r}^{\prime}\vert} + \frac{1}{\vert \overrightarrow{r} - \overrightarrow{r}^{\prime}\vert} \nabla ^{\prime} \times \overrightarrow{I} \rho (r^{\prime})\]

\[\Rightarrow \frac{1}{\vert \overrightarrow{r} - \overrightarrow{r}^{\prime}\vert} \nabla^{\prime} \times \overrightarrow{I} \rho (r^{\prime}) = \nabla ^{\prime} \times (\frac{\overrightarrow{I}\rho (r^{\prime})}{\vert \overrightarrow{r} - \overrightarrow{r}^{\prime}\vert})- \overrightarrow{I} \rho (r^{\prime}) \times \nabla^{\prime} \frac{1}{\vert \overrightarrow{r} - \overrightarrow{r}^{\prime}\vert} \]

\[\overrightarrow{A} (\overrightarrow{r}) = \frac{g_p e}{2m_p } \int d^3 r^{\prime } \lbrace \nabla^{\prime} \times (\frac{\overrightarrow{I}\rho (r^{\prime})}{4 \pi \vert \overrightarrow{r} - \overrightarrow{r}^{\prime}\vert}) - \overrightarrow{I} \rho (r^{\prime}) \times \nabla^{\prime } \frac{1}{4 \pi \vert \overrightarrow{r} - \overrightarrow{r}^{\prime}\vert}\rbrace \]

\begin{equation}
\overrightarrow{A} (\overrightarrow{r}) =- \frac{g_p e}{2m_p } \int d^3 r^{\prime}\rho(r^{\prime}) \overrightarrow{I} \times \nabla ^{\prime} \frac{1}{4 \pi \vert \overrightarrow{r}-\overrightarrow{r}^{\prime}\vert} 
\end{equation}

\[\overrightarrow{B}(\overrightarrow{r}) = - \frac{g_p e}{2m_p }\int d^3 r^{\prime} \rho (r^{\prime})\nabla \times I \times \nabla^{\prime } \frac{1}{4 \pi \vert\overrightarrow{r}-\overrightarrow{r}^{\prime}}\vert \]

\[\nabla ^{\prime} \frac{1}{\vert\overrightarrow{r}-\overrightarrow{r}^{\prime}} = \nabla^{\prime} [(x-x^{\prime})^2 + \cdots)^{-1/2}] = + \frac{1}{\vert\overrightarrow{r}-\overrightarrow{r}^{\prime}\vert ^3} (\overrightarrow{r}-\overrightarrow{r} ^{\prime}) \]

\[\nabla \frac{1}{\vert\overrightarrow{r}-\overrightarrow{r}^{\prime}} = \nabla^{\prime} [(x-x^{\prime})^2 + \cdots)^{-1/2}] = - \frac{1}{\vert\overrightarrow{r}-\overrightarrow{r}^{\prime}\vert ^3} (\overrightarrow{r}-\overrightarrow{r} ^{\prime}) \]

\[\nabla ^{\prime} \frac{1}{\vert \overrightarrow{r}-\overrightarrow{r}^{\prime}\vert }=- \nabla \frac{1}{\vert \overrightarrow{r}-\overrightarrow{r}\vert} \]

\[\lbrace \nabla \times (I \times \nabla)\rbrace_i = \epsilon_{ijk} \partial_j (\epsilon_{klm} I_l\partial_m ) \]

considerando $\epsilon_{ijk}\epsilon_{klm}= \delta_{il}\delta_{jm}-\delta_{im}\delta_{jl}$

\[= I_i \partial_{j}^{2} - I_j \partial_j \partial_i \]

\[\nabla \times (I \times \nabla ) = I \nabla ^2 - I \cdot \nabla (\nabla) \]

promediando

\[\partial_j \partial_i \Rightarrow \frac{1}{3} \nabla^2 \delta_{ij} \]

\[\nabla \times (I \times \nabla ) \Rightarrow \overrightarrow{I} (\nabla^2 - 1/3 \nabla^2) = \overrightarrow{I} 2/3 \nabla^2 \]

\[\overrightarrow{B}(\overrightarrow{r}) = \frac{g_p e}{2m_p } (2/3) \int d^3 r^{\prime} \rho (r^{\prime} \overrightarrow{I} \nabla^2 \frac{1}{4 \pi \vert \overrightarrow{r} - \overrightarrow{r}^{\prime} \vert})\]

\[\frac{g_p e}{2m_p } (2/3) \overrightarrow{I} \rho (\overrightarrow{r})\]

\[\bigtriangleup E_n = \langle n \vert V_{int} \vert n \rangle\]

\[V_{int} = \frac{e}{2m} \overrightarrow{\sigma} \cdot \overrightarrow{B}\]

\[\bigtriangleup E_n = \frac{g_p e}{2m_p } 2/3 \frac{e}{2m} \overrightarrow{\sigma} \cdot \overrightarrow{I} \int \psi^{*}_{n} \rho \psi _n dr \]

\[\simeq \frac{g_p e^2 }{6 m_p m} \overrightarrow{\sigma} \cdot \overrightarrow{I} \vert \psi_n (0) \vert ^2 \]

\[\text{aparentemente} \quad \vert \psi_n (0) \vert ^2 = \frac{4 m^3 \alpha ^3 z^3}{n^3 e^2 }\]

Nótese que $[\vert \psi_n \vert ^2 = \frac{4}{n^3} (m c \alpha/\hbar)^3 $

\[\bigtriangleup E_n \simeq \frac{g_p e^2}{6 m_p m} \frac{4 m^3 \alpha^4 z^3 }{n^3 e^2 } \overrightarrow{\sigma} \cdot \overrightarrow{I} \]

\[\frac{2}{3} \frac{g_p m^2 \alpha ^4 z^3 }{m_p n^3} \overrightarrow{\sigma} \cdot \overrightarrow{I}\]

\[\sum _s \equiv (\overrightarrow{\sigma}/2 + \overrightarrow{I})\]

\[\sum _s ^2 = \sigma _s (\sigma _{s +1}) = \begin{cases}
                        2 \quad \sigma_s = 1 \quad  \text{triplete} \quad \sum _{s_{z}} = 1,0,-1 \\
                        0 \quad \sigma_s = 0 \quad  \text{singulete} \quad \sum _{s_{z}} = 0 
                    \end{cases}\]

\[(\overrightarrow{\sigma}/2+I)^2 = 3/4 + 3/4 + \overrightarrow{\sigma} \cdot \overrightarrow{I} = 3/2 + \overrightarrow{\sigma} \cdot \overrightarrow{I}\]

\[ = \begin{cases}
                        2 \quad   \text{triplete}  \\
                        0 \quad  \text{singulete}  
                    \end{cases}\]

triplete:

\[3/2 + \overrightarrow{\sigma} \cdot \overrightarrow{I} = 2 \quad \Rightarrow \overrightarrow{\sigma} \cdot \overrightarrow{I} = 1/2 \]

singulete:

\[3/2 + \overrightarrow{\sigma} \cdot \overrightarrow{I} = 0 \quad \overrightarrow{\sigma} \cdot \overrightarrow{I} = -3/2\]

\[\delta _n = \frac{4}{3} \frac{g_p m^2 \alpha ^4 z^3 }{m_p n^3 }\]

\[= \frac{1}{2} m \alpha^2 [\frac{8}{3} g_p \frac{z^3 \alpha ^2}{n^3} (\frac{m}{m_p })] \]

\subsection{Nota: Para el caso de un potencial electrost\'atico }

\begin{equation}
V(r) \equiv e \varphi (r) 
\end{equation}

\[\text{con} \quad e \overrightarrow{E} = - \nabla V = - \frac{\vec{r}}{r} \frac{dV}{dr }\]

\[e \nabla \cdot \overrightarrow{E} = - \nabla V \]

se tiene a segundo orden en $r^2/c^2 $ que

\begin{equation}
H^{\prime}_{n.r.} = H_{n.r.} - \frac{\overrightarrow{P}^4}{8 m_{0}^{3}} + \frac{1}{4m_{0}^2} \frac{1}{r} \frac{dV}{dr} (\overrightarrow{\sigma} \cdot \overrightarrow{L}) + \frac{1}{2m_{0}^2} \nabla^2 V
\end{equation}

\[\text{con} \quad H_{n.r. } = \frac{\overrightarrow{P}^2}{2m_0 } + V(r) \]

 el t\'ermino siguiente al $\overrightarrow{P}^2 / 2m_0$ de la energ\'ia cin\'etica

\[\text{el t\'ermino } \sim \frac{dV}{rdr} (\overrightarrow{\sigma} \cdot \overrightarrow{L}) = \quad \text{acoplamiento espín-órbita}\]

\[\text{el t\'ermino} \quad \nabla^2 V \quad \text{es el término Darwin, en el caso} \quad V(r) = - \frac{z e^2}{r} \]

\[\text{este término es igual a } \quad \frac{\pi z e^2 }{2 m_{0}^{2}} \delta (\overrightarrow{r}) \quad \text{y no afecta más que a los estados} \quad  S \]

Volviendo al caso de un potencial, tal que

\[\langle \delta V \rangle = \langle V (\overrightarrow{x} + \delta \overrightarrow{x}) -V(\overrightarrow{x}) \rangle \]

\[ = \langle \delta x_i \frac{\partial V }{\partial x_i } \rangle + \langle \frac{1}{2} \sum_{ij} \delta x_i \delta x_j \frac{\partial ^2 V}{\partial x_i \partial x_j } \rangle \]

con $\langle \delta x_i \delta x_j \rangle = \delta _{ij} \delta x^2 1/2$

\[= \frac{1}{6} \langle (\delta x)^2 \rangle \nabla ^2 V \]

\[\Rightarrow \bigtriangleup E_n (\text{Larm} ) = \frac{1}{6} \int \psi^{*}_{m}(x) \langle (\delta x)^2 \rangle \nabla^2 V \psi_{n} (x) d^3 x\]

\[= \frac{1}{6} \langle (\delta x)^2 \rangle \int \psi^{*}_{m} \nabla^2 V \psi_{n} (x) d^3 x\]

\[V = \frac{Z \alpha}{\vert \overrightarrow{r} \vert } \quad \Rightarrow \nabla^2 V = z \alpha 4 \pi \delta (\overrightarrow{x})\]

\[= \frac{2z \alpha \pi}{3} \langle (\delta \overrightarrow{x})^2 \rangle \vert \psi _{m}^{(0) \prime} \vert ^2 \quad \text{ondas} S \]

de la segunda ley de Newton:

\[\delta \ddot{x} = \frac{e}{m} E\]

\[\text{radio de Bohr :} \quad (z \alpha m)^{-1} \quad \Rightarrow \omega_{min} \sim z \alpha m\]

y la longitud de onda de compton $\lambda \sim \frac{1}{m} $ además con $\omega _{max} \sim m$

\[\int_{z \alpha m} ^{m} \frac{d \omega}{\omega} \sim (\frac{1}{z \alpha })\]

\[\langle (\delta \overrightarrow{x})^2 \rangle _{\epsilon = 0} \frac{e^2}{2m^2 \pi^2} em (\frac{1}{z \alpha }) = \frac{2 \alpha}{\pi} em (\frac{1}{z  \alpha})(\frac{1}{m})^2 \]

\[\frac{e^2 }{4 \pi } = \alpha \]

\[\bigtriangleup E_m = \frac{2 z \alpha \pi }{3 } \langle (\delta \overrightarrow{x})^2 \rangle \vert \psi _{n}(0) \vert ^2 \]

\[= \frac{2 z \alpha \pi }{3} (\frac{2 \alpha}{\pi} em (\frac{1}{z \alpha})\frac{1}{m^2 }) \vert \psi_{n}(0) \vert ^2 \]

\[\frac{4z \alpha ^2}{3} (\frac{1}{m})^2 em (\frac{1}{z \alpha}) \vert \psi _m (0) \vert ^2 \quad \text{orden} S \quad \vert \psi _n (0) \vert ^2 = \frac{4m^3 \alpha ^4 z^3}{n^3 e^2 } \quad \text{en estados } S \]

\begin{equation}
[\frac{8}{3 \pi} \frac{z^4 \alpha ^3}{n^3 } (em \frac{1}{z \alpha})] (\frac{1}{2} \alpha ^2 m)\delta_{l0}
\end{equation}

que es equivalente a $1Khz$ para $n=2$ , $z=1$, $l = 0$ y para estados $(2 S_{1/2})$

\[\delta \vec{x} \omega \equiv \int _{- \infty}^{\infty } e^{-i\omega t }\]

\[\delta \dot{\vec{x}} \equiv \int_{-\infty }^{\infty } e^{-i \omega t} \delta \dot{\vec{x}} dt = - \int _{-\infty }^{\infty } e^{-i \omega t } \delta x (- i \omega ) dt \]

\[= i \omega \int _{-\infty}^{\infty} e^{i \omega t} \delta x d t \]

\[\delta \ddot{\vec{x}} = - \omega \int_{-\infty}^{\infty} \cdots = - \omega ^{2} \delta \overrightarrow{x}_{\omega}\]

\[\delta_{\omega} = -e/ \omega ^4 m^2 \langle E^{2}\]

\[\delta \vec{x}_{\omega} = - \frac{e}{\omega ^2 m} E_{\omega }\]

\begin{equation}
\langle ( \delta \vec{x}_{\omega} \rangle =\frac{e^2 }{\omega ^{4}m^{2}} \langle E^{2}_{\omega} \rangle
\end{equation}

\[\Rightarrow \langle (\delta \vec{x})^2 \rangle \vert _{t=0} \equiv \frac{1}{2 \pi } \int _{0}^{\infty } \frac{e^2 }{m^ 2 \omega ^ 4} \langle E_{\omega }^{2} d \omega \]

pero

\[\frac{1}{2} \int d^{3} x (\theta ^{2} + B^{2}) = \text{energía total} = \sum_{\lambda =1}^{2} \sum _{k} \frac{1}{2} \omega \quad (\epsilon _{0} = 1 = \mu _0 )\]

\[\text{y dado que} \quad L^3 \int d^3 x \quad \sum _{k} \rightarrow \frac{L^3}{(2 \pi)^3 } \int d^3 k\]

\[ \int d^3 x E^2 = \int d^\rightarrow3 x B^2 \]

\[= \int d^3 x E ^2 \quad \langle E^2 \rangle = \frac{1}{L^3} \int E^2 d^3 x\]

\[\langle \vec{E} ^2 \rangle = \frac{1}{L^3 } \sum _{\lambda = 1}^{2} \sum _{k} \frac{1}{2} \omega = \frac{2}{(2\pi )^3 } \int d^3 k \frac{1}{2} \omega \]

nótese que $\overbrace{k} ^2 = k^{02} - \vec{k}^2 = 0$, esto es $k^{02} = \vec{k}^2$ y además $\omega ^2 = \vec{k} ^2 = k^{02}$ con $\vert \vec{k} \vert \equiv k$ y $\omega v2 = k^2$

\[d^3 k = k^2 d \Omega dk = \omega ^2 d \Omega d \omega\]

\[= \frac{2}{(2 \pi)^3 } \int \frac{\omega ^3}{2} d \Omega d \omega = \frac{4 \pi}{(2 \pi)^3 } \int \omega ^3 d \omega \]

\[- \frac{1}{2 \pi ^2 } \int \omega ^3 d \omega = \langle E ^2 \rangle = \text{teorema ergódico} = \int d \omega \langle E^{2}_{\omega} \rangle \]

\begin{equation}
\Rightarrow \langle E^2 _{\omega} \rangle = \frac{\omega ^3 }{2 \pi ^2}
\end{equation}2 

\[ \Rightarrow ( \delta \vec{x}) ^2 \rangle _{t=0} \equiv \frac{1}{2 \pi } \int _{0}^{\infty } \frac{e^2 }{m^2 \omega ^4 } \frac{\omega}{2 \pi ^2 } d \omega = \frac{e^2 }{m^2 4 \pi ^3 } \int _{0}^{\infty} \frac{d \omega}{\omega} \]

\[ \langle ( \delta \overrightarrow{x})^2 _{t=0} = \frac{e^2}{4 \pi ^3 m^2 } ln (\frac{1}{z \alpha }) \]


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Geometría Diferencial}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Espacios Topol\'ogicos}


\textbf{Definici\'on 8.1} Sea $X$ un conjunto y $Y = \lbrace \displaystyle \bigcup_{i | i \in Y} \rbrace$ denota una colecci\'on de dimensi\'on finita o infinita, de subconjuntos de $X$. Entonces el par $(X, Y)$ forma un \textit{espacio topol\'ogico} que satisface:\\

%bibitem: Nakahara, Nash
%revisar más bibliografía
%Nash dice lo mismo, sólo cambiar notación del Nakahara, en vez de T es Y

(i) $\emptyset , X \in T$\\

(ii)Si $Z$ es una subcolecci\'on de $X$, la familia de subconjuntos $\lbrace \displaystyle \bigcup_{i} |i \in I \rbrace$ satisface: $\displaystyle \bigcup_{i \in Y} \displaystyle \bigcup_{i} \in Y $\\

(iii)Si $Z_{\alpha}$ es un subconjunto finito de $X_{\alpha}$, se satisface que $\displaystyle \bigcap _{\alpha \in \mathbb{N}} U_{\alpha} \in Y$\\

Se dice que el conjunto $X$ es un \textit{espacio topol\'ogico}, mientras que el conjunto $\displaystyle \bigcup_{i}$ es llamado el conjunto de abiertos, de tal manera que $Y$ da una topolog\'ia a $X$.

%Lo siguiente lo voy a poner en vez de las subsecciones escritas, estas pasan a comentarios hasta decidir cuál permanece.
%bibitem: J.M. Lee, Introduction to Smooth Manifolds, Graduate Texts in Mathematics 218, DOI 10.1007/978-1-4419-9982-5, © Springer Science+Business Media New York 2013

El par $(X,Y)$, consistente en la topolog\'ia de $Y$ en $X$, puede construirse a partir de definiciones asociadas. Pueden sumarizarse las propiedades m\'as elementales y fundamentales para los propositos de este trabajo.\\
\\

\textbf{Definici\'on 8.2}: Sea $X$ un espacio topol\'ogico tal que, $p \in X$ y $Y \subseteq X$, entonces se cumple:

\begin{itemize}

\item \textbf{Una vecindad de $p$} es un subconjunto abierto que contiene a $p$. Similarmente, \textbf{una vecindad del conjunto $Y$} es un subconjunto abierto que contiene a $Y$.\\

\item Se dice que $Y$ es un conjunto cerrado, si el complemento de $X$, $X \setminus Y$, de esta manera se denota la diferencia entre conjuntos $\lbrace x \in X : x \not\in Y\rbrace$. En otras notaciones $Y^{c}$.\\

\item El \textbf{interior del conjunto $Y$}, denotado como Int$Y$, es la uni\'on de todos los subconjuntos abiertos de $X$ contenidos en $Y$.\\

\item El \textbf{exterior del conjunto $Y$}, denotado por Ext$Y$ , es la uni\'on de todos los subconjuntos abiertos de $X$ contenidos en $X \setminus Y$.\\

\item La \textbf{frontera de $Y$}, denotada por $\partial Y$, es el conjunto de todos los puntos de $X$ que no pertenecen a Int$Y$ o a Ext$Y$, es decir $\lbrace x \in X : x \not\in Int Y |x \not\in Ext Y\rbrace$.\\

\item Un punto $p \in X$, aunque no necesariamente en $Y$, se dice que es \textbf{punto lìmite de $Y$} si cada vecindad de $p$ contiene al menos un punto de $Y$ distinto de $p$.\\

\item Un punto $p \in Y$ se dice que es \textbf{un punto aislado de $Y$} si $p$ tiene una vecindad $U \subseteq X$ tal que $U \cap Y = \lbrace p \rbrace$.\\

\item Se dice que $Y$ \textbf{es denso en $X$} si y y s\'olo si, la clausura topol\'ogica del conjunto es todo el espacio, denotada por $\overline{Y} = X$.\\

\item Se dice que $S$ \textbf{es no denso en ninguna parte} en $X$ o \textbf{diseminado en $X$}  si $\overline{Y}$ no contiene un conjunto abierto no vac\'io. En otras palabras, si el Int$\overline{Y} = \emptyset$.\\

\end{itemize}

\textbf{Ejemplo:} el ejemplo más simple puede ser

\[d f = \frac{\partial f}{\partial x ^{\mu }} dx^{\mu } \quad \text{aquí } \quad dx^{\mu } \quad \text{es labase }\]

\[\text{definido como } \quad (df, V ) \equiv V [f] = \frac{\partial f}{\partial x^{\mu}} V^{\mu} \]

\[\text{para un vector} \quad V^{\mu} \frac{\partial}{\partial x^{\mu}} \quad \text{en la base coordenada.}\]

\[\Rightarrow (\frac{\partial f }{\partial x^{\mu}} dx^{\mu} , V^{\mu } \frac{\partial }{\partial x^{\nu}}) = \frac{ \partial f }{\partial x^{\mu} } V^{\nu} (dx^{\mu} , \frac{\partial }{\partial x^{\nu}})\]

entonces $dx^{\mu}$ es la base dual:

\[(dx^{\mu} , \frac{\partial }{\partial x^0}) = \delta _{\nu} ^{\mu }\]

en general una uno-forma se pondrá

\begin{equation}
\omega \equiv \omega _{\mu} dx^{\mu}
\end{equation}




%Bibitem: Nakahara
%Bibitem (2) Functional analysis, Rudin
\subsection{Mapeos conformes o continuos}

\textit{``La relación entre los elementos del espacio con respecto a su imagen inversa.''}

Sea $X$ y $Y$ conjuntos. Se define un \textit{mapeo} donde la función $f$ es una regla, la cual asigna $y \in Y$ para cada $x \in X$. Escribiendo

\begin{equation}
f : X \rightarrow Y
\end{equation}

donde $f$ está definida explícitamente como $f : \mapsto f(x)$.\\

Pueden existir más de dos elementos en el conjunto $X$ de tal forma que, correspondan al mismo $y \in Y$. Un subconjunto de $X$ cuyos elementos son mapeados a $y \in Y$ bajo $f$ se llama la \textit{imágen inversa} de $y$, denotada $f^{-1}(y)$, tal  que

\begin{equation}
f^{-1}(y) = \lbrace x \in C| f(x) = y \rbrace
\end{equation}

El conjunto $X$ es el dominio, mientras que $Y$ es el rango del mapeo. Se define a la imagen del mapeo como $f(x) = \lbrace y \in Y | y = f(x)$ para alguna $x \in X \rbrace$ o bien, $Im_{f}$.\\

Un mapa satisface las siguientes propiedades:\\

i) El mapeo $f: X \rightarrow Y$ es \textit{inyectivo} si $x \neq x$ implica $f(x) \neq f (x)$ para alguna $x \in X$\\

ii) El mapeo se dice que es \textit{suprayectivo} si, para algún $y \in Y \exists x \in X$ tal que $f(x) = y$ \\

iii) Se dice que el mapeo es\textit{ biyectivo} si cumple con $i)$ y $ii)$\\

Dados dos mapeos $f : X \rightarrow Y$ y $g : Y \Longrightarrow Z$, el \textit{mapeo composición} de $f$ y $g$, como $f \circ g : X \rightarrow Z $ como

\begin{equation}
g \circ f (x) = g (f(x))
\end{equation}

Si $A \subset X$ se dice que es un \textit{mapa de inclusión}, entonces se define como

\begin{equation}
I : A \rightarrow X \Rightarrow I(a)=a
\end{equation}

para algún $a \in A$

entonces el \textit{mapeo identidad} $I dx : X \rightarrow X$ es un caso especial de un mapeo de inclusión, para algún $A = X$.\\

Si $f : X \rightarrow Y$ definida por $f : X \mapsto f(x)$ es biyectiva, entonce existe un \textit{mapeo inverso} $f^{-1} : Y \rightarrow X$ tal que $f ^{-1} : f(x) \rightarrow X$ biyectiva.\\

Los mapeos $f^{-1}$ satisfacen $f \circ f ^{-1} = idy$ y $f^{-1}\circ f = idx$. Inversamente, si $f : X \rightarrow Y$ y $g : Y \rightarrow X$ se satisface que

 \begin{equation}
f \circ g = i dy \wedge g \circ f = i dx
 \end{equation}

$\therefore f$ y $g$ son biyectivas.\\

 Si se consideran ciertas estructuras algebraicas, las cuales están presentes en los conjuntos $X$ y $Y$. Si $f : X \rightarrow Y$ preserva álgebras, entonces se llama a $f$ un \textit{homeomorfismo.} Si un homeomorfismo en $f$ es biyectivo, se dice que la aplicación $f$ es un $isomorfismo$, denotado

\begin{equation}
X \cong Y
\end{equation}

Generalizando: Sean $X$ y $Y$ espacios topológicos, un mapeo $f : X \rightarrow Y$ es continuo, si la imagen inversa de un abierto en $Y$ es un abierto en $X$. \\


%%\subsection{Vecindades y espacios de Hausdorff}

%Dada una topología $T$ en $X$, se dice que $N$ es una vecindad de un punto $x \ in X$ si $N$ es un subconjunto de $X$ y $N$ contiene al menos un conjunto abierto $U_{i}$ tal que $x \in U_{i}$. Se dice que  es una \textit{vecindad} de $x$. Ya que están contenidos en si mismos, no se requiere que $N$ sea un conjunto abierto en si mismo, en el caso contrario, si $N$ es un conjunto cerrado, se dice que $T$ es una vecindad abierta.\\

%Un ejemplo de esto es el \textit{Espacio de Hausdorff} definido como el espacio topológico $(X, T)$ si, para cada par arbitrario de puntos distintos $x_{i}, x^{i} \in X$ existen vecindades $U_{i}$ y $U_{j}$ de $x$ tal que

%\begin{equation}
%U_{i} \cap U_{j} = \emptyset
%\end{equation}

%%\subsection{Conjunto cerrado}

%Sea $(X, T)$ un espacio topológico. Un subconjunto $A$ de $X$ es cerrado si su complemento es abierto. Esto es, $X - A \in T$. De acuerdo con la definición $X, \emptyset$ son ambos abiertos y cerrados.\\

%Si se considera el conjunto $A$ (abierto o cerrado). \textit{La cerradura} de $A$ es el cerrado más pequeño que contiene a $A$ y es denotado como $\overline{A}$. Con esta propiedad, la intersección de todos los subconjuntos cerrados $\cap F_{i}$ se llama la cerradura de $\overline{A}$.\\

%El \textit{interior} de un conjunto $A$ es la unión de todos los conjuntos abiertos $F_{i}$ de $A$. Se define entonces el \textit{interior} de $A$, denotado $A°$ como el subconjunto más grande que contiene a $A$. Siendo entonces que $A° = \cup _{i} F_{i}$.\\

%La \textit{frontera} $b (A)$ de $A$ es el complemento de $A°$ en $\overline{A}$. Se concluye entonces que un abierto siempre es disjunto de su frontera, mientras que un cerrado siempre incluye a su frontera.\\

\subsection{Conjunto Compacto}

Sea $(X,T)$ un espacio topológico. Una familia $\lbrace A_{i} \rbrace$ de subconjuntos de $X$ es llamada la cubierta de $X$ si

\begin{equation}
\cup A_{i} = X
\end{equation}

Si todos los subconjuntos $A_{i}$ son abiertos, la topología es llamada una \textit{cubierta abierta}.

Sea $X$ un conjunto y todas las posibles cubiertas de $X$. Dicho conjunto se dice que es \textit{compacto} si, para cada cubierta abierta $\lbrace U_{i} | i \in I \rbrace$ existe una subcubierta finita $J$ de $I$ tal que $\lbrace U_{i} | i \in J \rbrace$es cubierta de $X$.

En general, si un conjunto es compacto en $\mathbb{R}^{n}$ debe ser cerrado y acotado, lo que nos conduce al siguiente teorema.

Teorema: Sea $X$ un subconjunto de $\mathbb{R}^{n}$, $X$ es compacto $\Leftrightarrow$ es cerrado y acotado.

\subsection{Conexidad}

Un espacio topológico $X$ es conexo si no puede ser disconexo o inconexo. Sea $X = x_{1} \cup x_{2}$ donde $x_{1}$ y $x_{2}$ son abiertos y $x_{1} \cap x_{2} = \emptyset$.

De igual manera se puede definir un espacio topológico $X$ es conexo si, para algún punto $x,y \in X$, existe un mapeo continuo $f : [0,1] \rightarrow X$ tal que $f(0) = x$ y $f(1) = y$\\

\subsection{Homeomorfismos e invariantes topológicos}

\textit{`` El mapeo conserva estructuras algebráicas''}

Sea $X_{1}$ y $X_{2}$ espacios topológicos cuyo mapeo $f: X_{1} \rightarrow X_{2}$ es un homeomorfismo si es continuo y tiene una inversa $f^{-1}: X_{2} \rightarrow X_{1}$ tambien continua. Si existe un homeomorfismo entre $X_{1}$ y $X_{2}$ se dice que $X_{1}$ es isomorfo a $X_{2}$ y viceversa.\\

En otras palabras, $X_{1}$ es homeomorfismo de $X_{2}$ si existe un mapeo $f : X_{1} \rightarrow X_{2}$ y $g : X_{2} \rightarrow X_{1}$ tal que $fg = idX_{2}$ y $gf=idX_{1}$.\\

Si se dividen a los espacios topológicos como clases de equivalencia, de acuerdo con que tanto es posible deformar un espacio u otro por un homeomorfismo. Entonces, dos espacios topológicos son homeomorfos mutuamente si pueden deformarse uno a otro continuamente.\\

Si dos espacios topológicos tienen distintos invariantes topológicos no pueden ser mutuamente homeomorfos.\\

\section{Variedades}

\textit{``Un objeto que localmente parece un plano''}\\

Se define a $M$ como una \textit{variedad} diferenciable m-dimensional si:\\

i)$M$ es un espacio topológico\\

ii) $M$ está provisto con una familia de pares $\lbrace (U _{i}, \varphi _{i}) \rbrace$\\

iii) $\lbrace U_{i} \rbrace$ Es una familia de abiertos que cubren $M$ tal que $\cup _{i}  U_{i} = M$ con $\varphi _{i}$ un homeomorfismo de $U_{i}$ al subconjunto abierto $U ' _{i}$ de $\mathbb{R}^{m}$.\\

iv) Dado $U_{i}$ y $U_{j}$ tal que $U_{i} \cap U_{j} \neq \emptyset$ el mapeo $\psi _{ij} = \varphi _{i} \varpi _{i} ^{-1}$ de $\varphi_ {j}(U_{i} \cap U_{j})$ infinitamente diferenciable.\\

Un homeomorfismo $\phi$ mapea $U_{i}$ en un subconjunto abierto $U' _{i} \subset \mathbb{R}^{m}$, provee coordenadas al punto $p \in U_{i}$. Entonces, si $U_{i} \neq U_{j} \neq \emptyset$ la transición de un sistema a otro es \textit{suave}.\\

El par $(U_{i}, \varphi _{i})$ es llamado \textit{carta} mientras que toda la familia es llamada \textit{atlas}. Siendo así, el subconjunto $U_{i}$ es llamado \textit{la vecindad coordenada}, mientras que $\varphi _{i}$ es llamada la \textit{función coordenada}. $\varphi _{i}$ es representada por $m$ funciones $\lbrace X^{1}(p), ..., X^{m}(p) \rbrace$. El conjunto $\lbrace X^{\mu} (p) \rbrace$ es llamado la \textit{coordenada}.\\
Es importante señalar que un punto sobre una variedad $p \in M$ existe de manera independiente a su coordenadas, esto quiere decir que es el mismo punto el que nos dice de qué manera determinar sus coordenadas.\\

$M$ es localmente Euclidiano, pues en cada vecindad coordenada $U_{i}$, $M$ aparece como un abierto de $\mathbb{R}^{m}$, cuyo elemento es $\lbrace X^{1}, ..., X^{m} \rbrace$. Nótese que no se requiere que $M$ sea $\mathbb{R}^{m}$ globalmente; basta con que este parezca serlo localmente.\\

Si existe un traslape entre $U_{i}$ y $U_{j}$, dos sistemas coordenados se asignan a un punto en $U_{i} \cap U_{j}$ Por $iv)$, la transición de un sistema coordenado a otro es suave $(C)^{\infty})$. Entonces, el mapeo $\varphi _{i}$ asigna $m$ valores coordenados $ x^{\mu }(1 \leq \mu \leq m)$ a un punto $p \in U_{i} \cap U_{j}$. Mientras que $\varphi _{i}$ asigna $y^{\mu}(1 \leq \mu \leq m)$ al mismo punto; mientras que la transición $y$ a $x$ $x ^{\mu} =x^{\mu}(y)$ está dada por $m$ funciones de $m$ variables. La transformación de funciones coordenadas $x^{\mu} = x^{\mu} (y)$ es la forma explícita del mapeo\\

\begin{equation}
\psi _{ij} = \varphi _{i} \varphi _{j}^{-1}
\end{equation}

La unión de dos atlas $\lbrace (U_{i}, \varphi _{i}) \rbrace$ y $\lbrace (V_{j}, \psi _{j}) \rbrace$ es un atlas. Estos dos atlas se dicen ser \textit{compatibles.} Mientras que tal compatibilidad es una relación de equivalencia, la cual es llamada \textit{estructura diferenciable}. Lo cual concluye que si el atlas es mutuamente compatible, define a la misma estructura diferenciable $M$.\\

\subsection{Cálculo de variedades}

El significado de \textit{variedad diferencial} reside en el hecho que se puede usar el cálculo usual en $\mathbb{R}^{n}$. La suavidad de las transformaciones de coordenadas asegura que el cálculo es independiente de las coordenadas escogidas.\\

\subsection{Mapas Diferenciables.}

Sea $f: M \rightarrow N$ un mapeo para una variedad m-dimensional $M$ a una variedad n-dimensional $N$. Un punto $p \in M$ es un mapeo a un punto $f (p) \in N$ definido $f : p\rightarrow f(p)$. Tomando una carta $(U, \varphi)$ en $M$ y $(V, \psi)$ en $N$, donde $p \in U$ y $f (p) \in V$. Entonces, $f$ tiene la presentación coordenada $\psi f \varphi ^{-1} : \mathbb{R}^{m} \rightarrow \mathbb{R}^{n}$.\\

Si se escribe $\varphi(p) = \lbrace x^{\mu} \rbrace$ y $\psi(f(p)) = \lbrace y^{\alpha} \rbrace$ entonces $\phi f \varphi ^{-1}$ es sólo la función vectorial (valuada) $y = \psi f \varphi ^{-1}$ de $m$ variables. Si $y = \psi f \varphi ^{-1} (x)$ o, en otra notación usual $y ^{\alpha} = f^{\alpha}(x^{\mu})$ es $C^{\infty}$ con respecto a cada $x^{\mu}$. Se dice que $f$ es suave o infinitamente diferenciable en $p$ o en $x = \varphi (p)$. Nótese que se requiere que sea infinitamente diferenciable $(C ^{\infty})$ en armonia con la suavidad de las funciones de transición $\psi _{ij}$, donde dicha diferenciabilidad es independiente del sistema coordenado.\\

%%Esta copiado literal del Nakahara combinado con los comentarios del seminario, le urgen doscientas revisadas y añadir mas bibliografia.

\subsection{ Mapeos inducidos y subvariedades}

Un mapeo suave $f: M \rightarrow N$ induce naturalmente un mapeo $f_{*}$ llamado el \textit{mapeo diferencial}

\begin{equation}
f* : T_{p}M \rightarrow T_{f(p)}M
\end{equation}

La forma explícita de $f$ es obtenida por la definición de vector tangente, como una derivada direccional sobre una curva. Si $g \in F(N)$, entonces $gf \in F(M)$.\\

Un vector $v \in T_{p}M$ actua en $gf$ para dar un número $V[gF]$ se puede definir $f_{*}V \in T_{f(p)}M$ N por

\begin{equation}
(f* V)[g] = V[gf]
\end{equation}

O en términos de mapas, $(U, \varphi)$ en $M$ y $(U\psi)$ en $N$

\begin{equation}
(f_{*}V)[g\psi ^{-1} (x)] = V[gf \varphi ^{-1} (x)]
\end{equation}


%%fusionar ambas ideas, necesita más bibliografia

%%La subseccion de vectores es copia de Nakahara, se tiene que revisar y hasta que no se revise no se incluye fuera de comentarios.

\subsection{Vectores}

%%\subsection{Preeliminares: Espacios Vectoriales y dependencia lineal}


%En la definición clásica de vectores y espacio vectorial, se dice que un espacio vectorial (o espacio lineal) $V$ que actua sobre un campo $K$ es un conjunto en el cual dos operaciones $(+, \times)$ por un elemento del campo, está definido. Dichos elementos satisfacen las ocho propiedades básicas de un espacio vectorial $V$.

%Sea $\lbrace v_{i} \rbrace$ un conjunto de $K$ vectores. Si la ecuación

%\begin{equation}
%x_{1}v_{1}+,...,+x_{k}v_{k}
%\end{equation}

%tiene solución no trivial $x_{i} \neq 0$. para algún $i$, el conjunto de vectores $\lbrace v_{i} \rbrace$ es llamado linealmente dependiente, mientras que si tiene sólo soluciones triviales, $x_{i} = 0$ $\forall i$, el conjunto es llamado linalmente independiente. Si al menos uno de los vectores es el vector $\overline{0}$ el sistema es linealmente dependiente.

%Un conjunto de vectores linealmente indepedientes $\lbrace e_{i} \rbrace$ es llamado la \textit{base} de $v_{i}$, esto implica que cualquier elemento de $v \in V$ se escribe como combinación lineal de los elementos de la base de $V$.
%%Los números $v \in K$ son las componentes del vector $v$ con respecto a la base $\lbrace e_{i} \rbrace$. Si existen $n$ elementos de la base, la dimensión de $V$ es denotada la dimensión de $V = n$ o el espacio n-dimensional (finito).

%Entonces, dados dos espacios vectoriales $V$ y $W$, el mapeo $f : V \rightarrow W$ es llamado el mapa lineal si satisface

%\begin{equation}
%f(a_{1}v_{1}+a_{2}v_{2}) = a_{1}f(v_{1}) + a_{2}f(v_{2}) \forall a_{1},a_{2} \in K
%\end{equation} 



%Un mapeo lineal es un ejemplo de un homeomorfismo que preserva operaciones. La imagen de $f$ es $f(v) \subset W$ y el kernel de $f$ es $\lbrace v \in V |f(v) = 0 \rbrace$ entonces $ker f \neq \emptyset$ ya que $f(0) = 0$ siempre.

%Si $W$ es propiamente el campo $K$, $f$ es llamada una función lineal. Si $f$ es un isomorfismo, se dice que $V$ es isomorfo a $W$ y viceversa. Denotado así como $V \cong W$ lo que lleva al siguiente teorema.

%Teorema: Si $f : V \rightarrow W$ es un mapeo lineal, entonces

%\begin{equation}
%dim v = dim (ker f) + dim (im f)
%\end{equation}

%o bien $dim V = dim W$.

Sea una variedad, se define una curva $\gamma$ tal que

\begin{equation}
\gamma : \langle a, b\rangle \rightarrow M
\end{equation}

la cual es tangente a un punto y existe en el espacio tangente. De esta manera se puede constreñir a la derivada como una función $f : M \rightarrow \mathbb{R}$. Supongamos que el parámetro de cambio es el tiempo $t$ tal que

\begin{equation}
\frac{df}{dt} = \frac{\partial f}{\partial x^{\mu}}
\end{equation}

Se puede definir un difeomorfismo entre las partes, tal que al realizar la derivada de la composición con su inversa $f \cdot \phi ^{-1} (x) |_{p}$ con $\gamma (0) = P$.

\begin{equation}
\frac{df}{dt} \mid _{p} = \frac{\partial f}{\partial x^{\mu} \frac{dx^{\mu}}{dt}}\mid_{t=0}
\end{equation}

con $\mu = 1, 2, ..., n, P \in M, x \in \mathbb{R}^{m}$.

Definición: Sea

\begin{equation}
X^{m} \frac{\partial}{\partial x^{\mu}}\equiv x^{\mu} \hat e_{\mu}
\end{equation}

con $\hat e_{\mu}$ la base coordenada de vectores que es, finalmente el operador derivada.

\begin{equation}
V \equiv V^{\mu} \frac{\partial}{\partial x^{\mu }} = V^{\mu} \partial _{\mu} = V^{\mu} \hat e_{\mu}
\end{equation}

De igual manera que $X[f]$ actua sobre las funciones

\begin{equation}
V[F] = V^{\mu} \frac{\partial}{\partial x^{\mu}} f
\end{equation}

con $f : M \rightarrow \mathbb{R}^{m}$ Dicho operador actua sobre los puntos $P$ tales que al conjunto de todos estos vectores lo llamamos $T_{p}M$. A estos vectores usualmente se les conoce como \textit{covariantes} en la nomenclatura del grupo de Lorentz, mientras que a sus covectores $\omega \in T_{p*}M$ se les llama \textit{contravariantes}; es decir, $\omega _{p} = \omega_{p} dx^{\mu}$ con $dx^{\mu}$ la base de las uno-formas.\\

Nótese que si $df = \frac{\partial f}{x^{\mu} dx^{\mu}}$, $df$ como una 1-forma. Entonces, como $\partial _{\mu} f$ es un vector covariante, su derivada es un vector contravariante. Esto es, si $\omega _{p} \in T_{p*}M \rightarrow \mathbb{R}$ es decir, si se toma un vector este regresa a cambio un número.\\

Dado $\omega _{p} \equiv \omega _{\mu}$ y $V \equiv V^{\mu} \hat e_{\mu}$ se puede escribir\\

\begin{equation}
\omega _{p}[V] \equiv \omega_{\mu} V^{\mu} \equiv \langle \omega_{p},V_{p} \rangle
\end{equation}

como un invariante que es congruente con el producto interno entre vectores y uno-formas. Es decir\\

\begin{equation}
df[V] = \langle df, V\rangle = \frac{\partial f}{\partial x^{\mu}} V^{\mu} \equiv V[f]
\end{equation}

ya que $V \equiv V^{\mu} \partial_{\mu}$. El fin es que $\omega_{\mu} V^{\mu}$ sea invariante ante el grupo de difeomorfismos.\\

\subsection{Uno-formas y su acci\'on}


Como $T_{p}M$ es un espacio vectorial, existe un espacio vectorial dual a $T_{p}M$, cuyo elemento es la función lineal $T_{p}M \rightarrow \mathbb{R}$. El espacio dual es llamado el \textit{espacio contangente} a $p$, denotado como $T* _{p}M$.\\

Un elemento $w : T_{p}M \rightarrow \mathbb{R}$ de $T _{p*}M$ es llamado el vector dual o vector tangente, mientras que en el contexto de formas diferenciales se define como una \textit{uno-forma}.\\

El vector $V$ actua tambien sobre uno-formas, tal que

\begin{equation}
V \equiv V^{\mu} \hat e_{\mu}
\\ V[F] \equiv V^{\mu} \frac{\partial f}{\partial x^{\mu}}
\\ V[\omega] \equiv V^{\mu}\omega _{\mu}
\\ \equiv \langle V, \omega \rangle
\end{equation}

esta congruencia con el producto interno, nos indica que dicho producto es bilineal

\begin{equation}
\omega [V] = V[\omega]
\end{equation}

es decir $V^{\mu} \frac{\partial}{\partial x} (f \cdot \psi^{-1})$\\

%Tengo la solución a este ejemplo, preguntar si va o no dentro de la tesis
Un ejemplo de esto es la identidad. Se puede resolver el caso trivial para $\mathbb{R}^{2}$

\subsection{Base Dual}

En el estudio de uno-formas y vectores, tales que\\

\left\{ \begin{array}{ll} \omega = \omega _{\mu} dx
\\ V = V^{\mu} \hat e_{\mu}
      \end{array} \right. \]

es decir $\omega [V] = \omega_{\mu} V^{\mu}$ define al \textit{vector de contracción}, tal que

\begin{equation}
\begin{split}
= \omega_{p}dx^{\mu}[V]
& \quad = \omega_{\mu dx^{\mu}}[V^{\rho}\hat e_{\rho}]
& \quad = \omega_{\mu V^{\rho}dx^{\mu}}[\hat e_{\rho}]
& \quad = \omega_{\mu} V^{\rho}
& = \delta _{\nu} ^{\mu}
\end{split}
\end{equation}

con $f: M \rightarrow \mathbb{R}$ Observese que en el último término se encuentra una delta de Kronecker que indica la contracción de tensores, es decir\\

\begin{equation}
dx^{\mu}[\hat e_{\rho}] = \delta _{\rho} ^{\mu}
\end{equation}

Esta construcción se observa, dadas las propiedades del tensor de Kronecker, o bien $\delta  = \sum _{\mu = 1} ^{n} e^{\mu} \otimes e_{\mu} = \sum _{\mu \nu} \delta _{\mu}^{\nu} e^{\mu} \otimes e_{\nu} \in T_{p}M \otimes T_{p}*M$.\\


Como se trata de formas bilineales $V \in T_{p}M$ tal que $V:T_{p}*M    \rightarrow \mathbb{R}$. De igual forma $\omega \in T_{p}*M$ tal que $\omega : T_{p}M \rightarrow \mathbb{R}$. Entonces

\begin{equation}
V[\omega] = V^{\mu} \omega_{\nu} = \langle V, \omega \rangle = \langle \omega, V \rangle
\end{equation}

es un invariante ante difeomorfismos.\\

\subsubsection{Espacios Vectoriales duales}

Sea $f: V \rightarrow K$ una función lineal en un espacio vectorial. $V(n,k)$ sobre un campo $K$. Sea $\lbrace \widehat{e}_{i} \rbrace$ la base y tomese un vector arbitrario $v = v' _{1} \widehat{e}_{1}+...,+v^{n} _{n} \widehat{e}_{n}$. A partir de la linealidad de $f$ se puee definir $f(e_{i}) \forall i$. Es importante señalar que el conjunto de funciones lineales está hecha de un espacio vectorial, llamemosle, una combinación lineal de dos funciones lineales, siendo el resultado de esta, una función lineal

\begin{equation}
(a_{1}f_{1}+a_{2}f_{2})(v) = a_{1}f_{1}+a_{2}f_{2}(v)
\end{equation}

Este espacio lineal es llamado el \textit{espacio vetorial dual} de $V(h,k)$ y es denotado $V* = dim V$. Si se introduce la base dual $\lbrace e*^{i} \rbrace$ de $V*$, dado que es una función lineal, entonces $e**(e_{i}) \forall i$. Si se elige la base dual

\begin{equation}
e*^{i}(e_{j}) = \delta _{j} ^{i}
\end{equation}

cualquier función lineal $f$, llamado el vector dual, es expandido en términos de $\lbrace e*^{i} \rbrace$

\begin{equation}
f = f_{i} e*^{i}
\end{equation}

La acción de $f$ en $v$ es interpretada como el producto interno entre el vector fila y el vector columna.

\begin{equation}
f(v) = f_{i}e*^{i}(v^{i}e_{j})=f_{i}v^{j}e+^{i}(e_{j})
\end{equation}


\textbf{Ejemplo:} el producto interior entre una 1-forma y un vector como 

\[ \langle \omega , V \rangle = \omega _{\rho } V^{\rho} \quad \text{invariante bajo difeomorfismos }\]

\[\text{así} \quad V[f]= V^{\mu}\partial_{\mu} f\]

se puede también definir la acción de $v$ ahora en una forma diferencial como

\[V[\omega ]\equiv \langle\omega , V \rangle = \omega_{\rho} V^{\rho} \quad ; \quad V: T_{p}^{*} M \rightarrow\mathfrak{R} \]

y al revés, para una forma 

\[\omega [V] = \langle V , \omega \rangle = V ^{\rho}\omega _{\rho} \quad \omega : T_{p}M \rightarrow \mathfrak{R}\]

así que $V \in T_{p}M$ actúa sobre 1-formas y sobre funciones

\[V[\omega] = V^{\rho} \omega _{\rho }\]

\[V [f] V^{\rho} \partial_{\rho}f \]

\textbf{Ejemplo:} (bases): Tétradas (Vierbein-Vielbein) Tétradas-políadas. Sea $\hat{e}_{\alpha}$ una base arbitraria, ($\hat{e}_{\mu}\equiv \partial_\mu $ se llama base coordenada), entonces

\begin{equation}
\hat{e}_{\alpha} =e_{\alpha} ^{\mu} \frac{\partial}{\partial x^{\mu}} \quad e_{\alpha}^{\mu} g_{\mu \nu} e_{\beta}^{\nu} = \eta _{\alpha \beta}
\end{equation}

a los coeficientes $e^{\mu}_{\alpha}$ se llaman tétradas (en 4 dimensiones). As{i mismo, se definen las inversas $e^{\alpha}_{\mu} \Rightarrow$

\[e_{\mu} ^{\alpha} e_{\nu}^{\beta} \eta {\alpha \beta} = e_{\mu}^{\alpha}(e_{\alpha}^{\rho} g_{\rho \sigma}e_{\beta}^{\sigma})e_{\nu}^{\beta}\]

\begin{equation}
e_{\mu}^{\mu}\eta_{\alpha \beta} e_{\nu}^{\beta} =g_{\mu \nu}
\end{equation}

igualmente

\begin{equation}
e_{\alpha}^{\mu} g_{\mu \nu}e_{\beta}^{\nu} = \eta _{\alpha \beta }
\end{equation}

\subsection{Tensores y campos tensoriales.}

En la forma usual o clasica, un vector es un objeto lineal que mapea un vector a un escalar. Esto puede generalizarse a objetos multilineales llamados \textit{tensores}, que mapean vectores o vectores duales a un escalar. Un tensor dl tipo $(p,q)$ es un mapeo multilinear que mapea al vector o a su dual a $\mathbb{R}$.\\

En una variedad, un tensor mapea $q$ elementos d $T*_{p}M$ y $r$ elementos de $T_{p}M$a un número real

\begin{equation}
T : \otimes^{p} V* \otimes^{q} V \rightarrow R
\end{equation}

l conjunto de todos los tensores de tipo $(p,q)$ es llamado el espacio tensorial de tipo $(p,q)$, denotado $T_{q} ^{p}$. Mientras que el producto tensorial $T = \mu \otimes v \in T_{q} ^{p}$ es un elemento de $T ^{p} _{q+q}$ definido por

\begin{equation}
\begin{split}
T(w_{1},...,w_{p}, \xi_{1},...,\xi _{p}; u_{1},...,u_{q};v_{1},...,v_{q}) 
& = \mu(w_{1},...,w_{p,}, u_{1},...,u_{q})V(\xi_{1},...,\xi_{p}; v_{1},...,v_{q})
\end{split}
\end{equation}


Si $v$ es un vector y este es asignado suavemente a cada punto de $M$, es llamado el campo vectorial sobre $M$. En otras palabras, $V$ es un campo vectorial si,$V[f] \in F(M)$ para alguna $f \in F(M)$.\\

claramente, cada componente de un campo vectorial es una función suave de $M$ a $\mathbb{R}$. $H(M)$ denota al conjunto de campos vectoriales en $M$. Similarmente $T_{p}M$ se define como un campo tensorial $H$ de tipo $(p,q)$ por la asignación suave de un elemento de $T_{c,p} ^{q}$ a cada punto $p \in M$.\\

El conjunto de los campos tensoriales de tipo $(p,q)$ en $M$ es denotado por $\omega ^{1}(M)$ en el contexto de formas diferenciales y, en el mismo $T_{0}^{0} = F(M)$, denotado como $\Omega ^{0}(M)$.\\

\subsection{Producto Tensorial}

%La definición viene del "mathematical for theoretical phycisist

\textbf{Definición:} Eproducto tensorial $E \otimes F$ de dos espacios vectoriales en el mismo campo $K$ está definido tal que, existe un mateo $f : E \times F \rightarrow E \otimes F$ tal que, para cualesquiera espacio vectorial $V$ y un mapeo bilineal $g : E \times F \rightarrow V$, existe un único mapeo lineal $G : E \otimes F \rightarrow V $ tal que $g = G \cdot F$.

Usando esta definición universal; y como se observó en $(1.2.6)$, $dx^{\mu}[V]$ forma lineal tal que

\begin{equation}
dx^{\mu}[V] = fx^{\mu} [V^{\rho} \hat e_{\rho}] = V^{\rho} dx^{\mu}[\hat e_{\rho}] =V^{\mu}
\end{equation}

Es decir, $T_{p}M \otimes T_{p}M \rightarrow (V_{1}, V_{1})$. Se define de esta manera al producto tensorial, actuando sobre el producto cartesiano a los reales

\begin{equation}
dx^{\mu} \otimes dx^{\nu} : T_{p}M \times T_{p}M \rightarrow \mathbb{R}
\end{equation}

demostremos esta afirmación como sigue:

\begin{equation} 
\begin{split}
dx^{\mu} \otimes dx(V_1,V_2) = V_1 ^{\mu} V_2 ^{\nu}
&
dx^{\nu} \otimes dx^{\mu}(V_1,V_2) = V_1 ^{\nu} V_2 ^{\nu}
\end{split}
\end{equation}

Es decir, el producto tensorial es no conmutativo

\begin{equation}
dx^{\mu} \otimes dx^{\nu }\neq dx ^{\nu} dx ^{\mu}
\end{equation}

Al ser formas bilineales, ahora tomando la uno-forma

\begin{equation}
\omega _1 \otimes \omega _{2} = \omega _1 ^{\mu} dx^{\mu} \otimes \omega_2 ^{\nu} dx^{\nu} = \omega_1 ^{\mu} \omega _2 ^{\nu} dx^{\mu} \otimes dx^{\nu}
\end{equation}

actuando sobre vectores

\begin{equation}
v_{1} \otimes v_{2} = v_{1} ^{\mu} v_{2}^{\nu} \hat{e} _{1\mu} \otimes \hat{e}_{2\nu}
\end{equation}

así mismo estos vectores actuan sobre formas $v_{1} \otimes v_{2} : T_{p}M \times T_{p}M$

Como es una forma multilineal, se define la contracción

\begin{equation}
\begin{split}
v_{1} \otimes v_{2} (\omega _{1} , \omega _{2}) =
& = v_{1} ^{\mu} v_{2} ^{\nu} \hat{e}_{1\mu} \otimes \hat{e}_{2\nu} (\omega _{\rho} dx^{\rho}, \omega _{2 \sigma} dx^{\sigma})
& = v_{1} ^{\mu} v_{2} ^{\nu} \omega _{1 \rho} \omega _{2 \sigma} \hat _{1 \rho} \otimes \hat{e}_{2\nu}(dx^{\rho}, dx^{\mu})
& = v_{1 }^{\rho}\omega _{1\rho} v_{2} ^{\sigma} \omega _{2\sigma}
\end{split}
\end{equation}

Se define al \textbf{producto exterior} o producto cuña, siendo $\hat e_{\mu}$ una base, tal que $\hat e _{\mu} \wedge \hat e_{\nu} = \hat e_{\mu \nu} = - \hat e_{\nu \mu}$ si y sólo si $ \mu \neq \nu $

Observe que, ya que el producto es asociativo, es tambien alternante, es decir, se anulan los índices repetidos. El producto es asociativo por definición y alternante, es decir se anula si dos índices son iguales. Nótese que a partir de una cantidad de vectores enésimos de la base, otorga 2-enésimos productos ortogonales. Dichas operaciones constituyen un espacio vectorial subyacente a un álgebra de Grassmann.\\

Tomando estas propiedades, tales que

\begin{equation}
dx^{\mu} \otimes dx^{\nu} - dx^{\nu} \otimes dx^{\mu} \equiv dx^{\mu} \wedge dx^{\nu}
\end{equation}


\textbf{Ejemplo:}

\[\lbrace \gamma^{\alpha} , \gamma^{\beta} \rbrace = 2 \eta ^{\alpha \beta }\]

\[\Rightarrow \gamma^{\alpha } e_{\alpha} ^{\mu} , \gamma ^{\beta} e_{\beta} ^{\nu} \rbrace = 2 e_{\alpha}^{\mu} \eta ^{\alpha \beta}e_{\beta} ^{\nu} =2g^{\mu \nu }\]

\begin{equation}
\lbrace \gamma^{\mu} , \gamma^{\nu}\rbrace= 2 g^{\mu \nu }
\end{equation}

\textbf{Ejemplo:} $dS^2 = dx^2 + dy^2 + dz ^2 $ en coordenadas esféricas

\[dx^{\alpha} \equiv e_{\mu}^{\alpha} dx^{\mu} \quad ; \quad dx = e_{r}^{\prime} dx + e_{\theta}^{\prime } d \theta + e_{\varphi} ^{\prime} d \varphi \]

\[= \frac{\partial x}{\partial r} dr + \frac{\partial x}{\partial \theta}d \theta + \frac{\partial x}{\partial \varphi} d \varphi \]

Cuyo sistema coordenado

\[x =r \sin \theta c\cos \varphi \\ 
y=r \sin \theta \sin \varphi \\
z = r \cos \theta\]

\[dx = \sin \theta \cos \varphi dx + r\cos \theta \cos\varphi d\theta - r \sin \theta \sin \varphi d \varphi \]

\[dy= \sin \theta \sin \varphi dr + r \cos \theta \sin \varphi d \theta + r \sin \theta \cos \varphi d \varphi \]

\[dz = \cos \theta dr-r \sin \theta d \theta \]

\[dx^2 + dy^2 + dz^2 =\]

\[=\sin ^2 \theta \cos ^2 \varphi dr V2 + r^2 \cos ^2 \theta \cos ^2 d \theta ^2 + r^2 \sin^2 \theta \sin ^2 \varphi d \varphi ^2 + \cdots \]

\[\sin ^2 \theta \sin^2 \varphi dr^2 + r^2 \cos ^2 + \sin^2 \varphi d \theta^2 + r^2 \sin^2 \theta \cos^2 \varphi d \varphi ^2\]

\[cps^2 \theta dr^2 \quad r^2 \sin^2 \theta d \theta ^2\]

\[= dr^2 + r^2 d\theta ^2 + r^2 \sin^2 d \varphi ^2 = dr^2 + r^2 (d \theta ^2 + m^2 d \varphi^2 )\]

\[e_{r}^{\prime} = \sin \theta \cos \varphi \quad e_{r}^{2} = \sin \theta \sin \varphi \quad e_{r}^{3} = \cos \theta\]

\[e_{\theta}^{\prime} =  r \cos \theta \cos \varphi \quad e_{\theta} ^{2}=  r \cos \theta \sin \varphi \quad e^{3}_{\theta} = - r \sin \theta \]

\[e_{\varphi}^{\prime}= -r \sin \theta \sin\varphi \quad e_{\varphi}^{2}=r\sin\theta \cos \varphi \quad e^{3}_{\varphi } = 0 \]

\section{Tensor métrico}

%%Capitulo siete, geometr\'ia riemmaniana, variedades riemmanianas y pseudo variedades riemannianas

En geometría elemental, el producto interno de dos vectores $V$ y $U$ está definido por $UV = \sum _{j = i} ^{m} U_{i}V_{i}$, donde $U_{i}$ y $V_{i}$ son las componentes de los vectores en $\mathbb{R}^{m}$. En una variedad, el producto interno está definido para cada espacio tangente $T_{p}M$.\\

%esto esta copiado literalmente del libro!! corregir urgente

Sea $M$ una variedad diferenciable. Se define una \textit{métrica Riemanniana} $g$ en $M$ como un tensor tipo $(0,2)$ en $M$ que satisface los siguientes axiomas para cada punto $p \in M$.\\

(i)$g_{p} (U,V) = g_{p}(V,U)$\\
(ii) $g_{p}(U,U) \geq 0$ Donde la igualdad se cumple si $U = 0$\\

Con $U$, $V \in T_{p}M$. Se define, entonces el producto interno entre un vector $V$ en la variedad $T_{p}M$ y su vector dual $\omega \in T_{p*}M$ como el mapeo de $T_{p*}M \times T_{p}M\rightarrow \mathbb{R}^{m}$. Si existe la métrica $g$, se define el producto como $g_{\rho}(U,V)$ con $U,V \in T_{p}M$. Tal que, $g_{\rho}$ da lugar a un insomorfismo entre el espacio vectorial y su espacio dual. Es decir, que el tensor métrico es una forma multilineal que mapea elementos ordenados del dual y así mismo, elementos del espacip vectorial a los reales.\\

%aca ya viene de el nakahara y de mi pluma
Para los propositos de este trabajo, se considera La métrica de una 2-forma  tal que, el mapeo lineal $U,\phi$ es una carta en $M$ y a esta le asignamos sistemas coordenados $x ^{\mu}$. En el sentido de métrica, se puede considerar el tomar un desplazamiento infinitesimal $dx^{\mu} \partial_{x_{\mu }} \in T_{p}M $ tal que,

\begin{equation}
ds^{2} = g (dx^{\mu} \frac{\partial}{\partial x^{\mu}} , dx ^{\nu} \frac{\partial}{\partial x^{\nu}}) = \\
\\ = g_{\mu \nu} dx^{\mu} dx_{\nu}
\end{equation}

Siendo esta la métrica usual, mientras que en un sentido estricto es un tensor $g = g_{\mu \nu} dx^{\mu} \bigotimes dx^{\nu}$. Dado que se puede ver a la métrica como una matrix simétrica, se consideran sus valores propios reales y estrictamente positivos.\\


%debo calcular la métrica y ponerla aquí bien explicada, podría citar a la de lorentz...



\section{\'Algebra de Lie}

%bibitem 1: Lee. Introduction to smooth manifolds. paj 185. Paréntesis de Lie
%bibitem 2: Mathematical methods, pj 350 Derivada de Lie, pj 412 àlgebras de Lie
%bibitem 3: Mathematical methods in classical mechanics, Arnold. Paj 208. Lie algebra.


Al dotar a un grupo de una estructura topológica, es decir, cuando sus operaciones son continuas se dice que este grupo es continuo, además si se le provee una estructura de variedad compatible con la estructura del grupo, se obtiene un grupo de Lie. 

Dada esta combinación de grupo se encuentran algunas propiedades, como lo es garantizar la suavidad de la variedad, entonces el espacio tangente al elemento de grupo, resume al propio grupo a travez de un álgebra de Lie. Esto implica que la mayoria de los grupos de Lie son isomorfos a un grupo lineal, es decir, se puede obtener por medio de un grupo de operadores matriciales, una representación lineal. 

Las siguientes proposiciones conforman la estructura conocida como álgebra de Lie.\\

\textbf{Definición:} Sean $X$ y $Y$ campos vectoriales suaves sobre una variedad continuamente diferenciable $M$. Dada una función suave $f: M \to \mathbb{R}$ se aplica de $X$ a $f$ para obtener una función $fX$ continuamente diferenciable. Al aplicar $Y$ a esta función, se obtiene una nueva función $YXf = Y(Xf)$. El operador $f \to YXf$ no satisface la regla del producto, es decir, no puede ser un espacio vectorial.\\

Como los vectores tangentes a una variedad en un punto son idetificados por derivadas en dicho punto, los campos vectoriales pueden considerarse como operadores diferenciales, los cuales asignan funciones suaves a funciones suaves, tales que

\begin{equation}
[X(f)](p) = X(p)(f); X \in  \mathfrak{X}(M); f \in C^{\infty}(M); p \in M
\end{equation}

con $\mathfrak{X}$ el conjunto de campos vectoriales. En este sentido, el campo vectorial $X$ es un mapeo lineal, el cual satisface la regla del producto de Leibniz. Es decir, el mapeo lineal $X: \mathfrak{F}(M) \to \mathfrak{F}(M)$ cuyo producto está definido

\begin{equation}
X(fg) = X(f)g + fX(g)
\end{equation}

Dicho mapeo lineal puede verse como $[X,Y] = X \circ Y - Y \circ X $ llamado el \textit{conmutador} del mapeo. A dicho mapeo se le conoce como el \textit{paréntesis de Lie.}\\

\textbf{Proposición:} Para todas $X, Y, Z \in \mathfrak{X}(M)$ El paréntesis de  Lie satisface las siguientes propiedades:\\

i) Antisimetría: Para $X, Y \in \mathfrak{X}$ se cumple

\begin{equation}
[X,Y] = -[Y,X]
\end{equation}

ii) Bilinearidad: Sean $a, b \in \mathbb{R}$, 

\begin{equation}
\begin{split}
[aX+bY,Z] = a[X,Z]+b[Y,Z],
& [Z, aX+bY] = a[Z,X]+b[Z,y]
\end{split}
\end{equation}


iii) Identidad de Jacobi: 

\begin{equation}
[X,[Y,Z]]+[Y,[Z,X]]+[Z,[X,Y]]=0
\end{equation}

iv) Para $f,g \in C^{\infty}(M)$,

\begin{equation}
[fX,gY]=fg[X,Y]+(fXg)Y-(gYf)X
\end{equation}  

Cabe mencionar que en el sentido estricto, esta no es un álgebra al no cumplir con la asociatividad del producto, sin embargo nótese que la identidad de Jacobi es un sustituto para la asociatividad, lo cual no ocurre en general para paréntesis en álgebras de Lie en espacios no reales. 

Para concluir esta sección se considera una última definición del conmutador de Lie.\\

\textbf{Definición:} Se dice que dos campos vectoriales $X, Y$ conmutan si su paréntesis de Lie es el campo vectorial cero $\hat 0$.\\

\subsection{Flujos y Derivada de Lie}

%bibitem es el Smooth manifolds de Lee, paj 205 esque está re bien escrito...
%bibitem Mathematical Methods
%bibitem Nakahara

Al asociar objetos geométricos sobre campos vectoriales suaves, se piensa en curvas integrales, o bien curvas suaves sobre las cuales, el cambio de posición en cada punto es igual al valor del campo vectorial en dicho punto. Una colección de curvas integrales en un campo vectorial dado sobre una variedad determina una familia de difeomorfismos, dicho conjunto es un \textit{flujo}. 


%%Editar con más cuidado las siguientes proposiciones y teoremas.


\textbf{Proposición:} Sea $X$ un campo vectorial suave sobre una variedad suave M. Para cada punto $p \in M$ existe $\epsilon > 0$ y una curva suave $\gamma:(-\epsilon , \epsilon) \rightarrow M$ que es una curva integral de $X$ que comienza en $p$\\

Si se considera que $X$ y $Y$ campos vectoriales, \textit{generan} un flujo, utilizando la siguiente proposición.

\textbf{Proposición:} Sea $\theta : \mathbb{R} \times M \rightarrow M$ un flujo global suave sobre una variedad suave $M$ el generador infinitesimal $V$ de $\sigma$ es un campo vectorial suave sobre $M$, y cada curva $\sigma ^{(p)}$ es una curva integral de $V$. Es decir, $\sigma^{(p)}$ es una curva integral sobre $V$.\\

Dados los flujos $\sigma (s,x)$ y $\tau(t,x)$ generados por $X$ y $Y$ campos vectoriales, tal que, al moverse de un parámetro $\epsilon$ a lo largo del flujo $\sigma$ y despues tomando un parámetro $\gamma$ del flujo $\tau$; por la propiedad de no conmutatividad en el producto de un álgebra de Lie visto en (), en otro caso, se toma ahora al $\gamma$ y despues $\epsilon$ a lo largo de $\tau$ y $\sigma$, respectivamente, obtenemos las propiedades de un paréntesis de Lie. 

A partir de la siguiente definición, se generaliza el concepto de derivada de Lie aplicada a uno-formas.\\

\textbf{Definición:} Sea define a $\omega \in \Omega ^{1}(M)$ a lo largo de $X \in \mathfrak{X}$ tal que, su derivada de Lie

\begin{equation}
\mathfrak{L}_{X} \omega \equiv lim_{\epsilon \to 0} \frac{1}{\epsilon} ((\sigma _{\epsilon})* \omega \vert _{\sigma _{\epsilon}(x)} - \omega \vert _{x})
\end{equation}  

si se toma $\omega = \omega _{\mu} dx^{\mu}$ esta expresión queda de la forma

\begin{equation}
\mathfrak{L}_{x}\omega = (X^{\nu} \partial _{\nu} \omega _{\mu} + \omega _{\nu} \partial_{\mu } X^{\nu}) dx^{\nu}
\end{equation}

nótese que al tomar a $\omega$ en $x$ en () con $(\sigma _{\epsilon})* \omega \vert _{\sigma _{\epsilon}(x)} = \omega _{\mu} (x) dx^{\mu} + \epsilon [X^{\nu}(x) \partial \nu \omega _{\mu}(x)+\partial _{\mu} X^{\nu}\omega _{\nu} (x)]dx^{\mu}$ implica ().

Finalmente, la derivada de Lie para cualesquiera campo vectorial tiene las siguientes propiedades.\\

%%corolario9.39 paj 230 Lee

\textbf{Corolario:} Sea $M$ una variedad suave y $X, Y, Z \in \mathfrak{X}(M)$ se cumplen las siguientes propiedades.

(i) $\mathfrak{L}_{X} Y = - \mathfrak{L} _{Y}X$
(ii) $\mathfrak{L}_{X}[Y,Z]=[\mathfrak{_{v} y, z}] + [yy, \mathfrak{L}_{X}Z$
(iii)$\mathfrak{L}_[X,Y]Z = \mathfrak{L}_{X}\mathfrak{L}_{Y} Z - \mathfrak{L}_{Y} \mathfrak{L}_{X}Z$
(iv)Si $g \in C^{\infty}(M)$, entonces $\mathfrak{L}_{X}(gY) = (Xg)Y + g \mathfrak{L}_{X}Y$
(v)Si $F:M \rightarrow N$ es un difeomorfismo, entonces $F_{*}(\mathfrak{L}_{X}Z) = \mathfrak{F_{*}X}F_{*}Z$


\subsection{Pull Back y Push Forward }



Para asociar campos tensoriales definidos sobre una variedad a otra, se consideran las aplicaciones diferenciales denotadas \textit{pull back} (aplicación regrediente) y \textit{push forward} (aplicación progrediente); estas actuan sobre mapas pero tienen un comportamiento partícular con respecto a otros tensores. Para estas transformaciones se considera una carta diferenciable, un difeomorfismo y estas dos operaciones que son opuestas mutuamente.

\textbf{Definición} \textit{(Pull back, push forward)}: Se define un mapeo diferenciable, sobre dos variedades $M, N$ tal que se cumple:\\

(i)Push-Forward. Para cualesquiera campo vectorial, si $f_{*}: \mathfrak{X}(TM) \rightarrow \mathfrak{X}(TN)$ es decir $f_{*} X = f'X$ si y sólo si $f_{*}X(f(q)) = f'(q)X(q)$\\
(ii)Pull-BacK. Para cualesquiera 0-formas, si $f_{*}: \mathfrak{X}((\sigma_{0})TM*) \rightarrow \mathfrak{X}(\omega _{0}TN*)$ es decir $f_{*}g = g \cirq f^{-1}$ si y sólo si $f_{*}g(q) = g (f^{-1} (q))$\\
(iii) Pull-Back. Para cualesquiera 1-formas, si $f_{*}: \mathfrak{X}((\sigma_{1})TN*) \rightarrow \mathfrak{X}(\omega _{1}TM*)$ es decir $f^{*}h = \mu \cirq f'$ si y sólo si $f^{*}\mu(p) = \mu (f (p)) \cirq f'(p)$\\

Notese que para realizar estas operaciones, no es necesario que las variedades posean la misma dimensión, es decir, no requiere el ser un difeomorfismo. Sin embargo, la definición se sigue a las operaciones inversas; donde si se requiere que $M$ y $N$ sean difeomorfos. Entonces, se definen las operaciones inversas como:

(i)Pull-Back. Para cualesquiera campos vectoriales, si $f_{*}: \mathfrak{X}(TN) \rightarrow \mathfrak{X}(TM)$ es decir $f_{*} Y = (f^{-1})'X$ si y sólo si $f_{*}Y(p) = (f^{1})'(f(p))Y (f(p))$\\
(ii) Push-Forward. Para cualesquiera 0-formas, si $f_{*}: \mathfrak{X}((\sigma_{0})TN*) \rightarrow \mathfrak{X}(\omega _{0}TM*)$ es decir $f^{*}h = h \cirq f$ si y sólo si $f^{*}h(p) = h (f (p))$\\
(iii) Pull-Back. Para cualesquiera 1-formas, si $f_{*}: \mathfrak{X}((\sigma_{1})TM*) \rightarrow \mathfrak{X}(\omega _{1}TN*)$ es decir $f_{*} = \overline{\omega} \cirq (f^{-1})'$ si y sólo si $f_{*}\nu(q) = \nu (f^{-1} (q)) \cirq (f^{-1}'(p))$\\

%\textbf{Observación:} Dichas operaciones, para tensores pueden definirse, tal que, para $M, N$ variedades con la misma dimensión, y cualesquiera difeomorfismo. La aplicación Pull-Back y Push-Forward para tensores se define, como () y () respectivamente.

%\begin{equation}
%f_{*}: \mathfrak{X}(\otimes_{s}^{r}TM)\rightarrow \mathfrak{X}(\otimes_{s}^{r}TN)::(f_{*}S_{p})(f(p))=f_{r,s}'(p)S_{p} 
%\end{equation}

%\begin{equation}
%f^{*}: \mathfrak{X}(\otimes_{s}^{r}TM)\rightarrow \mathfrak{X}(\otimes_{s}^{r}TN):: (f^{*}S_{q})(f^{-1}(q)) = %(f_{r,s}')^{-1}(q)S_{q}
%\end{equation}

%donde $f_{r,s}'(p) : \otimes _{s} ^{r} T_{p}M \rightarrow \otimes _{s} ^{r} T_{p}M t_{f_{(p)}}$ es una extención de las operaciones del isomorfismo $f'(p): T_{p}M \rightarrow T_{f_{(p)}}N$. \\

%Las propiedades de las operaciones de push-forward y pull-back pueden generalizarse con el siguiente teorema.\\

%%Teore 1514 paj 348 del mathematical methods, es priginalmente de Kolar p.62

\textbf{Teorema :} Para cualesquiera operadores push forward y pull back, $f_{*}$ y $f_^{*}$ respectivamente, aplicados a tensores, estos pueden verse como operadores lineales tales que $f^{*} \in \mathfrak{L} (\mathfrak{X}(\otimes _{s} ^{r} TM)); \mathfrak{X} (\otimes _{s} ^{r} TN$ sea el operador de push forward y  $f_{*} \in \mathfrak{L} (\mathfrak{X}(\otimes _{s} ^{r} TM)); \mathfrak{X} (\otimes _{s} ^{r} TN$ el operador de pull back. Ambos operadores asociados al mapeo inverso del otro, es decir $f^{*} = (f^{-1})_{*}$ y $f_{*} = (f^{-1})^{*}$; tales que, ambos preserven el conmutador entre campos vectoriales

\begin{equation}
[f_{*}X_{1}, f_{*}X_{2}] = f_{*}[X_{1}, X_{2}]
\end{equation}

Además, cumplen con la operación del producto exterior para r-formas
\begin{equation}
\begin{split}
f^{*}(\overline{\omega}) f^{*} \bigwedge \pi ) = f^{*}\overline{\omega} \bigwedge f^{*}\pi
& f_{*}(\overline{\omega}) f^{*} \bigwedge \pi ) = f_{*}\overline{\omega} \bigwedge f_{*}\pi 
\begin{split}
\end{equation}

Y su composición es de la forma

\begin{equation}
\begin{split}
(f \cirq g)^{*} = g^{*} \cirq f^{*}
& (f \cirq g)_{*} = f_{*} \cirq g_{*}
\end{split}
\end{equation}

Finalmente, los operadores conmutan con la derivada exterior (si esta es de clase 2):

\begin{equation}
\begin{split}
d(f^{*} \overline{\omega}) = f^{*}(d \overline{\omega})
& d(f_{*}\overline{\omega}) = f_{*}(d \overline{\omega})
\begin{split}
\end{equation}

nótese que para esta última parte del teorema, se retoma el concepto de derivada para r-formas, es decir, se define una estructura conocida como la \textit{derivada exterior} que se analizará en la próxima sección.\\


\textbf{Ejemplo:} Se puede definir el producto cuña

\begin{equation}
dx^{\mu} \wedge  dx^{\nu} \equiv dx^{\mu}\otimes dx^{\nu}-dx^{\nu} \otimes dx^{\mu }
\end{equation}

esta definición da al jacobiano correcto inmediatamente

\[\int fdx \wedge dy \wedge dz \longrightarrow \int f J dx^{\prime} \wedge dy^{\prime \wedge dz^{\prime}}\]

\[J \equiv det \vert \frac{\partial x^{\prime}}{\partial x} \vert\]








\subsection{Derivada Exterior}

Definimos a ls coeficientes de la conexión en la base coordenada como

\begin{equation}
\nabla_{\alpha}e_{\beta} \equiv \Gamma_{\alpha\beta}^{\gamma} e_{\gamma}
\end{equation}

con $\alpha ,\beta, \gamma , \delta , \cdots $ índices de la base no-coordenada y $\nu , \nu ,\rho ,\sigma,\cdot $ índices de la base coordenada. 

\[e_{\alpha} : \quad \text{base no coordenada}\]

\[dx^{\alpha} : \quad \text{base dual}\]

Ejemplo: (Tarea) En la base no coordenada

\[T_{\beta  \gamma} ^{\alpha} = \langle d \theta^{\alpha} ,T(e_{\beta} e_{\gamma}) \rangle = \Gamma_{\beta \gamma} ^{\alpha} -\Gamma_{\gamma \beta}^{\alpha} - C _{\beta \gamma} ^{\alpha} \]


y

\[ R_{\beta \gamma \delta}^{\alpha } = e_{\gamma}[\Gamma_{\delta \beta} ^{\alpha} ] + \Gamma _{\delta \beta} ^{\epsilon} \Gamma_{\gamma \epsilon } \]

\[-\Gamma_{\gamma\beta}^{\epsilon} - C_{\gamma \delta}^{\epsilon} \Gamma_{\epsilon \beta} ^{\alpha}\]

aquí

\[e_{\gamma} [\Gamma-{\delta \beta}^{\alpha} = e _{\gamma} ^{\nu}e_{\nu} (\Gamma_{\delta \beta} ^{\alpha} ) = e_{\gamma}^{\nu} \partial_{\nu} (\Gamma_{\delta \beta}^{\alpha}) \]

Definición (Derivada exterior) : Si $\omega$ es una k-forma 

\[\omega = \frac{1}{k!} \omega_{\alpha_{1} \cdots \alpha_{k}} dx^{\alpha_{1}} \wedge \cdots \wedge dx^{\alpha_{k}} \quad , \quad \text{entonces} \]

\[d \omega = \text{derivada exterior} = \frac{1}{k!}\partial _{\alpha} \omega_{\alpha_{1} \cdots \alpha_{k}} dx^{\alpha}\wedge dx^{\alpha_1 }\wedge \cdot \wedge dx^{\alpha _{k}}\]

\[\text{Lema 1: } \quad d(\zeta \wedge \omega ) =d \zeta \wedge \omega + (-)^{q} \zeta \wedge d \omega\]

cuando $\zeta$ es una q-forma y $\omega$ es una $p-forma$ $\zeta \in \mathfrak{T}_{q}^{0}$

\[d(\zeta \wedge \omega) =d (\frac{1}{q!} \zeta_{\mu_{1}\cdots \mu_{k}} dx^{\mu_{1}}\wedge \cdots \wedge dx^{\mu_{q}} \wedge \frac{1}{p!} \omega_{\nu_{1}\cdots \nu_{p}} dx^{\nu_{1}} \cdots \wedge dx^{\nu_{p}}) \]

\[= d(\frac{1}{q!p!} \zeta _{\mu_{1} \cdots \mu_{q}} \omega_{\nu_{1} \cdots \nu_{p}} dx^{\mu_{1}} \cdots dx^{\mu_{p}} \wedge dx^{\nu_{1}} \cdots dx^{\nu_{q}}) \]

\[=(\frac{1}{q!p!} \partial_{\mu} \zeta_{\mu_{1} \cdots \mu_{q}} \partial _{\nu} \omega_{\nu_{1}\cdots \nu_{p}} dx^{\nu} \wedge dx^{\mu_{1}} \cdots dx^{\mu_{q}}\wedge dx^{\nu_{1}}\cdots dx^{\nu_{q}})\]

\begin{equation}
d \zeta \wedge \omega + (-)^{q} \zeta \wedge d \omega
\end{equation}

Lema 2 : Si $X ,Y \in \mathfrak{T}_{0}^{1}$ vectores y $\omega \in \mathfrak{T}_{1}^{0}$ una uno-forma; entonces

\[X[\omega (Y)] - \omega ([X,Y]) = d \omega (X,Y)\]

\[X^{\mu} \partial_{\mu} (\omega_{\rho y^{\rho}})- Y^{\mu} \partial_{\mu}(\omega_{\rho}X^{\rho}) - \omega_{\rho}(X^{\mu}y^{\rho}-Y^{\mu} \partial_{\mu}X^{\rho})\]

\[X^{\mu} Y^{\rho}\partial_{\mu}\omega_{\rho}-Y^{\mu} X^{\rho} \partial_{\mu}\omega_{\rho}= \partial_{\mu}\omega_{\rho}(X^{\mu}Y^{\rho - Y^{\mu}X^{\rho }})\]

\[d \omega (X,Y) \quad \text{q.e.d.}\]


\subsection{Estructura de Cartán}

Definiremos a $\omega_{\beta} ^{\alpha}\equiv \Gamma_{\gamma \beta}^{\alpha} dx^{\gamma}$ como la conexión de 1-forma. Entonces $\omega_{\beta} ^{\alpha}$ satisface las ecuaciones de estructura de Cartán.

\begin{equation}
d(dx^{\alpha})+ \omega_{\beta}^{\alpha} \wedge dx^{\beta} =  T^{\alpha}
\end{equation}

\begin{equation}
d \omega_{\beta} ^{\alpha} + + \omega_{\gamma}^{\alpha}\wedge \omega_{\beta}^{\gamma} = R_{\beta}^{\alpha}
\end{equation}

\[\text{Donde} \quad T^{\alpha} \equiv \frac{1}{2} T_{\beta \gamma}^{\alpha} dx^{\beta} \wedge dx^{\gamma} \quad \text{Curvatura 2-forma}\]

y

\[R_{\beta} ^{\alpha} \equiv \frac{1}{2} R^{\alpha} _{\beta \gamma \delta} dx^{\gamma} \wedge dx^{\delta} \quad \text{Curvatura 2-forma}\]

Dem: $T^{\alpha} (\hat{e}_{\gamma},\hat{e}_{\delta}) =d (dx^{\alpha})(\hat{e}_{\gamma},\hat{e}_{\delta}) + \omega_{\beta}^{\alpha} (\hat{e_{\gamma}}) \otimes dx^{\beta} (\hat{e}_{\delta})$

\[- dx^{\beta} (\hat e_{\gamma})\otimes \omega^{\alpha}_{\beta}(\hat{e}_{\delta})\]

pero $\omega_{\beta}^{\alpha} = \Gamma_{\epsilon \beta}^{\alpha} dx^{\epsilon}$ y

\[d(dx^{\alpha}) (\hat{e}_{\gamma} ,\hat{e}_{\delta})= e_{\gamma}[dx{\alpha}(\hat{e}_{\delta})] - e_{\delta} [dx^{\alpha}(\hat{e}_{\gamma})] - dx^{\alpha}([\hat{e}_{\gamma},\hat{e}_{\delta}])\]

\[= -dx^{\alpha} (C_{\gamma \delta}^{\epsilon}\hat{e}_{\epsilon}) = -C_{\gamma \delta}^{\alpha}\]

\[\Rightarrow T^{ \alpha} ( \hat{e}_{ \gamma}, \hat{e}_{ \delta})=-C_{ \gamma \delta}^{ \alpha}+ \Gamma_{ \gamma \delta }^{\alpha} - \Gamma_{\delta \gamma }^{\alpha }\]

\[= \frac{1}{2} T_{\beta \epsilon}^{\alpha} d x^{\beta} \wedge dx^{\epsilon} (\hat{e}_{\gamma} ,\hat{e}_{\delta}) = T_{\gamma\delta}^{\alpha}\]

análogamente como $\omega_{\beta}^{\alpha} \in \mathfrak{T}_{1}^{0}(M)$ es una 1-forma

\[d \omega _{\beta}^{\alpha} (X,Y) = X[\omega_{\beta}^{\alpha}(Y)] -Y [\omega_{\beta}^{\alpha} (X)] - \omega_{\beta}^{\alpha} ([X,Y])\]

queda

\[R_{\beta}^{\alpha}(\hat{e}_{\gamma},\hat{e}_{\delta}) =[d \omega _{\beta}^{\alpha} + \omega _{\epsilon} ^{\alpha} \wedge \omega_{\beta}^{\epsilon}] (\hat{e}_{\gamma}, \hat{e}_{\delta})\]

\[= e_{\gamma} [\omega_{\beta}^{\alpha} (\hat{e}_{\delta})] - e_{\delta} [\omega_{\beta}^{\alpha} (\hat{e}_{\delta})] - \omega_{\beta}^{\alpha} ([\hat{e}_{\gamma},\hat{e}_{\delta})\]

\[+ \omega _{\epsilon}^{\alpha} \wedge \omega _{\beta}^{\epsilon} (\hat{e}_{\gamma},\hat{e}_{\delta}) \Gamma_{\kappa \epsilon}^{\alpha} dx^{\kappa} \wedge \Gamma_{\iota \gamma} ^{\epsilon} dx^{\iota}(\hat{e}_{\gamma} , \hat{e}_{\delta})\]

\[= e_{\gamma}[ \Gamma_{\delta \beta}^{\alpha}]-e_{\delta} [\Gamma_{\gamma \beta}^{\alpha} -C_{\gamma \delta}^{\epsilon} \Gamma_{\epsilon \beta}^{\alpha} +\]

\[+ \Gamma_{\gamma \epsilon} ^{\alpha}\Gamma_{\delta \beta}^{\epsilon} - \Gamma_{\delta \epsilon}^{\alpha} \Gamma_{\gamma \beta} ^{\epsilon}\]

\textbf{TAREA:} Demostrar las identidades de Bianchi, esto tomando la derivada exterior

\begin{equation}
dT^{\alpha} + \omega _{\beta} ^{\alpha} \wedge T^{\beta} =R_{\beta}^{\alpha} \wedge dx^{\beta}
\end{equation}

\begin{equation}
dR_{\beta}^{\alpha} + \omega _{\gamma} ^{\alpha} R_{\beta}^{\alpha} - R_{\gamma}^{\alpha} \wedge \omega _{\beta}^{\gamma} = 0
\end{equation}

Demostración: 

a) \[T^{\alpha} = d (dx^{\alpha}) + \omega_{\beta}^{\alpha} \wedge dx^{\beta}\]

\[\Rightarrow dT^{\alpha} = d \omega _{\beta}^{\alpha} \wedge dx^{\beta} - \omega_{\beta}^{\alpha} \wedge d(dx^{\beta})\]

pero

\[d (dx^{\beta}) =T^{\beta} -\omega _{\gamma}^{\beta} \wedge dx^{\gamma}\]

\[\Rightarrow dT^{\alpha}= d\omega_{\beta}^{\alpha} \wedge dx^{\beta} - \omega_{\beta}^{\alpha} \wedge [T^{\beta} - \omega_{\gamma}]\]

\[\Rightarrow dT^{\alpha} + \omega _{\beta}^{\alpha} \wedge T \beta = d \omega _{\beta}^{\alpha } \wedge dx \beta + \omega_{\beta}^{\alpha} \wedge \omega _{\gamma}^{\beta} \wedge dx^{\gamma}\]

y

\[dT^{\alpha} + \omega _{\beta}^{\alpha} = R_{\beta}^{\alpha} - \omega _{\gamma}^{\alpha} \wedge \omega _{\beta}^{\gamma }\]

\[\Rightarrow dT^{\alpha} + \omega _{\beta}^{\alpha} \wedge T^{\beta} = [R_{\beta} ^{\alpha} - \omega_{\gamma} ^{\alpha} \wedge \omega _{\beta}^{\gamma}] \wedge dx^{\beta}+\]

\[+ \omega_{\beta}^{\alpha} \wedge \omega_{\gamma}^{\beta} \wedge dx^{\gamma} = R_{\beta}^{\alpha} \wedge dx^{\beta} \quad \text{q.e.d.}\]

\[\text{b)} \quad R_{\beta}^{\alpha} = d \omega _{\beta}^{\alpha} + \omega_{\gamma}^{\alpha} \wedge \omega _{\beta}^{\gamma} \]

\[\Rightarrow d R_{\beta}^{\alpha} =  d \omega_{\gamma}^{\alpha} \wedge \omega_{\beta}^{\gamma} - \omega_{\gamma}^{\alpha} \wedge d \omega _{\beta}^{\gamma}\]

pero

\[d \omega_{\beta}^{\alpha} = R_{\gamma}^{\alpha} - \omega _{\epsilon}^{\alpha} \wedge \omega _{\gamma}^{\epsilon } \]

\[d \omega_{\beta}^{\gamma} = R^{\gamma}_{\beta} - \omega _{\epsilon}^{\gamma} \wedge \omega _{\beta}^{\epsilon } \]


\[\Rightarrow d R_{\beta}^{\alpha} = [R_{\gamma}^{\alpha} - \omega _{\epsilon}^{\alpha} \wedge \omega _{\gamma}^{\epsilon}] \wedge \omega _{\beta}^{\gamma} -\]

\[- \omega_{\gamma}^{\alpha} \wedge [R_{\beta}^{\gamma} - \omega_{\epsilon}^{\gamma} \wedge \omega_{\beta}^{\epsilon}] \]

\[= R_{\gamma}^{\alpha} \wedge \omega_{\beta}^{\gamma} - \omega_{\gamma}^{\alpha} R_{\beta}^{\gamma}\]

\[dR _{\beta}^{\alpha} + \omega _{\gamma}^{\alpha} \wedge R_{\beta}^{\gamma} - R_{\gamma}^{\alpha} \wedge \omega _{\beta}^{\gamma} = 0\]

\[\frac{1}{2} \partial _{\mu} R^{\alpha}_{\beta\gamma \delta} dx^{\mu} \wedge dx^{\gamma } \wedge dx^{\delta} \]

\[+ \Gamma_{\epsilon \kappa }^{\alpha} dx^{\epsilon} \wedge R_{\beta \gamma \delta }^{\kappa} dx^{\gamma} \wedge dx^{\delta } \]

\[- R_{\kappa \gamma \delta}^{\alpha } dx^{\gamma} \wedge dx^{\delta} \wedge \Gamma-{\epsilon \beta} ^{\kappa } dx^{\epsilon } = 0 \]

lo que da

\[R_{\kappa \gamma \delta ; \mu }^{\alpha} + R_{\gamma \mu \gamma ; \delta}^{\alpha} + R_{\gamma \delta \mu ; \gamma} ^{\alpha} = 0 \]

El marco local

\[g_{\mu\nu} : m(m+1)/2 \text{indica que en } \quad m , \quad m=4 \Rightarrow \text{hay 10}\]

\[e_{\alpha}^{\mu} : m^2 \quad \text{y hay 16}\]

\[m^2 - m(m+1)/2 = 6\]

En una variedad Lorentziana es el grupo de Lorentz

\[dx_{(p)}^{\alpha} \rightarrow dx^{\prime \alpha}_{(p)} = \Omega _{\beta}^{\alpha} dx_{(p)}^{\beta} \quad \text{base no coordenada}\]

\[en P \in M \quad \text{donde } \quad \Omega _{\beta }^{\alpha}\in \text{SO}(m-q,1)\]

y de nuevo, como $\langle \omega , x\rangle = \omega _{\alpha} x^{\alpha}$ es un invariante, entonces

\[\Omega_{\beta} ^{\alpha}\eta ^{\beta \gamma}\Omega_{\gamma}^{\delta} = \eta ^{\alpha\delta} \quad \text{Loretziana} \]

\[\Omega\in \text{SO}(m-1,1)\]

\textbf{Ejemplo: }Para un tensor $T$

\[T = T_{\nu}^{\mu} e_{\mu}\otimes dx^{\nu} = T_{\beta}^{\alpha} e_{\alpha} \otimes dx^{\beta} \quad e_{\mu}^{\alpha} e_{\beta}^{\nu}\]

\[T_{\beta}^{\alpha} = e_{\mu}^{\alpha}e_{\beta}^{\nu}T^{\mu \nu}\]

\[e_{\alpha}^{\prime} = (\Omega^{-1})_{\alpha}^{\beta} e_{\alpha} \quad dx^{\prime \alpha} =\Omega _{\beta}^{\alpha}dx^{\beta}\]

En el caso de la torsión $T^{\alpha } = d(dx^{\alpha }) + \omega _{\beta}^{\eta } \wedge d x^{\beta}$ tenemos

\[T^{\alpha} \rightarrow T^{\prime \alpha }= d(dx^{\prime \alpha})+\omega _{\beta}^{\prime \alpha} \wedge dx^{\prime \beta}\]

\[= \Omega_{\gamma}^{\alpha} (dx^{\gamma}) + \omega_{\beta}^{\gamma} \wedge dx^{\beta } \]

pero $dx^{\prime \alpha}=\Omega _{\delta}^{\alpha} dx^{\delta}$

\[\Rightarrow T^{\prime \alpha} = d(d\Omega_{\delta}^{\alpha} dx^{\delta}) \]

\[= d(\Omega_{\delta}^{\alpha}) \wedge dx^{\delta}+\Omega_{\delta}^{\alpha}d(dx^{\delta}) +\omega _{\beta}^{\prime \alpha} \wedge \Omega _{\gamma}^{\beta}dx^{\gamma }\]

comparando

\[\Rightarrow\Omega_{\gamma}^{\alpha} [d(dx^{\gamma})+\omega_{\beta} ^{\gamma}\wedge dx^{\beta}] =d(\Omega _{\delta}^{\alpha})\wedge d x^{\delta} + \Omega_{\delta}^{\alpha}d (dx^{\delta}) \]

\[+ \omega_{\beta}^{\prime \alpha} \wedge \Omega _{\gamma}^{\beta} dx^{\gamma}\]

\[\Rightarrow \Omega _{\gamma}^{\alpha}\omega _{\beta}^{\gamma}\wedge dx^{\beta}-d (\Omega _{\delta}^{\alpha}) \wedge dx^{\delta} = \omega_{\beta}^{\prime \alpha}\wedge \Omega_{\gamma}^{\because}dx^{\gamma}\]

\[\omega _{\beta}^{\prime \alpha} \Omega_{\gamma}^{\beta} = \Omega_{\beta}^{\alpha}- d(\Omega_{\gamma}^{\alpha}) \]

multiplicando $x (\Omega^{-1})_{\delta}^{\gamma} \Rightarrow$

\[\omega_{\delta}^{\prime \alpha} = \Omega _{\beta}^{\alpha}\omega_{\gamma}^{\beta} (\Omega ^{-1})_{\delta}^{\gamma} - d (\Omega_{\gamma}^{\alpha}) (\Omega ^{-1})^{\gamma} - \Omega_{\gamma}^{\alpha}d(d\Omega^{-1})_{\delta}^{\gamma}\]

\begin{equation}
\therefore \omega_{\delta}^{\prime \alpha}=\Omega_{\beta}^{\alpha} \omega_{\delta}^{\gamma}(\Omega^{-1})_{\delta}^{\gamma}+\Omega_{\gamma}^{\alpha}d(d \Omega^{-1}))_{\delta}^{\gamma}
\end{equation}

\section{Ecuación de Dirac}

En espacios planos la ecuación de Dirac es:

\begin{equation}
[i\gamma^{\alpha}(\partial _{\alpha} - m)] \alpha = 0
\end{equation}

\[\text{aquí} \quad \psi \rightarrow \Lambda \psi \quad \text{y} \quad \overline{\psi} \rightarrow \overline{\psi} \Lambda^{-1}\]

\[\Omega_{\delta}^{\alpha}dx^{\gamma}\]

cuando hacemos una transformación de $SO(m-1,1)$

\[x^{\alpha} \rightarrow x^{\prime \alpha}= \Omega_{\beta}^{\alpha}x^{\beta}\]

Queremos definir una derivada covariante que en el sistema (marco) local se transforme como 

\[\nabla_{\alpha} \psi \rightarrow \nabla_{\alpha}^{\prime} = e_{\alpha}^{mu} \quad \nabla_{\mu} \text{es la base coordenada} \]

además, $\nabla_{\mu}$ se llama \textit{derivada covariante}

\[\nabla_{\mu} = \partial_{\mu} + \Gamma_{\mu}\]

donde, se desconoce $\Gamma_{\mu }$, entonces

\[\nabla _{\alpha} \psi \rightarrow \nabla _{\alpha}^{\prime} \psi^{\prime} = e_{\alpha}^{\prime \mu} (\partial_{\mu} + \Gamma_{\mu}^{\prime})\psi^{\prime}\]

\textbf{Nota: }

\[\Gamma_{\mu} \equiv \frac{i}{2} \Gamma_{\mu}^{\alpha \beta} S_{\alpha \beta}\]

\[\Rightarrow \Gamma_{\mu}^{\prime} = \frac{i}{2} \Gamma_{\mu \beta}^{\alpha} S_{\alpha}^{\beta} - \frac{i}{2} \Gamma_{\mu \epsilon}^{\alpha} \epsilon_{\beta}^{\epsilon} S_{\alpha}^{\beta}+\]

\[+ \frac{i}{2} \epsilon_{\epsilon}^{\alpha} \Gamma_{\mu \beta}^{\epsilon} S_{\alpha}^{\beta} - \partial_{\mu} \epsilon_{\beta}^{\alpha} S_{\alpha}^{\beta} \frac{i}{2}\]

\[= \Gamma_{\mu} + \frac{i}{2} \epsilon_{\alpha \beta} (\Gamma_{\mu}^{\alpha} S_{\gamma}^{\beta} - \Gamma_{\mu \delta}^{\beta}S^{\alpha \delta}) - \frac{i}{2} \partial_{\mu} \epsilon_{\alpha \beta} S^{\alpha \beta}\]

en partícular, la parte $e_{\alpha}^{\mu} \psi \rightarrow e_{\alpha}^{\prime \mu}\partial_{\mu} \psi^{\prime} $

\[= (\Omega^{-1})_{\alpha}^{\beta}e_{\beta}^{\mu} \partial_{\mu} \Lambda \psi\]

\[(\Omega^{-1})_{\alpha}^{\beta}e_{\beta}^{\mu} [\partial_{\mu} \Lambda \psi + \Lambda \partial _{\mu} \psi] \]

\begin{equation}
\therefore \nabla_{\alpha}^{\prime} \psi^{\prime} = (\Omega^{-1})_{\alpha}^{\beta} e_{\beta}^{\mu} \lbrace (\partial_{\mu} \Lambda \psi + \Lambda \partial_{\mu} \psi) + \Gamma_{\mu}^{\prime} \Lambda \psi \rbrace
\end{equation}

\[\text{pero esto es igual a} \quad = \Lambda (\Omega^{-1})_{\alpha}^{\beta} e_{\beta}^{\mu} (\partial_{\mu} + \Gamma_{\mu}) \psi\]

\begin{equation}
\therefore (\Omega^{-1})_{\alpha}^{\beta} e_{\beta}^{\mu} [(\partial_{\mu} \Lambda ) \psi + \Gamma _{\mu}^{\prime} \Lambda \psi ] = \Lambda(\Omega^{-1})_{\alpha}^{\beta} e_{\beta}^{\mu} \Gamma_{\mu} \psi
\end{equation}

en índices espinoriales queda:

\[\partial_{\mu} \Lambda + \Gamma_{\mu}^{\prime} \Lambda = \Lambda \Gamma_{\mu} \quad \text{ecuación matricial con índices espinoriales}\]

\begin{equation}
\Rightarrow \Gamma_{\mu}^{\prime} = \Lambda \Gamma_{\mu} \Lambda^{-1} - (\partial_{\mu} \Lambda) \Lambda^{-1}
\end{equation}

recordemos que $\Lambda = e^{i/2 \epsilon_{\alpha \beta} S^{\alpha \beta}}$ y además

\[S^{\alpha \beta} = \frac{i}{4} [\gamma^{\alpha} , \gamma^{\beta}\]

\[\Rightarrow \Gamma_{\mu} \rightarrow (\Gamma + \frac{i}{2} \epsilon_{\alpha \beta} S^{\alpha \beta}) \Gamma_{\mu} (\Gamma- \frac{i}{2} \epsilon_{\alpha \beta} S^{\alpha \beta }) - \frac{i}{2} \partial _{\mu} \epsilon_{\alpha \beta} S^{\alpha \beta}\]

recordemos ahora que la conexión 1-form $\omega_{\delta}^{\alpha} \equiv \Gamma_{\beta \delta}^{\alpha} dx^{\beta}$ se transforma como

\[\omega_{\delta}^{\alpha} \rightarrow \omega_{\delta}^{\prime \alpha} = \Omega_{\beta}^{\alpha} \Omega_{\gamma}^{\beta} \omega_{\gamma}^{\beta} (\Omega^{-1})_{\delta}^{\gamma} + \Omega_{\gamma}^{\alpha} d(\Omega^{-1})_{\delta}^{\Gamma}\]

\[\Gamma_{\mu \delta}^{\prime \alpha} \equiv (\delta_{\beta}^{\alpha} + \epsilon_{\beta}^{\alpha}) \Gamma_{\mu \epsilon}^{\beta} (\delta_{\delta}^{\epsilon} - \epsilon_{\delta}^{\epsilon}) - (\delta_{\epsilon}^{\alpha} + \epsilon_{\epsilon}^{\alpha} )\]

\[=\Gamma_{\mu \delta}^{\alpha} - \Gamma_{\mu \epsilon}^{\alpha} \epsilon_{\delta}^{\epsilon} + \epsilon_{\beta}^{\alpha} \Gamma_{\mu \delta}^{\beta} - \partial_{\mu} \epsilon_{\delta}^{\alpha}\]

hay que notar que

\[\omega_{\beta}^{\prime \alpha} = \Gamma_{\mu \beta}^{\prime \alpha }dx^{\prime \mu } \equiv \Gamma_{\mu \beta}^{\prime \alpha} dx^{\mu}\]

porque $dx^{\mu}$ es inerte frente a $SO(d-1,1)$. entonces, para el coeficiente mixto $\Gamma_{\mu \delta}^{\prime \alpha }$

\[\Rightarrow \Gamma_{\mu \delta}^{\prime \alpha} = \Gamma_{\mu \delta}^{\alpha} - \Gamma_{\mu \epsilon}^{\alpha} e
_{\delta}^{\epsilon} + \epsilon_{\beta}^{\alpha} \Gamma_{\mu \delta}^{\beta} - \partial_{\mu} \epsilon_{\delta}^{\alpha}\]

de donde se obtiene la solución, con $S_{\alpha \beta} = i/4 [\gamma ^{\alpha} , \gamma^{\beta}$

\begin{equation}
\Gamma_{\mu} = \frac{i}{2} \Gamma{\mu}^{\alpha \beta} S_{\alpha\beta} 
\end{equation}

donde $\nabla _{\alpha} \hat{e}_{\beta} = \Gamma_{\alpha \beta}^{\gamma} \hat{e}_{\gamma}$

\[\Rightarrow \nabla_{\mu} \hat{e}_{\beta} = \Gamma_{\mu \beta}^{\gamma} \hat{e}_{\gamma}\]

Demostración: 

\[\frac{i}{2} \Gamma_{\mu}^{\alpha \beta} S_{\alpha \beta} \rightarrow \frac{i}{2}(\Gamma_{\mu}^{\alpha \beta} + \epsilon_{\gamma}^{\alpha} \Gamma_{\mu}^{\gamma \beta} - \Gamma_{\mu \gamma}^{\alpha} \epsilon^{\gamma \beta} - \partial_{\mu} \epsilon ^{\alpha \beta})S_{\alpha \beta} \]

\[= \frac{i}{2} \Gamma_{\mu}^{\alpha \beta} \S_{\alpha \beta }  + \frac{i}{2} (\epsilon_{\gamma}^{\alpha} \Gamma_{\mu}^{\gamma \beta }S_{\alpha \beta}  \Gamma_{\mu \gamma}^{\alpha} \epsilon^{\gamma \beta} S_{\alpha \beta}) - \frac{i}{2} \partial_{\mu} \epsilon^{\alpha \beta } S_{\alpha \beta}\]

\[= \frac{i}{2} \Gamma_{\mu} + \frac{1}{2} (\epsilon_{\gamma}^{\alpha} \Gamma_{\mu}^{\gamma \beta} S_{\alpha \beta} - \Gamma_{\mu \gamma}^{\alpha} \epsilon^{\gamma \beta} S_{\alpha \beta}) - \frac{1}{2} \partial_{\mu} \epsilon^{\alpha \beta} S_{\alpha \beta }\]

pero

\[\frac{i}{2} \epsilon^{\alpha \beta} [S_{\alpha \beta} , \frac{i}{2} \Gamma_{\mu}^{\gamma \delta} S_{\gamma \delta}\]

\[= \frac{i}{2} \epsilon^{\alpha \beta} (\frac{i}{2} \Gamma_{\mu}^{\gamma \delta}) [S_{\alpha \beta} , S_{\gamma \delta}]\]

nótese que el conmutador es igual a 

\[[S_{\alpha \beta} , S_{\gamma \delta}] = - i (\eta_{\gamma \beta} S_{\alpha \delta} - \eta_{\gamma \delta} S_{\beta \delta} + \eta_{\delta \beta} S_{\gamma \alpha} - \eta _{\delta \alpha} S_{\gamma \beta})\]

\[\Rightarrow \frac{i}{2} \epsilon^{\alpha \beta} \frac{1}{2} \Gamma_{\mu}^{\gamma \delta } (\eta_{\gamma \beta} S_{\alpha \delta} - \eta_{\gamma \delta} S_{\beta \delta} + \eta_{\delta \beta} S_{\gamma \alpha} - \eta _{\delta \alpha} S_{\gamma \beta})\]

\[= \frac{i}{2} \epsilon_{\gamma}^{\alpha} \frac{1}{2} \Gamma_{\mu}^{\gamma \beta} S_{\alpha \beta} + \frac{i}{2} \epsilon_{\beta}^{\alpha} \frac{1}{2} \Gamma_{\mu}^{\beta \delta} S_{\alpha \delta}\]

\[= \frac{i}{2} \epsilon_{\gamma}^{\alpha} \Gamma_{\mu}^{\gamma \beta} S_{\alpha \beta} - \frac{i}{2} \Gamma_{\mu \gamma}^{\alpha} e^{\gamma \beta} S_{\alpha \beta }\]


así, entonces

\[0 = [i \gamma^{\alpha} \nabla_{\alpha} - m] \psi = [i \gamma e_{\alpha}^{\mu} (\partial_{\mu} + \Gamma_{\mu}) - m]\psi  \]

\[\text{con} \quad \Gamma_{\mu} = \frac{i}{2} \Gamma_{\mu}^{\alpha \beta} S_{\alpha \beta}: \quad \text{conexión de espín}\]

y en presencia de un campo electromagnético $A_{\mu}$:

\begin{equation}
[i \gamma^{\alpha} e_{\alpha}^{\mu} (\partial_{\mu} + A_{\mu} + \Gamma_{\mu}) - m] \psi = 0
\end{equation}

\end{document}
