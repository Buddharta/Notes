% Created 2024-10-29 Tue 16:14
% Intended LaTeX compiler: pdflatex
\documentclass[letterpaper]{book}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{lmodern} % Ensures we have the right font
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath, amsthm, amssymb, amsfonts, amssymb, amscd}
\usepackage[table, xcdraw]{xcolor}
%\usepackage{mdsymbol}
\usepackage{tikz-cd}
\usepackage{float}
\usepackage[spanish, activeacute, ]{babel}
\usepackage{color}
\usepackage{transparent}
\graphicspath{{./figs/}}
\usepackage{makeidx}
\usepackage{afterpage}
\usepackage{array}
\usepackage{pst-node}
\newcommand{\ind}{\perp\!\!\!\!\perp}
\newtheorem{teorema}{Teorema}[section]
\newtheorem{prop}[teorema]{Proposici\'on}
\newtheorem{cor}[teorema]{Corolario}
\newtheorem{lema}[teorema]{Lema}
\newtheorem{def.}{Definici\'on}[section]
\newtheorem{afir}{Afirmaci\'on}
\newtheorem{conjetura}{Conjetura}
\renewcommand{\figurename}{Figura}
\renewcommand{\indexname}{\'{I}ndice anal\'{\i}tico}
\newcommand{\zah}{\ensuremath{ \mathbb Z }}
\newcommand{\rac}{\ensuremath{ \mathbb Q }}
\newcommand{\nat}{\ensuremath{ \mathbb N }}
\newcommand{\prob}{\textbf{P}}
\newcommand{\esp}{\mathbb E}
\newcommand{\eje}{{\newline \noindent \sc \textbf{Ejercicio. }}}
\newcommand{\exe}{{\newline \noindent \sc \textbf{Ejemplo. }}}
\newcommand{\obs}{{\newline \noindent \sc \textbf{Observación. }}}
\newcommand{\sol}{{\newline \noindent \sc \textbf{Solución. }}}
\newcommand{\dem}{{\noindent \sc Demostraci\'on. }}
\newcommand{\bg}{\ensuremath{\overline \Gamma}}
\newcommand{\ga}{\ensuremath{\gamma}}
\newcommand{\fb}{\ensuremath{\overline F}}
\newcommand{\la}{\ensuremath{\Lambda}}
\newcommand{\om}{\ensuremath{\Omega}}
\newcommand{\sig}{\ensuremath{\Sigma}}
\newcommand{\bt}{\ensuremath{\overline T}}
\newcommand{\li}{\ensuremath{\mathbb{L}}}
\newcommand{\ord}{\ensuremath{\mathbb{O}}}
\newcommand{\bs}{\ensuremath{\mathbb{S}^1}}
\newcommand{\co}{\ensuremath{\mathbb C }}
\newcommand{\con}{\ensuremath{\mathbb{C}^n}}
\newcommand{\cp}{\ensuremath{\mathbb{CP}}}
\newcommand{\rp}{\ensuremath{\mathbb{RP}}}
\newcommand{\re}{\ensuremath{\mathbb R }}
\newcommand{\hc}{\ensuremath{\widehat{\mathbb C} }}
\newcommand{\pslz}{\ensuremath{\mathrm{PSL}(2,\mathbb Z) }}
\newcommand{\pslr}{\ensuremath{\mathrm{PSL}(2,\mathbb R) }}
\newcommand{\pslc}{\ensuremath{\mathrm{PSL}(2,\mathbb C) }}
\newcommand{\hd}{\ensuremath{\mathbb H^2}}
\newcommand{\slz}{\ensuremath{\mathrm{SL}(2,\mathbb Z) }}
\newcommand{\slr}{\ensuremath{\mathrm{SL}(2,\mathbb R) }}
\newcommand{\slc}{\ensuremath{\mathrm{SL}(2,\mathbb C) }}
\newcommand{\mdlr}{\ensuremath{\mathrm{M}}}
\author{Carlos Eduardo Martínez Aguilar}
\date{\today}
\title{Notas de Estadistica y Aprendizaje de máquina estadístico-probabilistico Latex Export}
\hypersetup{
 pdfauthor={Carlos Eduardo Martínez Aguilar},
 pdftitle={Notas de Estadistica y Aprendizaje de máquina estadístico-probabilistico Latex Export},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 29.4 (Org mode 9.7.11)}, 
 pdflang={Esp}}
\begin{document}

\maketitle
\tableofcontents

\part{Repaso elemental de probabilidades.}
\label{sec:org9a00240}

\chapter{Teoría básica de la probabilidad.}
\label{sec:org2f2616e}

\noindent \textbf{\huge Introducción}
\vspace{0.5cm}\\
\noindent Al hablar de las bases de una teoría, existe una tendencia en matemáticas a enfocarse en los principios axiomaticos de la misma, la estadística como ciencia se basa en la teoría de la probabilidad como base fundamental de la misma. Esto de por sí trae controversias puesto que la teoría de la probabilidad en su versión convencionalmente aceptada por la comunidad matemática, se fundamenta en la visión Kolmogoroviana basada en la teoría de la medida, la cual trae con sigo las pradojas obenida através de lateoría de conjuntos y el axioma de elección. Esto sin mencionar las particularidades que la estadística como disciplina ha obetnido como disciplina humana a través de los años.

Por otra parte La historia de la probabilidad como rama matemática comienza con Fermat cuando investiga los problemas que un amigo apostador de Pascal le propone. La así llamada "definición clásica" de la probabilidad se puede encontrar por ejemplo en el libro "Teoría Analítica de las Probabilidades" de Laplace donde define

\begin{quote}
La probabilidad de un suceso es la razón en número de casos favorables y el número total de casos posibles, siempre que nada obligue a creer que alguno de estos casos debe de tener lugar de preferencia a los demás, lo que hace que todos sean para nosotros, igualmente posibles. -- P.S. Laplace
\end{quote}

Al leer uno la definición anterior se da uno cuenta de que ésta es una noción intuitiva, pero claramente incompleta de probabilidad ya que desde el comienzo estanoción no puede tratar facilmente con probabilidades tonde los casos nostan todos "igualmente posibles". Desde los tiempos de Bernoulli (1713) se conocian resultados como la ley (débil) de los grandes números, sin embargo no es hasta el siglo veinte que se considera una base axiomática basada en la teoría de la medida para la teoria probabilística que se encuentra una demostración rigurosa del la ley de los grandes números y el teorema del límite central. Más aún, existen dos vertientes ideológicas distintas de la teoría de la probabilidad

\begin{itemize}
\item Una es la Probabilidad ``objertiva'', la cual se basa en la axiomatización dada por la teoría de la medida, la cual fue detallada por Kolmogorov y postula a la probabilidad como el resultado que permite calcular los ``outcomes'' de una serie de experimentos los cuales son en teoría infinitamente repetibles (von Mises).

\item La otra es la Probabilidad "subjetiva", propuesta por Keynes, la cual busca responder preguntas como: ``¿Mañana va a lloverá?'' o ``Existe vida en saturno?'' Su influencia mayor está dada por la escuela bayesiana de estadística.
\end{itemize}
\section{Probabilidad y medida}
\label{sec:org8e8e3ed}
\noindent Por nuestra parte haremos uso de la visión Kolmogoroviana de la probabilidad.
\begin{def.}
Una \emph{sigma-álgebra} sobre un conjunto \(\Omega\) es una colección de subconjuntos de \(\Omega\), la cual denotamos por \(\Sigma\) y cumple lo siguiente
\begin{itemize}
\item \(\emptyset\in\Sigma\) y \(\Omega\in\Sigma\)
\item \(A\in\Sigma\) implica \(\Omega\setminus A\in\Sigma\)
\item \(\{A_n\}_{n\in\nat}\subset\Sigma\) entonces \(\bigcup_{n\in\nat} A_n\in\Sigma\)
\end{itemize}
A los elementos de \(\Sigma\) se les llama como \emph{conjuntos medibles} o para la probabilidad \emph{eventos} y al par \((\Omega,\Sigma)\) se le llama un espacio medible.
\end{def.}

\eje Se define a la \(\sigma\) álgebra generada por \(\mathrm{F}\) como la \(\sigma\) álgebra más chica que contiene a \(\mathrm{F}\), esto es equivalente a la intersección de todas las \(\sigma\) álgebras que contienen a \(\mathrm{F}\) y se denotará por \(\sigma(\mathrm{F})\).

\eje Un ejemplo típico de espacio medible es el comjunto de números reales \(\Omega=\re\) y la sigma álgebra generado por los intervalos abiertos, también conocida como la sigma-álgebra de Borel (en escencia son conjuntos complicados del tipo \(F_{\sigma}\) y \(G_{\delta}\)).

\obs Dado un conjunto \(\Omega\) y una sigma álgebra \(\Sigma\) y \(U\subset\Omega\) siempre existe la sub-sigma-álgebra
\[
    \Sigma(U)=\{U\cap E\,|\,E\in\Sigma\}
\]

\begin{def.}
Dado un conjunto \(\Omega\), decimos que una familia de subconjuntos \(\mathrm{F}\) de \(\Omega\)
\begin{itemize}
\item Es un \(\pi\) sistema, si es cerrado bajo intersecciones finitas.

\item Es un \(\lambda\) sistema o sistema de Dynkin si cumple
\begin{itemize}
\item \(\Omega\in\mathrm{F}\)
\item \(A,B\in\mathrm{F}\) y \(A\subset B\), entonces \(A\setminus B\in\mathrm{F}\)
\item Si \(\{A_n\}_{n\in\nat}\subset\mathrm{F}\) es una sucesion creciente, entonces \(\bigcup_{n\in\nat}A_n\in\mathrm{F}\)
\end{itemize}
\end{itemize}

\end{def.}
\begin{lema}
    Una familia $\Sigma$ de subconjuntos de un conjunto $\Omega$ es una $\sigma$ álgebra si y sólo si es un $\pi$ sistema y $\lambda$ sistema.
\end{lema}
\begin{def.}
Sea \(\mathrm{F}\) una familia de subconjuntos de \(\Omega\)
\begin{itemize}
\item Definimos a la \(\sigma\) álgebra generada por \(\mathrm{F}\) como la \(\sigma\) álgebra más chica que contiene a \(\mathrm{F}\), esto es equivalente a la intersección de todas las \(\sigma\) álgebras que contienen a \(\mathrm{F}\) y se denotará por \(\sigma(\mathrm{F})\).
\item De forma similar el \(\lambda\) sistema generado por \(\mathrm{F}\) se denota \(\lambda(\mathrm{F})\) y es el \(\lambda\) sistema más chico que contiene a \(\mathrm{F}\).
\end{itemize}
\end{def.}
\begin{teorema}[Teorema de clases monótonas de Sierpinsky]
Sea $\mathrm{F}$ un $\pi$ sistema de sunconjuntos de $\om$ y $\mathrm{G}$ un $\lambda$ sistema tal que $\mathrm{F}\subset\mathrm{G}$, entonces $\sigma(\mathrm{F})\subset\mathrm{G}$ o equivalentemente $\lambda(\mathrm{F})=\sigma(\mathrm{F})$
\end{teorema}
\noindent\dem Como una \(\sigma\) álgebra s un \(\pi\) sistema y \(\lambda\) sistema, entonces \(\lambda(\mathrm{F})\subset\sigma(\mathrm{F})\). Por lo tanto si demostramos que \(\lambda(\mathrm{F})\) es una \(\sigma\) álgebra, entonces el teorema estaría demostrado, así sólos hay que ver que \(\lambda(\mathrm{F})\) es un \(\pi\) sistema. Sea \(C\in\mathrm{F}\), entonces definimos
\[
    \mathrm{G}_{1}:=\{A\in\lambda(\mathrm{F})\,\vert\,A\cap C\in\lambda(\mathrm{F})\}.
\]
\noindent Veamos que \(\mathrm{G}_1\) es un \(\lambda\) sistema
\begin{itemize}
\item Como \(\om\cap C=C\in\lambda(\mathrm{F})\), entonces \(\om\in\mathrm{F}\)
\item Si \(A,B\in\mathrm{G}_1\) con \(A\subset B\), entonces \(A\cap C\subset B\cap C\in\lambda(\mathrm{F})\) y como es \(\lambda\) sistema por definición, entonces \(A\cap C\setminus B\cap C=(A\setminus B)\cap C\in\lambda(\mathrm{F})\)
\item De igual manera, si  \(\{A_n\}\subset\mathrm{G}_1\) es una sucesión creciente de subconjuntos, entonces \(\bigcup A_n\cap C\in\lambda(\mathrm{F})\) y por lo tanto \(\bigcup A_n\mathrm{G}_1\).
\end{itemize}
Por lo tanto \(D_1\) es un \(\lambda\) sistema contenido en \(\lambda(\mathrm{F})\) que contiene a \(\mathrm{F}\) pues éste es un \(\pi\) sistema, así \(\lambda(\mathrm{F})\subset\mathrm{G}_1\), por lo tanto \(C\cap A\in\lambda(\mathrm{F})\) para todo \(C\in\mathrm{F}\) y \(A\in\lambda(\mathrm{F})\). Similarmente, si tomamos \(A\in\lambda(\mathrm{F})\) fija, definimos
\[
    \mathrm{G}_2=\{B\in\lambda(\mathrm{F})\,\vert\,B\cap A\in\lambda(\mathrm{F})\},
\]
\noindent se puede demostrar que \(\mathrm{G}_2\) es un \(\lambda\) sistema que contiene a \(\mathrm{F}\), entonces \(\lambda(\mathrm{F})\subset\mathrm{G}_2\), así \(\lambda(\mathrm{F})\) contiene intersecciones finitas y por lo tanto es una \(\sigma\) álgebra.
\qed\\

\begin{def.}
\noindent Entenderemos un \emph{espacio de probabilidad} como un espacio de medida \((\Omega,\Sigma,\prob)\) con medida uno, es decir un conjunto con una \emph{sigma-álgebra} \(\Sigma\) y una medida \textbf{P}, tal que sea una medida de probabilidad, es decir que cumple:

\begin{itemize}
\item \(\prob(A)\geq 0\)
\item \(\prob(\Omega)=1\)
\item Si \(\{A_n\}\subset\Sigma\) es tal que \(A_n\cap A_{n+1}=\emptyset\), entonces
\end{itemize}
\[
\prob(\bigcup_{n=1}^{\infty} A_n)=\sum_{n=1}^{\infty} \prob(A_n)
\]
\end{def.}

Se le denomina a los conjuntos medibles de un espacio de probabilidad como \emph{eventos} y a dicha medida le llamamos \emph{medida de probabilidad}, así mismo se le denomina al espacio total \(X\) como \emph{espacio de muestras}. Así la medida de probabilidad \textbf{P} evaluada en un evento \(E\), \(\prob(E)\) es naturalmente la probabilidad del evento \(E\) y a un evento de probabilidad cero se le conoce como \emph{evento nulo}. Si un evento tiene como complemento un evento nulo, en otra palabras, si el evento tiene probabilidad 1 diremos que el evento es \emph{casi seguro}. Cuando una proposicion ocurre fuera de un conjunto de probabilidad cero, decimos que ocurre \emph{casi seguramente}.

\eje El ejemplo prototípico son los intervalos finitos de números reales cuya sigma álgebra es el álgebra de los conjuntos \emph{borelianos} o los \emph{medibles de lebesgue} y la medida de Lebesgue reescalada para medir exactamente 1 \(P([a,b])=1/(b-a)\lambda\), donde \(\lambda\) es la medida de lebesque usual.


\noindent \big*Algunas propiedades*

\begin{enumerate}
\item \(\prob(\emptyset)=0\), pues \(\prob(E)=\prob(E\cup\emptyset)=\prob(E)+\prob(\emptyset)\).
\item \(\prob(\om\setminus A)=1-\prob(A)\) para todo evento \(A\), pues \(1=\prob(\om)=\prob(A\cup(\om\setminus A))=\prob(A)+\prob(\om\setminus A)\).
\item Si \(A_1\subset A_2\), donde \(A_1,A_2\in\sig\), entonces \(\prob(A_1)\leq\prob(A_2)\), pues
\[
   A_2=A_1\cup(A_2\cap(\om\setminus A_1))\,\implies\prob(A_2)=\prob(A_1)+\prob(A_2\cap(\om\setminus A_1))\geq\prob(A_1)
   \]
\item Si \(A_1,A_2\in\sig\), entonces
\[
   \prob(A_1\cup A_2)=\prob(A_1)+\prob(A_2)-\prob(A_1\cap A_2)
   \]
\end{enumerate}
\noindent\dem Observamos que
\[
A_1=(A_1\cap A_2)\cup(A_1\cup(\om\setminus A_2)),\quad A_2=(A_2\cap A_1)\cup(A_2\cup(\om\setminus A_1)).
\]
Por lo tanto
\[
\prob(A_1)=\prob(A_1\cap A_2)+\prob(A_1\cup(\om\setminus A_2)),\quad \prob(A_2)=\prob(A_2\cap A_1)+\prob(A_2\cup(\om\setminus A_1)).
\]
Ahora, por otra parte podemos describir a la unión como 
\[
A_1\cup A_2=(A_1\cap A_2)\cup(A_1\cup(\om\setminus A_2))\cup((\om\setminus A_1)\cap A_2).
\]
Como éstos conjuntos son disjuntos, nos da
\[
\prob(A_1\cup A_2)=\prob(A_1\cap A_2)+\prob(A_1\cup(\om\setminus A_2))+\prob((\om\setminus A_1)\cap A_2).
\]
Lo cual aunado a lo anterior, nos da el resutado.
\eje Demuestre de forma inductiva la siguiente fórmula para \(A_1,\dots,A_n\in\sig\)
\begin{align*}
\prob(A_1\cup A_2\cup\dots\cups A_n)=\sum_{i}\prob(A_i)-\sum_{i\leq j}\prob(A_i\cap A_j)+\\
\sum_{i\leq j\leq k}\prob(A_i\cap A_j\cap A_k)+\cdots+(-1)^{n-1}\prob(A_1\cap\dots\cap A_n).
\end{align*}

\begin{lema}
Sea $(\om,\sig,\prob)$ un espacio de probabilidad tal que $\sig=\sigma(\mathrm{F})$ si $\mu_1$ y $\mu_2$ son medidas en $\sig$ tales que $\mu_1=\mu_2$ en $\mathrm{F}$, entonces $\mu_1=\mu_2$.
\end{lema}
\obs A partir de estos resultados se puede demostrar el teorema de extensión de Caratheodory.
\eje Si \textbf{P} y \textbf{Q} son dos probabilidades en \((\Omega,\Sigma)\), se puede ver que
\[
    \mathrm{F}=\{A\in\Sigma\,\vert\,\textbf{P}(A)=\textbf{Q}(A)\}
\]
es un \(\lambda\) sistema, entonces si un \(\pi\) sistema contenido en \(\mathrm{F}\) que genera a \(\Sigma\), entonces \textbf{P} y \textbf{Q} son iguales.
\section{Probabilidad condicional e independencia}
\label{sec:org6c2eb32}
\noindent Comunemente se define la probabilidad condicional de un evento \(A\) con respecto a otro evento no nulo \(B\) como la siguiente fórmula
\[
    \prob(A|B)=\frac{\prob(A\cap B)}{\prob(B)},
\]
Notamos que entonces \(\prob(A\cap B)=\prob(B)\prob(A|B)\).
\begin{def.}
Decimos que dos eventos \(A,B\in\sig\) son \emph{independientes} si se cumple que
\[
\prob(A|B)=\prob(A),
\]
o equivalentemente 
\[
\prob(A\cap B)=\prob(A)\prob(B),
\]
\end{def.}
\obs Dado un espacio de probabilidad \((\om,\sig,\prob)\) y un evento no nulo \(A\in\sig\), es posible definir el \emph{subespacio de probabilidad} \((A,\sig_A,\prob_A)\) como \(\sig_A=\{A\cap B\,|\,B\in\sig\}\) y \(\prob_A(B)=\prob(B|A)\)

\eje Demuestre que \((A,\sig_A,\prob_A)\) en efecto es un espacio de probabilidad.

Si nos enfocamos en la indepencenia de \(\sigma\) álgebras y queremos describimos a partir de este la noción usual de independencia, entonces se define la independencia como
\begin{def.}{Independencia}
Sea \((\om,\sig,\prob)\) un espacio de probabilidad, decimos que una familia de sub \(\sigma\) álgebras \(\{\mathrm{F}_i\}_{i\in I}\) son \emph{independientes} si para cualquier \(\{i_1,\dots,i_n\}\subset I\) subconjunto fininto, se tiene que
\begin{equation}\label{indepen}
    \prob(A_{i_1}\cap\dots\cap A_{i_n})=\prod_{j=1}^n\prob(A_{i_j})\hspace{0.2cm}A_{i_j}\in\mathrm{F}_{i_j}.
\end{equation}
Similarmente a los eventos que cumplan con la ecuación \ref{indepen}, les llamaremos eventos \emph{independientes}.
\end{def.}

En términos de de \emph{eventos independientes} como sabemos la ecuación que se cumple es
\[
    \prob(A_{i_1}\cap\dots\cap A_{i_n})=\prod_{j=1}^n\prob(A_{i_j}),
\]
\noindent queremos generalizar este tipo de ecuaciones pero en el sentido de \(\pi\) sistemas en lugar de en nuestro sentido original de \(\sigma\) álgebras ya que estas son más complicadas de usar. Por lo que el siguiente lema es muy útil para este tipo de cuestiones
\begin{lema}\label{sigma_ind}
Sean $\mathrm{G}$ y $\mathrm{H}$ sub sigma álgebras de $\sig$ y sean $I$ y $J$ $\pi$ sistemas tales que
\[
    \sigma(I)=\mathrm{G},\quad\sigma(J)=\mathrm{H}.
\]
Entonces $\mathrm{G}$ y $\mathrm{H}$ son independientes si y solo si
\[
    \prob(A\cap B)=\prob(A)\prob(B)\quad A\in J,\,B\in J
\]
\end{lema}
\dem
Es claro que si las \(\sigma\) álgebra son independientes entonces los \(\pi\) sistemas son independientes. Por lo que sólo es necesario demostrar que si los \(\pi\) sistemas son independientes entonces las \(\sigma\) álgebras lo son, entonces dadas \(A\in I\) y \(B\in J\) fijos, definimos las siguientes medidas finitas
\[
    C\mapsto\prob(A\cap C)\quad C\mapsto\prob(B)\prob(C).
\]
\noindent Como ambas medidascoinciden en \(I\) y \(J\), por el lema de clases monótonas coincide en \(\sigma(I)=\mathrm{G}\) y \(\sigma(J)=\mathrm{H}\) y por lo tanto el lema es válido.
\qed
\section{Ejemplos ilustrativos}
\label{sec:org4c448eb}
\exe El siguiente problema se encuentra por primera vez en el libro de P. R. Montmort titulado \emph{Essai d'Analyse sur les jeux de Hasard}, publicado en 1708: Se tienen dos urnas con \(n\) bolitas, cada una numerada de 1 a \(n\). Se va sacando simultaneamente una bolita de cada urna y se quiere hallar la probabilidad de que, al terminar la extracción de todas las bolitas, se haya extraído, por lo menos una vez el mismo número de bolita de cada urna.

\noindent El espacio muestral \(\om\) es el espacio de las matrices de \(2\times n\) de la forma
\begin{equation}
\begin{pmatrix}
a_1 a_2\dots a_n\\
b_1 b_2\dots b_n
\end{pmatrix}
\quad \{a_i,b_j\}\subset\{1,\dots,n\}\,a_k\neq a_l,\,k\neq l,\,b_k\neq b_l,\,k\neq l.
\end{equation}
\noindent Donde las \(a_i\) representan los números de las bolitas sacadas de la primera urna y las \(b_j\) las bolitas de la segunda urna. El número total de elementos de \(\om\) claramente es \((n!)^2\). Sea \(A_i\) el conjunto de elementos de \(\om\) donde el número \(i\in\{1,\dots,n\}\) coincide con el \(j=i\) de la segunda sin importar la ubicación. Por lo tanto queremos calcular \(\prob(A_1\cup A_2\cup\dots\cup A_n)\). Por lo tanto podemos utilizar la ecuación generalizada de la probabilidad de la unión.
\begin{itemize}
\item Primero calculemos la probabilidad de los eventos individuales \(A_i\). Supongamos que fijamos el lugar donde coinciden, entonces en los demás \(n-1\) son irrelevantes, es dcir cualquier combinación en los otros \(n-1\) lugares es válida, entonces tenemos entonces \(((n-1)!)^2\) combinaciones por cada lugar donde coinciden, al tener n de éstos y \((n!)^2\) posibilidades totales, tenemos
\[
  \prob(A_i)=\frac{n((n-1)!)^2}{(n!)^2}=\frac{1}{n},\,\,\sum_{i=1}^n \prob(A_i)=1
  \]
\item Ahora calculamos las probabilidades de las intersecciones de dos eventos \(\prob(A_i\cap A_j)\).En este caso tenemos coincidencia en dos puntos y en el resto \(n-2\) son arbitrarias, similarmente al caso anterior tenemos entonces \(n(n-1)\) combinaciones de lugares donde pueden coincidir los dos números de bolitas y \(((n-2)!)^2\) de posibilidades en los demás, entonces 
\[
  \prob(A_i\cap A_j)=\frac{n(n-1)((n-2)!)^2}{(n!)^2},\,\,\sum_{i<j}^n \prob(A_i\cap A_j)={n\choose2}\frac{1}{n(n-1)}=\frac{1}{2}
  \]
\item Siguiendo la misma lógica 
\[
  \prob(A_i\cap A_j\cap A_k)=\frac{n(n-1)(n-2)((n-3)!)^2}{(n!)^2}=\frac{1}{n(n-1)(n-2)},
  \]
entonces
\[
  \sum_{i,j,k}\prob(A_i\cap A_j\cap A_k)={n\choose3}\frac{1}{n(n-1)(n-3)}=\frac{1}{3!}.
  \]
\begin{itemize}
\item Es decir que podemos generalizar el proceso y concluir por inducción que
\[
  \sum_{i_1,i_2,\dots,i_k}\prob(A_{i_1}\cap A_{i_2}\cap\dots\cap A_{i_k})={n\choose k}\frac{1}{n(n-1)\dots(n-k)}=\frac{1}{k!}.
  \]
\end{itemize}
\end{itemize}
\noindent Por lo tanto podemos concluir por la fórmula de la probabilidad que
\[
  \prob(A_1\cup A_2\cup\dots\cup A_n)=\sum_{k=1}^n (-1)^{k-1}\frac{1}{k!}.
\]
Por último notamos que si \(n\rightarrow\infty\), esta probabilidad tiende a \(1-e^{-1}\)

\eje El ejemplo anterior se aplica a casos como el siguiente: Si dos jugadores tienen una baraja de 52 cartas y van sacando una carta a la vez simultaneamente ¿Cuál es la probabilidad de que al terminar saquen la misma carta a la vez?

\exe Cumpleaños compartidos: En una fiesta con con \(k\) personas ¿Cuál es la probabilidad de que dos de ellas compartan cumpleaños? 

\exe El esquema de contagio de Polya: Supóngase que se tiene una urna con \(n\) bolitas negras y \(r\) bolitas rojas. Se saca una bolita y se regresa a la urna donde se sacó con una cantidad fija \(c\) de bolitas del mismo color. Se desea hallar la probabilidad de que en \(N=k_1+k_2\) estracciones hayan salido \(k_1\) bolitas negras y \(k_2\) bolitas rojas.

\noindent Consideremos la probabilidad de que las bolitas negras salgan justo en las primeras \(k_1\) bolitas al sacarlas. La probabilidad de que las primera boliata sea negra es
\[
\frac{n}{n+r}
\]
\section{Distribuciones y variables aleatorias discretas}
\label{sec:orgd96cf0f}

\noindent Asociadas a los probabilidades están las distribuciones y las variables aleatorias que éstas definen, una distribución es el nombre que se le da a las funciones que definen una probabilidad y se utilizan en la práctica como sinónimos de las probabilidades. Aunadas a las distribuciones estan las \emph{variables aleatorias} que abreviamos como \textbf{v.a}, las cuales son la fenomenología probabilística de las distribuciones, es decir, los eventos aleatorios los describiremos por medio de las variables aleatorias. Dentro de la práctica estadística tienen un valor importante las disribuciones de probabilidad \emph{discretas} o \emph{finitas}, a pesar de que se pueden consideara como un caso particular del total de distribuciones, conviente estudiarlas en particular ya que muchas veces en la práctica estadística, éstas son las más frecuentes. 

\begin{def.}
Una variable aleatoria es una función \(X:\om\rightarrow\re\), donde queremos que los conjunos \(X^{-1}(B)\in\sig\) para todo \(B\subset\re\) boreliano. Si \(X\) toma a lo más una cantidad a lo más numerable de valores, diremos que X es una variable aleatoria \emph{discreta}
\end{def.}

Así, con esta definición el lema anterior (lema \ref{sigma_ind}) tiene el siguiente corolario.

\begin{cor}
Ahora si $X$ e $Y$ son variables aleatorias en un espacio de probabilidad $(\om,\sig,\prob)$ tales que para todo $\{x,y\}\subset\re$ se cumple
\[
    \prob(\{X\leq x\}\cap\{Y\leq y\}):=\prob(X\leq x\,,\, Y\leq y)=\prob(X\leq x)\prob(Y\leq y)
\]
entonces $X$ e $Y$ son variables aleatorias independientes
\end{cor}
\dem
Esto es claro de la proposición anterior debido a que la familia de conjuntos \(\{X^{-1}(-\infty,x]\,\vert\,x\in\re\}\) es un \(\pi\) sistema para \(\sigma(X)\) para toda \textbf{v.a} \(X\), así del lema anterior se sigue el resultado.
\qed\\

\obs Toda probabilidad en un espacio discreto, es discreta.
\begin{def.}
Una distribucion de probabilidad discreta definida por la variable alearotia \(X\) se escribe como la suma
\[
    \prob(X\in E)=\sum_{x\in A\cap E}\prob(X=x)\quad E\in\sig
\]
\end{def.}
Donde \(A\) es un conjunto numerable que cumple que \(\prob(X\in A)=1\)

\eje La escalera del diablo es una funcion continua que solo toma un numero numerable de valores fuera de un conjunto de medida cero.

\begin{def.}
Para una disribucion de probabilidad dicreta asociada a una variable aleatoria \(X\) se define la \emph{funcion de masa probabilistica} o la \emph{funcion de densidad discreta} como
\[
        f_{X}(x)=\prob(X=x)\quad\sum_{x\in\om}f_{X}=1
\]
De forma similar se define la \emph{funcion cumulativa de probabilidad} como
\[
        F(x)=\prob(X\leq x)=\sum_{\omega\leq x}f_{X}(\omega)
\]
\end{def.}

\exe La distribucion binomial con parametros \(n\in\nat\), \(p\in[0,1]\) se define por medio de la funcion de masa
\[
    \prob(k,n,p)={n\choose k}p^{k}(1-p)^{n-k}\quad{n\choose k}=\frac{n!}{k!(n-k)!}
\]
Esta probabilidad mide \(k\) outcomes deseados de \(n\) eventos independientes
\exe \textbf{Bolados} Si se lanza una moneda un número finito de veces \(n\), una variable aleatoria definida por esa serie de eventos es contar el número de veces que cae en sol
\[
    X(n)=|\{\text{veces que cae en sol}\}\in\{0,\dots,n\}|
\]
\noindent Así por ejemplo al lanzar una moneda tres veces con una moneda \emph{justa} (con probabilidades de \(1/2\) por cara) tiene la siguiente tabla de probabilidades

\begin{center}
\begin{tabular}{llllr}
\(X\) & 0 & 1 & 2 & 3\\
\hline
\(f_X\) & \(1/8\) & \(3/8\) & \(3/8\) & \(1/8\)\\
\(F\) & \(1/8\) & \(1/2\) & \(7/8\) & 1\\
\end{tabular}
\end{center}

La ventaja de las variables aleatorias discretas es que el calculo de sus \emph{valores esperados} o \emph{promedios} y sus \emph{variazas} son mucho más fáciles que en los casos continuos.

\begin{def.}[Valor esperado discreto] 
Dado un espacio de probabilidad \((\om,\sig,\prob)\) una variable aleatoria discreta \(X\), definimos el valor esperado de \(X\) como
\[
    \mu=\esp(X)=\sum_{x\in X(\om)} x\prob(X=x).
\]
\end{def.}

\begin{def.}[Valor esperado de una función] 
Dado un espacio de probabilidad \((\om,\sig,\prob)\) una variable aleatoria discreta \(X\) \(g:\re\rightarrow\re\), definimos el valor esperado de \(g(X)\) como
\[
    \mu=\esp(X)=\sum_{x\in X(\om)} g(x)\prob(X=x).
\]
\end{def.}

\obs Si pensamos a \((\om,\sig,\prob)\) como un espacio de medida finita y discreto, la esperanza no es más que la integral de una función medible real, esto lleva a que todo lo que se define a partir de ahorita se pueda generalizar al caso continuo simplemente intercambiando sumas por integrales. 

\textbf{Algunas propiedades de la esperanza:}

\begin{enumerate}
\item Si \(X\),\(Y\) son variables aleatorias y \(a\), \(b\) son números reales, entonces
\[
   \esp(a\,X+b\,Y)=a\,\esp(X)+b\,\esp(Y).
   \]
\noindent En particular
\[
   \esp(a\,X+b)=a\,\esp(X)+b.
   \]
\item Si \(X\geq Y\),entonces
\[
   \esp(X)\geq\esp(Y).
   \]
\item Si denotamos por 1\textsubscript{A} a la función generadora de \(A\in\sig\), entonces
\[
   \esp(1_A)=\prob(A)
   \]
\end{enumerate}

\begin{def.}[Variaza discreta]
Dado un espacio de probabilidad \((\om,\sig,\prob)\) una variable aleatoria discreta \(X\), definimos su varianza o variación a su valor esperado como
\[
    \sigma^2=\mathrm{Var}(X)=\esp((X-\mu)^2)
\]
\end{def.}

\begin{def.}[Desviación Estandard]
Dado un espacio de probabilidad \((\om,\sig,\prob)\) una variable aleatoria discreta \(X\), definimos su \emph{desviación estandard} como
\[
    \sigma(X)=\sqrt{\mathrm{Var}(X)}
\]
\end{def.}

\obs \(\sigma^2=\mathrm{Var}(X)=\esp(X^2)-\mu^2\)
\exe Supongase que se tiene una moneda cargada de tal forma que la probabilidad de obtener sol sea \(0.6\) en lugar de \(0.5\). Si tiramos dos veces esta moneda queremos saber cual es el valor esperado y variaza a la variable aleatoria definida por el númro de veces que la moneda cae en sol. Entonces podemos calcular la probabilidad de que la moneda caiga en sol siguiendo la siguiente tabla
\begin{center}
\begin{tabular}{llll}
AA & AS & SA & SS\\
\hline
\(0.4*0.4\) & \(0.4*0.6\) & \(0.6*0.4\) & \(0.6*0.6\)\\
\end{tabular}
\end{center}
Lo que nos da la siguiente tabla
\begin{center}
\begin{tabular}{lrrr}
X & 0 & 1 & 2\\
\hline
\textbf{P} & 0.16 & 0.48 & 0.36\\
\end{tabular}
\end{center}
Entonces el valor esperado y varianza son
\begin{align*}
    \mu=\esp(X)= 0*0.16+1*0.48+2*0.36=1.2\\
    \sigma^2=\mathrm{Var}(X)=(0-1.2)^2*0.16+(1-1.2)^2*0.48+(2-1.2)^2*0.36&=1.92
\end{align*}

\exe \textbf{La distribuciónde Bernouli} Sirve principalmente para modear un sólo evento donde la probabilidad del evento se denota por \(p\). Así se define la distribución como
\[
    P(X=x)=p^{x}(1-p)^{1-x}\quad x\in\{0,1\}.
\]
\noindent Donde los valores \(0,1\) son los valores de \emph{fracaso} o \emph{éxito}, es decir esta probabilidad mide eventos que tienen una probabilidad de pasar o no pasar.
\eje Demuestre que
\[
    \mu=p\quad\sigma^2=p(1-p).
\]
\textbf{Hint: Vea la combinatoria de los eventos exitosos}


\exe La distribución binomial se puede pensar como la distribución de probabilidad de tener \emph{(n}) eventos de bernoulli independientes que sean exitosos

\exe \textbf{La distribución hipergeométrica:} Se aplica cuando tenemos sucesiones de eventos que no son independientes, en contraste a la distribucion binomial. Intuivamente suponga que tenemos un sample de \(n\) objetos y escoger \(a\) de estos se consideran exitosos y \(N-a\) son fracasos, donde \(N \geq n\) (por ejemplo cuando se escoge quieres hacen servicio militar) así la distribucion se define como
\begin{align*}
\prob(X=x)=\frac{{a\choose x}{N-a\choose n-x}}{{N\choose n}}\\
x\in\{\max(0,n+a-N),\dots\min(a,n)\}
\end{align*}
Posteriormente veremos con más detalle las distribuciones de probabilidad definidas previamente.

\begin{teorema}[Desigualdad de Markov]
Sea $Z$ una variable aleatoria no negativa y sea $a>0$, entonces
\[
\prob(Z\geq a)\leq\frac{\esp[Z]}{a}
\]
\end{teorema}
\dem Claramente se cumple la siguiente desigualdad
\[
Z\geq a\,1_{Z\geq a},
\]
\noindent así, por las propiedades de las esperanza tenemos

\[
\esp[Z]\geq a\esp[\,1_{Z\geq a}]=a\,\prob(Z\geq a).
\]
\qed
\begin{cor}[Desigualdad de Chebyshev]
Sea $X$ una variable aleatoria con esperanza $\mu$, si se aplica la desigualdad de Markov a $Z=(X-\mu)^2$ y se toma $a=\epsilon^2>0$,entonces
\[
\prob(|X-\mu|\geq \epssilon)\leq \frac{\sigma^2(X)}{\epsilon^2}
\]
\end{cor}
\section{Productos de variables e independencia}
\label{sec:org71c1d10}
\noindent Inicialmente definimos independencia en términos de eventos y probabilidades, sin embargo también es posible definir estos conceptos en términos de variables aleatorias y esperanzas. De cierta manera es más natural el pensar la independencia en estos términos que en en términos de eventos. 

\begin{def.}[Variables aleatorias independientes.]
Decimos que las variables aleatorias \(\{X_i\}_{i\in I}\) en un espacio de probabilidad \((\om,\sig,\prob)\) son \emph{independientes} si \[\{\sigma(X_i)\}_{i\in I}\] son independientes. Equivalentemente, dos variables aleatorias \(X\) e \(Y\) son independientes si 
\[
\esp[XY]=\esp[X]\esp[Y].
\]
Se denotará la relación de independencia entre \(X\) e \(Y\) como \(X\ind Y\) y no distinguiremos entre variables aleatorias y \(\sigma\) álgebras, e incluso denotaremos \(X\ind\mathrm{F}\) cuando una variable aleatoria sea indepeniente de una sub \(\sigma\) álgebra.
\end{def.}

\obs Si la distribución de probabilidad de \(X\) es \(f_X\) y la distribución de probabilidad de \(Y\) y \(f_{XY}\), entonces dos v.a son independientes si y sólo si
\[
f_{XY}=f_X\,f_Y
\]

\eje Demueste que las dos definiciones de variables aleatorias independientes son en efecto \textbf{equivalentes}.

Entonces la independencia se describe en términos del producto de varaibles aleatorias, sin embargo, existen otras fromas de describir y \emph{medir} la independencia de variables aleatorias. Una de estas involucra la definición de una nueva operación entre variables aleatorias, la cual relaciona la suma de las variacias de dos variables aleatorias y la varianza de las sumas

\begin{def.}[Covarianza]
Dadas dos variables aleatorias \(X\) e \(Y\) con valores esperados finitos \(\mu_X\) y \(\mu_Y\) respectivamente, se define la covariaza como
\[
\mathrm{Cov}(X,Y)=\esp[XY]-\esp[X]\esp[Y]=\esp[(X-\mu_X)(Y-\mu_Y)].
\]
\end{def.}
\exe Demuestre la igualdad entre las dos expresiones de \(\mathrm{Cov}(X,Y)\)
\noindent Claramente la covarianza es una forma de medir que tanto se alejan dos variables aleatorias de ser independientes, es decir
\[
X\ind Y\text{ si y sólo si }\mathrm{Cov}(X,Y)=0.
\]
\begin{teorema}
Sean $X$ e $Y$ variables aleatorias, entonces
\begin{equation}
\mathrm{Var}(X+Y)=\mathrm{Var}(X)+\mathrm{Var}(Y)-2\mathrm{Cov}(X,Y).
\end{equation}
En particular, $X$ e $Y$ son independientes si y sólo si 
\begin{equation}
\mathrm{Var}(X+Y)=\mathrm{Var}(X)+\mathrm{Var}(Y).
\end{equation}
\end{teorema}
\dem Por definición de la varianza, si \(\mu_X\) y \(\mu_Y\) son los valores esperados de \(X\) e \(Y\) y \(\mu_{X+Y}\) es el valor esperado de \(X+Y\), entonces \(\mu_{X+Y}=\mu_X+\mu_y\) y por lo tanto
\begin{align*}
&\mathrm{Var}(X+Y)=\esp[(X+Y-\mu_{X+Y})^2]=\esp[(X+Y-(\mu_{X}+\mu_{Y}))^2]\\
&=\esp[((X-\mu_{X})+(Y-\mu_{Y}))^2]=\esp[(X-\mu_X)^2-2(X-\mu_X)(Y-\mu_Y)+(Y-\mu_Y)^2]\\
&=\esp[(X-\mu_X)]^2+\esp[(Y-\mu_Y)^2]-2\esp[(X-\mu_X)(Y-\mu_Y)]\\
&=\mathrm{Var}(X)+\mathrm{Var}(Y)-2\mathrm{Cov}(X,Y).
\end{align*}
Por lo tanto es claro que si \(X\ind Y\), entonces \(\mathrm{Cov}(X,Y)=0\) y por lo tanto
\[
\mathrm{Var}(X+Y)=\mathrm{Var}(X)+\mathrm{Var}(Y).
\]
\qed
\exe Suponganse que se tienen dos urnas con cinco bolitas enumerdas de 1 al 5 y se saca una bolita de cada urna ¿Cuál es el valor esperado del produco de los números?

\noindent Usamos \(X\) para la variable de la primera urna e \(Y\) para la segunda, la probabilidad de cada número es \(1/5\), entonces \(\esp[X]=\esp[Y]=1/5(1+2+3+4+5)=3\), como ambas variables son independientes \(\esp[XY]=\esp[X]\esp[Y]=9\).  

\eje ¿Qué sucede si en lugar de en lugar tomar las bolitas de dos urnas, se sacan dos bolitas de la misma urna?
\section{Momentos y la función generadora}
\label{sec:org1397db6}
\noindent Una variable aleatoria \(X\), es heuristicamente una función que se comporta de una forma que no permite predicción de los valores que va a tomar, por esta razón se utilza el nombre adjetivo de \textbf{aleatoria}, sin embargo, sí hay forma de obtener información de \(X\), pricipalmente por medio de su esperanza y varianza, una mide los promedios pesados de los valores posibles y la otra la concentraciones de valores alrededor de dicho promedio. Sin embargo la esparenza y la varianza no son las únicas maneras de extaer información útil de una variable aleatoria, éstas cantidades tienen generalizaciónes a "mayor grado", donde podríamos decir que la esperanza tiene grado 1 y la varianza tiene grado 2. Lo anterior puede que tenga sentido a nivel inituivo, pero podemos precisarlo con mucha más formalidad con la siguiente definición

\begin{def.}[Momentos de una variable aleatoria]
Se llama \emph{momento de orden k} de una variable aleatoria discreta, \(X\) a la cantidad
\[
\alpha_k[X]=\esp[X^k]=\sum_{j\in\nat}x_j^k\prob(X=x_j).
\]
En particular \(\alpha_1=\esp[X]=\mu\). En este sentido se definen los momentos centrados como
\[
\mu_k=\esp[(X-\alpha_1)^k]=\esp[(X-\mu)^k]=\esp[(X-\esp[X])^k].
\]
\end{def.}

\begin{def.}[Función generadora de momentos]
Sea \(X\) una variable aleatoria discreta con valores en \(\{x_0,x_1,x_2,\dots,\}\subset\re\). Se define la \emph{función generadora demomentos} de \(X\) como
\[
M_X(t)=\esp[e^{tX}].
\]
Más explicitamente
\[
M_X(t)=\sum_{n=0}^{\infty}e^{tx_n}\prob(X=x_n).
\]
\end{def.}

\noindent Cuando la función \(M_X(t)\) existe para valores cerca de \(t=0\), entonces es diferenciable y más aun analítica. Ahora, la razón por la cual se le llama \emph{función generadora de momentos} es porque \textbf{es posible obtener todos los momentos de una variable aleatoria por medio de las derivadas de \(M_X(t)\)}. Es por esto que hay una equivalencia entre la distribución de una variable aleatoria y su función generadora de momentos ¿En qué sentido es esta equivalencia? Si \(X,Y\) son variables aletorias, entonces son \emph{identicamente distribuidas} si sus funciones de distribución son iguales, resulta que bajo ciertas condiciones sobre las distribuciones de \(X\) e \(Y\) se puede asegurar que \textbf{si sus funciones generadoras de momentos son iguales (o equivalentemente sus momentos son iguales), entonces son identicamente distribuidas}.
\begin{teorema}
Si $X$ tiene una función generadora de momentos $M_X(t)$ definida en una vecindad del cero entonces
\[
\esp[X^n]=M_X^{(n)}(0)=\dfrac{d^n}{dt^n}M_X(t)\vert_{t=0}.
\]
\end{teorema}
\section{Correlacion y regresión lineal}
\label{sec:org53b15fd}
\noindent Como se vió, la covarianza es una medida de qué tatnto se alejan dos variables aleatorias de ser independientes. Sin embargo ésta medida tiene el detalle de que no es independiente de las unidades de medida, esto es debido a que
\[
\mathrm{Cov}(aX,bY)=ab\,\mathrm{Cov}(X,Y)\quad\forall\,\{a,b\}\subset\re.
\]
\noindent Por lo tanto queremos definir una medida que sea invariante bajo cambios de unidades
\begin{def.}[Coeficiente de correlación de Pearson]
Se define el \emph{coeficiente de correlación} entre dos variables aleatories \(X\) e \(Y\) con desviación estandard \(\sigma(X)\) e \(\sigma(Y)\) como
\[
\rho(X,Y)=\frac{\mathrm{Cov}(X,Y)}{\sigma(X)\sigma(Y)}.
\]
\end{def.}
\noindent Claramente \(\rho(aX,bY)=\rho(X,Y)\). Ahora se demostrará que \(-1\leq\rho\leq1\). Primero para \(\{\alpha,\beta\}\subset\re\), consideramos
\begin{equation}
\esp[(\alpha(X-\mu_X))+\beta(Y-\mu_Y))^2]=\alpha^2\sigma^2(X)+\beta^2\sigma^2(Y)-2\alpha\beta\,\mathrm{Cov(X,Y)}\geq 0
\end{equation}
\noindent Pues la esperanza de algo positivo es positivo. Como la ecuación anterior es válida para toda \(\alpha,\beta\in\re\), en particular si tomamos \(\alpha=\sigma(Y)\) y \(\beta=\sigma(X)\), al agrupar y elimira la constante \(2\) obtenemos
\[
\sigma^2(X)\sigma^2(Y)-\sigma(X)\sigma(Y)\mathrm{Cov}(X,Y)\geq0.
\]
\noindent Por lo tanto \(1\geq\rho(X,Y)\), similarmente se elije \(\alpha=-\sigma(Y)\) y \(\beta=-\sigma(X)\) para obtener \(\rho(X,Y)\geq-1\), por lo tanto \(-1\leq\rho(X,Y)\leq1\).

\obs En la discusión anterior, es evidente que si \(\rho(X,Y)=\pm1\), entonces exiten \(\{\alpha_0,\beta_0\}\subset\re\setminus\{0\}\) tales que
\[
\esp[(\alpha_0(X-\mu_x)+\beta_0(Y-\mu_Y))^2]=0\hspace{0.2cm}\implies\hspace{0.2cm}\alpha_0(X-\mu_x)+\beta_0(Y-\mu_Y)=0,
\]
\noindent entonces los únicos puntos \((x,y)\in\re^2\) con probabilidades \(\prob(X=x)\) y \(\prob(Y=y)\) no nulas, cumplen la ecuación
\[
\alpha_0(x-\mu_X)+\beta_0(y-\mu_Y)=0.
\]
\noindent Es decir que los datos de \(X\) e \(Y\) son colineales, entonces los datos de \(X\) e \(Y\) se encuentran en una recta. Entonces el coeficiente de correlación no sólo mide la independencia de las dos variables \(X,Y\) (\(\rho\)(X,Y)=0 implica que \(X\ind Y\)), si no que también mide que tanto se encuentran linealmente relacionadas, entre más cerca se encuentren de \(1\), más linealmente correlacionadas se encuentran.\vspace{0.5cm}
\noindent \textbf{\Large Regresión lineal}
\vspace{0.5cm}
\chapter{Explorando distribuciones a mayor profundidad}
\label{sec:orgf1f39fe}

\section{Distribución binomial y la ley de lo grandes números}
\label{sec:org673d912}
\noindent En 1713 se publica el famoso libro de Jacob Bernoulli \emph{Ars Conjectandi}. En este libro Bernoulli hace una de las primeras referencias a lo que hoy conocemos como inferencia en el sentido estadístico. Además Bernoulli introduce el modelo probabilístico de las "pruebas repetidas" o "pruebas de Bernoulli", las cuales miden la probabilidad de repetidos eventos independientes con una distribuición de tipo Bernoulli, esto da a lugar a la ditribución que se estudia en esta sección, la \emph{distribución binomial}.
Un ejemplo clásico de esto son las probabilidades de bolados repetidos, donde la probabilidad es una Bernoulli con \(p=1/2\) para una moneda justa y \(p\in[0,1]\setminus\{1/2\}\) para una moneda cargada. Otro ejemplos claros son los de dados de \(4,6,12,20\) etc. caras (tanto justo como cargados).

Otro ejemplo: Supónganse que se van sacando bolitas de una urna con \(n\) bolitas negras y \(m\) blancas,entonces la probabilidad de sacar una bolita negra es \(p=n/(n+m)\) y la de sacar una bolita blanca es \(q=1-p=m/(n+m)\). Si \(k\) personas van sacando una bolita a la vez de forma aleatoria de tal forma que cuando toman la bolita, la regresan a la bolsa de nuevo, entonces queremos la probabilidad de que esas \(k\) personas hayan sacado \(r\) bolitas negras y \(k-r\) bolitas blancas.
Claramente los eventos de sacar las bolitas son independientes porque las bolitas se regresan cada que se saca una por lo tanto si importara el orden la probablidad sería \(p^r(1-p)^{k-r}\), pero como no importa el orden la combinatoria nos dice entonces que
\[
\prob(N=r,B=k-r)=\mathrm{Bin}(r;k,p)={k\choose r}p^r(1-p)^{k-r}.
\]
\noindent Donde \(N\) representa la variable aleatoria del número de bolitas negras y \(B\) las de bolitas blancas. 

Si nos fijamos, claramente esta función es parte del binomio
\[
1=1^k=(p+(1-p))^k=\sum_{r=0}^k{k\choose r}p^r(1-p)^{k-r}.
\]
\noindent Esto muestra que la distribución binomial es claramente una distribución discreta de probabilidad, donde su función cumulativa de probabilidad es
\[
F(k)=\sum_{r\leq k}\mathrm{Bin}(r;k,p)=\sum_{r\leq r}{k\choose r}p^r(1-p)^{k-r}.
\]
Todos los ejemplos anteriores siguen el mismo patrón de medir probabilidades de series de eventos de la forma
\[
EEEFEF\dots FFEE.
\]
\noindent Donde los \(E\) representan el "éxito" de un evento con probabilidad \(p\) y \(F\) representan los fracasos con probabilidad \(1-p\).

\eje Demuestre que el cosiente de las funciones binomiales
\[
\frac{\mathrm{Bin}(r;k,p)}{\mathrm{Bin}(r-1;k,p)}=\frac{(k-r+1)p-r}{r(p-1)}=1+\frac{(n+1)p-r}{r(1-p)}.
\]
\noindent Está acotado.

\noindent \textbf{\Large Momentos de la distribución binomial}\\
Recordemos la \textbf{esperanza y varianza de Bernoulli}, a partir de ésta deducir la esperanza y varianza de una distribución binomial.
\section{Distribución de Poisson}
\label{sec:org39abb14}
Resulta dificil calcular los valores binomiales \(\textrm{Bin}(k;n,p)\) cuando \(k\) y \(n\) son bastante grandes, por lo que es mejor en el sentido computacional hacer aproximaciones relevantes.

Para esto veamos qué sucede si el valor de \(p\) tiende a cero al mismo tiempo que \(n\) tiende a infinito en una distribución binomial. Además queremos que la esperanza \(np\) se mantenga constante \(np=\lambda\), esto nos permite calcular los valores asintóticos de \(\textrm{Bin}(k;n,p)\).
\begin{align*}
\textrm{Bin}(k;n,p)&={k\choose n}p^k(1-p)^{n-k}\\
&={k\choose n}\Big(\frac{\lambda}{n}\Big)^k\Big(1-\frac{\lambda}{n}\Big)^{n-k}\\
&=\frac{n!}{k!(n-k)!}\Big(\frac{\lambda}{n}\Big)^k\Big(1-\frac{\lambda}{n}\Big)^{n-k}\\
&= \Big(\frac{\lambda^k}{k!}\Big)\frac{n!}{[n(1-\lambda/n)]^k(n-k)!}\Big(1-\frac{\lambda}{n}\Big)^{n}\\
&=\Big(\frac{\lambda^k}{k!}\Big)\frac{(1-1/n)(1-2/n)\dots(1-k/n)}{(1-\lambda/n)^k}\Big(1-\frac{\lambda}{n}\Big)^{n}.\\
\end{align*}
\noindent Así si \(n\rightarrow\infty\) y dejamos a \(k\) fijo, entonces
\[
\lim_{n\rightarrow\infty}\textrm{Bin}(n;k,p)=\frac{\lambda^k}{k!}e^{-\lambda},
\]
esto es claro del hecho de que \((1+x/n)^n\rightarrow e^x\) cuando \(n\rightarrow\infty\).

Esto nos define una nueva distribución de probabilidad, conocida como la \emph{distribución de Poisson} con parámetro \(\lambda\). Esta distribución fue definida por el matemático Siméon Denis Poisson en su trabajo \emph{Recherches sur la probabilité des jugements en matière criminelle et  en matiére civile.} En donde teoriza sobre el número de convicciones erroneas utilizando variables aleatorias que miden la probabilidad de cierto número de "llegadas" en un intervalo de tiempo de longitud dada. En generál la distribución de Poisson modela eventos donde se espera cierto número ocurrencias por unidad de tiempo como llegadas a un banco o tienda.

\begin{def.}[Variable aletoria de Poisson]
Decimos que una variable aletoria \(X\) con valores en \(\nat\) se encuentra distribuida por una distribución de Poisson de parámetro \(\lambda\), denotada por \(Poisson(\lambda)\) si
\[
        \prob(X=r|\lambda)=\frac{e^{-\lambda}\lambda^r}{r!},\quad r\in\nat.
\]
\end{def.}
Veamos primero que en efecto la distribución de Poisson es una distribución de probabilidad, es decir hay que verificar que
\[
\sum_{n=0}^{\infty}\prob(X=n|\lambda)=1.
\]
Esto es sencillo de demostrar si recordamos la definición de la serie de Taylor de la exponencial

\begin{align*}
\sum_{n=0}^{\infty}\prob(X=n|\lambda)&=\sum_{n=0}^{\infty}\frac{\lambda^n e^{-\lambda}}{n!}\\
&=e^{-\lambda}\Big(\sum_{n=0}^{\infty}\frac{\lambda^n}{n!}\Big)=e^{-\lambda}e^{\lambda}=1.
\end{align*}

Similarmete podemos calcular la esperanza de una variable aleatoria de Poisson es sencillo

\begin{align*}
\mathbb{E}[X]&=\sum_{n=0}^{\infty}n\prob(X=n|\lambda)\\
&=\sum_{n=0}^{\infty}\frac{n\lambda^n e^{-\lambda}}{n!}\\
&=e^{-\lambda}\Big(\sum_{n=1}^{\infty}\frac{\lambda^n}{(n-1)!}\Big)\\
&=\lambda e^{-\lambda}\Big(\sum_{n=1}^{\infty}\frac{\lambda^{n-1}}{(n-1)!}\Big)=\lambda e^{-\lambda}e^{\lambda}=\lambda.
\end{align*}
\section{Distribuciones continuas}
\label{sec:orgbad89b1}

\noindent Consideremos \((\om,\sig,\prob)\) un espacio de probabilidad y sea \((E,S)\) un espacio medible y \(\xi:\om\rightarrow E\) una función medible, a esta función le llamremos \emph{elemento aleatorio} así existe una función \(\xi^{*}:S\rightarrow\sig\), definida como \(\xi^{*}(B)=\xi^{-1}(B)\), entonces es posible definir una medida de probabilidad en \((\xi^{*})^{-1}(\sig)\), definida por \(\prob\circ\xi^{*}(B)=\prob(\xi^{*}(B))\), a la medida \(\prob\circ\xi^{*}\), le llamaremos una \emph{distribución} de probabilidad y usualmente no distinguiremos entre una medida de probabilidad y una distribución incluso en la absencia de un elemento aleatorio.
\obs Cuando \(E=\re\) y \(S=B_{\re}\), un elemento aleatorio es una variable aleatoria y cuando \(E=\re^{d}\) y \(S=B_{\re^{d}}\) un elemento aleatorio se le conoce como \emph{vector aleatorio}.
\begin{def.}{Función de una distribución}
Dado un espacio de probabilidad \((\om,\sig,\prob)\) y un elemento medible \(\xi:\om\rightarrow E\), definimos la \textit{función} de distribución como la función \(F_{\xi}:\re\rightarrow[0,1]\)
\[
    F_{\xi}(\alpha)=\prob\circ\xi^{*}(-\infty,\alpha]=\prob(\xi\leq\alpha)
\]
\end{def.}
\noindent \textbf{Algunas propidades:} Supongamos que \(F\) es la función de distribución asociada a una variable aleatoria \(X\), entonces.
\begin{enumerate}
\item \(F:\re\rightarrow[0,1]\) es no decreciente, es decir \(F(x)\leq F(y)\) siempre que \(x\leq y\).
\item \(\lim_{x\rightarrow\infty}F(x)=1\) y \(\lim_{x\rightarrow -\infty}F(x)=0\).
\item \(F\) es continua por la derecha.
\end{enumerate}
\dem La demostración de estos hechos es evidente, el único que no es inmediatemente obvio es el punto número tres, para esto notamos que si tenemos la susesión \(\{x+1/n\}\), entonces por convergencia monónona tenemos que
\[
    \lim_{n\rightarrow\infty}\prob\Big(X\leq x+\frac{1}{n}\Big)=\prob\Big(X\leq x\Big)
\]
\noindent \textbf{Existencia:} Ahora supongamos que \(F\) es una función que cumple las propiedades 1.,2. y 3., entonces similar a la existencia de Lebesgue, es posible construir una medida \(\nu:\mathbb{B}_{\re}\rightarrow\re^{+}\) tal que \(\nu((-\infty,x])=F(x)\). A la medida \(\nu\) se le conoce como la medida de Lebesgue-Stieltjes de \(F\).
\eje \textbf{(La representación de Skorokhod de una variable aleatoria con una función de distribución prescrita en el intervalo [0,1])}\\

Supongamos que \(F:\re\rightarrow[0,1]\) cumple con las propiedades 1.,2. y 3., como antes, entonces construimos una variable aleatoria en \(([0,1],\mathbb{B}[0,1],\lambda)\), donde \(\lambda\) es la medida de Lebesgue. Definimos
\begin{align*}
    X^{+}(\omega):=\inf\{\,z\,\vert\,F(z)>\omega\}=\sup\{\,y\,\vert\,F(y)\leq\omega\}\\
    X^{-}(\omega):=\inf\{\,z\,\vert\,F(z)\geq\omega\}=\sup\{\,y\,\vert\,F(y)<\omega\}.
\end{align*}

Observemos que por definición de \(X^{-}\), si \(\omega\leq F(c)\), entonces \(X^{-}(\omega)\leq c\). Ahora, si \(z>X^{-}(\omega)\), entonces \(F(z)\geq\omega\), entonces por continuidad a la derecha de \(F\), \(F(X^{-}(\omega))\geq\omega\) y por lo tanto
\[
     X^{-}(\omega)\leq c \implies \omega\leq F(X^{-}(\omega))\leq F(c)
\]
Por lo tanto tenemos que \(\{\,\omega\,\vert\,X^{-}(\omega)\leq c\}=\{\,\omega\,\vert\,\omega\leq F(c)\}\) lo que implica que
\[
    \prob(X\leq c)=\prob(\omega\leq F(c))=\int_{-\infty}^c F(x)\,dx=F(c)
\]
\qed
\obs \(X^{+}\) tiene la mimsa función de distribución ya que
\[
\prob(X^{-} = X^{+})=1.
\]
\section{Esperanza condicional}
\label{sec:org5805293}
\noindent Asi mismo entenderemos a la \emph{esperanza} de una variable aleatoria como la integral de dicha funcion medible, es decir
\begin{equation}
    \esp(f(z))=
    \begin{cases}
        & \sum_{i} p_i f(z)\\
        & \int_{\Omega}f(z)d\mu
    \end{cases}
\end{equation}
Recordemos que la probabilidad condicional de un evento \(A\) con respecto a otro evento \(B\) como la siguiente fórmula
\[
    \prob(A|B)=\frac{\prob(A\cap B)}{\prob(B)},
\]
\noindent sinembargo comunmente se utiliza el concepto más general de \emph{esperanza condicional}
\begin{def.}
    Si \((\Omega,\Sigma,\prob)\) es un espacio de probabilidad, \(X:\Omega\rightarrow\re\) es una variable aleatoria inegrable y \(\mathrm{T} \subset\Sigma\) es una sigma sub álgebra, entonces decimos que una funcion \(Y:\Omega\rightarrow\re\) es una \emph{esperanza} condicional si cumple lo siguiente.
\begin{itemize}
\item \(Y\) es \(\mathrm{T}\) medible
\item \(\esp(|Y|)<\infty\)
\item \(\esp(X 1_A)=\esp(Y1_A)\) para todo \(A\in\mathrm{T}\)
\end{itemize}
se denotará a la esperanza condicional de \(X\) con respecto a \(\mathrm{T}\) como \(\esp(X|\mathrm{T})\).
\end{def.}
\noindent\dem Podemos demostrar la existencia de la esperanza condicional por medio del teorema de Radon-Nikodym (1930)
\begin{teorema}[Radon-Nikodym]
    Sea $(\Omega,\Sigma)$ un espacio medible y $\mu:\Omega\rightarrow\re$ una medida ($\sigma$) finita y $\nu$ una medida con signo ($\sigma$) finita tal que $\nu$ sea absolutamente continua con respecto a $\mu$, entonces existe $f:\Omega\rightarrow\re$ funcion medible tal que
\begin{equation}
    \nu(A)=\int_{A} f d\mu \hspace{0.5cm}\forall A\in\Sigma,
\end{equation}
además la función $f$ es única casi donde quiera, a esta función se le conoce como *derivada de Radon-Nikodym* y usualmente se denota
\[
    f=\frac{d\nu}{d\mu}[\mu].
\].
\end{teorema}
\noindent Así la existencia de la esperanza condicional proviene de la derivada de Radon-Nikodym de la medida con signo definida por \(X\), para el álgebra \(\mathrm{F}\), es decir
\[
    \nu{\pm}(A)=\int_{A}X^{\pm}d\prob\hspace{0.5cm}\nu=\nu^{+}-\nu^{-},
\]
\noindent la cual es absolutamente continua con respecto a la medida de probabilidad \textbf{P}, entonces definimos \(Y=\frac{d\nu}{d\prob}[\prob]\), así simplemente verificamos que en efecto es una esperanza condicional. Por Radon-Nykodym, \(Y\) es medible, ahora

\begin{itemize}
\item \(\esp(|Y|)=\int_{\om}|Y|d\prob=\int_{\om}|X|d\prob<\infty\)
\item \(\esp(X1_A)=\int_{A}Xd\prob=\nu(A)=\int_{A}Yd\prob=\esp(Y1_A)\).
\end{itemize}

Algunas propiedades de la esperanza condicional son las siguientes:

\begin{itemize}
\item Linealidad: para todas \(a,b\in\re\) y \(X,Y\) \textbf{v.a} \(\esp(aX+bY|\mathrm{F})=a\esp(X|\mathrm{F})+b\esp(Y|\mathrm{F})\).
\item Monotonía: si \(X\geq0\), entonces \(\esp(Y|\mathrm{F})\geq 0\).
\item Si \(X\) es \(\mathrm{F}\) medible, entonces \(\esp(X|\mathrm{F})=X\).
\item Propiedad de Torre: Si \(\mathrm{G}\subset\mathrm{F}\subset\sig\), entonces:
\[
        \esp(\esp(X|\mathrm{F})|\mathrm{G})=\esp(\esp(X|\mathrm{G})|\mathrm{F})=\esp(X|\mathrm{G})
  \]
\end{itemize}

Más aún la esperanza condicional hereda las propiedades que son resultados comunes de la teoría de la integración, por ejemplo
\begin{itemize}
\item Convergencia monótona: Si \(\{X_n\}_{n\in\nat}\) es una sucesión de \textbf{v.as} no negativas que converge monótonamente \(X_N\uparrow X\), entonces \(\esp(X|\mathrm{F})=\lim\esp(X_n|\mathrm{F})\).
\item Convergencia dominada: Si \(\{X_n\}_{n\in\nat}\) es una sucesión de \textbf{v.as} convergente \(X_n\rightarrow X\), tal que las \textbf{v.as} sean integrables, no negativas y se encuentren dominadas por \(|X_n|\leq Y\) tal que \(\esp(Y|\mathrm{F})<\infty\),  entonces \(\esp(X|\mathrm{F})=\lim\esp(X_n|\mathrm{F})\).
\item Desigualdad de Jensen: Si \(f:\re\rightarrow\re\) es una función convexa (cóncava) y \(X\) es una variable aleatoria, entonces \(\esp(f(X)|\mathrm{F})\geq\esp(X|\mathrm{F})\) (resp \(\esp(f(X)|\mathrm{F})\leq\esp(X|\mathrm{F})\))
\end{itemize}

\obs Para la esperanza conicional, si \(X \ind\mathrm{F}\), entonces \(\esp(X|\mathrm{F})=\esp(X)\), además \(Y=\esp(X)\) es \(\mathrm{F}\) medible pues
\[
    \forall A\in\mathrm{F}\hspace{0.3cm}\esp(X1_A)=\esp(X)\prob(A)=Y\esp(1_A)=\esp(Y1_A)
\]
\begin{prop}
Sea $(\om,\sig,\prob)$ espacio de probabilidad, $\mathrm{F}\subset\sig$ sub $\sigma$ álgebra y $X,Y$ \texrbf{v.a.} tal que $X$ sea $\mathrm{F}$ medible, entonces para toda $\varphi:\re\times\re\rightarrow\re$ Borel medible, se cumple
\[
\esp(\varphi(X,Y)|\mathrm{F})=\int_{y\in\re}\varphi(X,y)\prob_Y(dy)=\esp(\varphi(x,Y))|_{x\in\re}
\]
\end{prop}
\begin{lema}[Borel-Cantelli]\label{bor-can}
Sea $\{E_n\}$ una sucesión de eventos indpendientes en un espacio de probabilidad $(\om,\sig,\prob)$, entonces si
\begin{equation}
\sum_{n=1}^{\infty}\prob(E_n)=\infty\quad\text{implica}\quad\prob(\{E_n,\,\text{i.o }\})=\prob(\,\limsup E_n\,)
\end{equation}
\end{lema}
\dem Primero, sabemos que se cumple
\[
    \om\setminus(\limsup E_n)=\liminf \{\om\setminus E_n\}=\bigcup_{m\in\nat}\bigcap_{n\geq m}(\om\setminus E_n).
\]
\noindent Ahora si \(p_n=\prob(E_n)\), entonces por independencia se cumple que
\[
    \prob\Big(\bigcap_{n\geq m}(\om\setminus E_n)\Big)=\prod_{n\geq m}(1-p_n).
\]
Por monotonía de ambos lados de la ecuación, al tomar el límite cuando \(r\uparrow\infty\) y la condición \(\{r\geq n\geq m\}\), obtenemos gracias a la independencia lo siguiente. Para cada \(x\geq 0\), \(1-x\leq\exp(-x)\), así como la serie de los \(p_n\) es divergente por hipótesis, entonces
\[
    \prod(1-p_n)\leq\esp\Big(-\sum_{n\geq m}p_n\Big)=0.
\]
\noindent Por lo que \(\prob(\{\om\setminus\limsup E_n\})=0\) \qed
\section{Distribuciones integrales}
\label{sec:org8cc3821}

\noindent Todo lo dicho anteriormente es válido para variables aleatorias continuas, solo se tiene que cambiar de sumas finitas a integrales y suponer finitud de ésta. Los ejemplos vistos previamente han sido ilustrativos para casos más complejos.

\exe La forma más simple de definir una probabilidad es mediante una función positiva en un conjunto que se quiera medir, uno de los ejemplos más importantes para la teoría de la probabilidad es \emph{distribución gausiana} definida en todos los reales:
\[
    f(x)=\frac{1}{\sigma\sqrt{2\pi}}e^{-1/2(\frac{x-\mu}{\sigma})^2}\quad \prob(x)=\int_{t\leq x}f(t)dt
\]

\begin{def.}
Dado un espacio de probabilidad \((\om,\sig,\mu)\) una función \(X:\om\rightarrow \re\), se dice que \(X\) es una variable aleatoria si
\[
    X^{-1}[E]\in\sig\quad\forall E \quad\text{boreliano}
\]
\end{def.}
\begin{def.}
Dado un espacio de probabilidad \((\om,\sig,\mu)\) y una varible aleatoria no-negativa \(X:\om\rightarrow \re\) y de integral finita (no necesareamente continua en el sentido usual), se define la \emph{distribución de probabilidad} con función de densidad definida por \(X\) como
\[
\prob(E)=\int_E Xd\mu
\]
\end{def.}

\begin{def.}
Dado una probabilidad en los reales \((\re,\mathbb{B}_{\re},\prob)\) y una varible aleatoria no-negativa, se define la \emph{función de probabilidad} relacionada a \(\prob\) como
\[
        f(x)=\prob(\{t\,|\,t\leq x\})
\]
\end{def.}
\eje En el caso de la distribución gaussiana, la función de probabilidad fue definida como
\[
    \prob(x)=\int_{t\leq x}\frac{1}{\sigma\sqrt{2\pi}}e^{(\frac{t-\mu}{\sigma})^2}dt =
    \int_{-\infty}^{x}\frac{1}{\sigma\sqrt{2\pi}}e^{(\frac{t-\mu}{\sigma})^2}dt
\]
\section{Momentos continuos y la función característica}
\label{sec:orgc6d7542}
\part{Aprendizaje de máquina probabilistico/aprendizaje estadístico}
\label{sec:orgc79f0a8}
\end{document}
